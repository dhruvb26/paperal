{"result": {"id": "chatcmpl-Afd1xFeVuG7oMjHmAf6gVfoUWloMw", "choices": [{"finish_reason": "stop", "index": 0, "logprobs": null, "message": {"content": "(\"entity\"<|>GUS DATASET<|>ORGANIZATION<|>A dataset used for training and evaluating the GUS-Net model, focusing on token-level bias classification)\n##\n(\"relationship\"<|>GUS-NET<|>GUS DATASET<|>GUS-Net was fine-tuned and evaluated on the GUS dataset for token-level bias classification<|>9)\n<|COMPLETE|>", "refusal": null, "role": "assistant", "audio": null, "function_call": null, "tool_calls": null}}], "created": 1734484845, "model": "gpt-4-0125-preview", "object": "chat.completion", "service_tier": null, "system_fingerprint": null, "usage": {"completion_tokens": 89, "prompt_tokens": 3500, "total_tokens": 3589, "completion_tokens_details": {"accepted_prediction_tokens": 0, "audio_tokens": 0, "reasoning_tokens": 0, "rejected_prediction_tokens": 0}, "prompt_tokens_details": {"audio_tokens": 0, "cached_tokens": 0}}}, "input": {"messages": [{"content": "\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity's attributes and activities\nFormat each entity as (\"entity\"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as (\"relationship\"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis's Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n(\"entity\"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n(\"entity\"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n(\"entity\"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis's money supply)\n##\n(\"relationship\"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal's (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation's debut on the public markets isn't indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n(\"entity\"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n(\"entity\"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n(\"relationship\"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad's capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia's capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia's Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n(\"entity\"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n(\"entity\"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n(\"entity\"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n(\"entity\"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n(\"entity\"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n(\"entity\"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n(\"entity\"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia's Alhamia Prison)\n##\n(\"entity\"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n(\"entity\"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n(\"entity\"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n(\"relationship\"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n(\"relationship\"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n(\"relationship\"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: GB of RAM. The codebase was imple-\nmented using PyTorch and the Transformers library,\nand executed on Ubuntu 20.04 with Python 3.8. We\nemployed pytorch-lightning to streamline the train-\ning loops and logging mechanisms.\n4.1.2 Hyperparameters We trained our BERT-\nbased multi-label token classification model with seven\noutput classes over 17 epochs. The training process uti-\nlized a batch size of 16 and an initial learning rate of\n5×10−5. The AdamW optimizer with weight decay was\nimplemented, along with a linear learning rate sched-\nuler featuring a warm-up ratio of 0.1. To handle class\nimbalance, we employed focal loss with α = 0 .65 for\nthe I-GEN label. The classification threshold for all\nlabels was set at 0.5. The original dataset was parti-\ntioned into training (70%), validation (15%), and test\n(15%) splits, ensuring similar distributions of biased en-\ntity types across these splits.\n4.2 Results\n4.2.1 Overall Performance on Multi-Label\nClassification for Token-level Bias The primary\ngoal of this experiment is to evaluate the performance\nof our model in accurately identifying biased entities at\nthe token-level within the GUS dataset. By focusing\non token-level classification, we aimed to capture occur-\nrences of social bias at the level of individual words and\nphrases, rather than merely at the sentence level.\nDue to the absence of directly comparable exist-\ning token-level work as a baseline for our model, we\nopted to implement a variant of Nbias, which was the\nstate-of-the-art (SOTA) method designed for token-level\nclassification. We modified Nbias into a multi-label\nframework and fine-tuned it on the GUS dataset. This\nadaptation allowed us to examine how well the Nbias\narchitecture could perform in at multi-label classifica-\ntion, even though it was designed for multi-class classi-\nfication, while also addressing the challenges associated\nwith imbalanced class distributions. In our implemen-\ntation, we employed focal loss instead of binary cross-\nentropy to better manage class imbalance. This choice\nwas critical, as our dataset exhibited significant dispari-\nties in the representation of biased entities, particularly\nwith respect to the Unfairness class. From the results\nin Table 3, we have the following observations:\n• The Hamming Loss observed for GUS-Net (0.0528)\nwas on par with Nbias (0.564), meaning they clas-\nsified a similar overall number of labels correctly.\n• GUS-Net demonstrates better overall F1 (0.80) and\nRecall (0.77) metrics compared to the F1 (0.68)\nand Recall (0.63) observed with Nbias. The su-\nperior F1 Score and Recall for GUS-Net highlight\nits effectiveness in identifying presences rather than\nabsences. This could be due to the incorpora-\ntion of focal loss during training, which allows the\nmodel to focus more on difficult-to-classify exam-\nples, thereby improving its overall sensitivity to the\npresence of bias. Conversely, while Nbias achieves\nhigher Precision (0.93) than GUS-Net (0.82), its\nlower Recall indicates that it may not be capturing\nall relevant instances of bias.\n• The detailed F1 scores for each entity type in\nGUS-Net show strong performance, particularly in\nthe Stereotypes (0.90) and Generalizations (0.74)\nclasses, without sacrificing performance on the\nNeutral (0.95) class. Our model balances perfor-\nmance across entity types, which suggests the ef-\nfectiveness of focal loss to encapsulate imbalance of\nCopyright © 2025 by SIAM\nUnauthorized reproduction of this article is prohibited\n\nPage 7\nFigure 5: Scatter plot showing the minimum normalized\nbiased entities versus normalized number of biased\nwords, along with the trend line. The understanding of\nbias given by GUS-Net aligns well with the definitions\nestablished in the BABE dataset.\nbiased entities.\n4.2.2 Comparison with Human Annoation The\nBABE (Bias Annotations By Experts) dataset [27] is a\nwell-established resource in bias detection, containing a\ndiverse range of biased statements annotated by trained\nexperts. This dataset is valuable as it provides insights\ninto various forms of bias across different demograph-\nics and contexts, making it a relevant benchmark for\nevaluating our model’s performance.\nIn this analysis, we aimed to compare the normal-\nized number of biased words per sentence in the\nBABE dataset with the number of positive (non-‘O’) la-\nbel classifications made by our model (GUS-Net). The\nnormalized number of biased words refers to the count\nof biased words adjusted for sentence length, allowing\nfor a fair comparison across sentences of varying lengths.\nTo obtain the normalized number of biased words,\nwe first filtered the training split of the BABE dataset\nto include only sentences classified as biased. Since our\nmodel labels multiple entity types (GEN, UNFAIR, and\nSTEREO) and the BABE dataset does not distinguish\nbetween different forms of bias, we adjusted for imbal-\nance by binning the results and using the minimum\nnumber of GUS entities found in each bin. The num-\nber of biased words from BABE was then normalized\nby dividing by the sentence length.\nThe scatter plot in Figure 5 reveals a positive corre-\nlation between the normalized number of biased words\nfrom the BABE dataset and the normalized minimum\nnumber of biased entities predicted by our model.\n######################\nOutput:", "role": "user"}, {"role": "assistant", "content": "(\"entity\"<|>PYTORCH<|>ORGANIZATION<|>PyTorch is an open-source machine learning library used for applications such as computer vision and natural language processing, employed in the codebase implementation)\n##\n(\"entity\"<|>TRANSFORMERS LIBRARY<|>ORGANIZATION<|>A library providing pre-trained models for natural language processing tasks, used alongside PyTorch in the codebase)\n##\n(\"entity\"<|>UBUNTU 20.04<|>GEO<|>An operating system based on the Linux kernel, used for executing the project's code)\n##\n(\"entity\"<|>PYTHON 3.8<|>ORGANIZATION<|>A version of the Python programming language, used for writing the project's code)\n##\n(\"entity\"<|>PYTORCH-LIGHTNING<|>ORGANIZATION<|>A lightweight PyTorch wrapper for high-performance AI research, simplifying the training loop and logging mechanisms)\n##\n(\"entity\"<|>BERT<|>ORGANIZATION<|>A transformer-based machine learning technique for natural language processing pre-training, used in a multi-label token classification model)\n##\n(\"entity\"<|>ADAMW OPTIMIZER<|>ORGANIZATION<|>An optimization algorithm used for training, incorporating weight decay for regularization)\n##\n(\"entity\"<|>GUS-NET<|>ORGANIZATION<|>A model developed for multi-label classification of token-level bias within the GUS dataset, showing improved performance over Nbias)\n##\n(\"entity\"<|>NBias<|>ORGANIZATION<|>A state-of-the-art method for token-level classification, which GUS-Net was compared against)\n##\n(\"entity\"<|>BABE DATASET<|>ORGANIZATION<|>A dataset containing biased statements annotated by experts, used for evaluating GUS-Net's performance)\n##\n(\"relationship\"<|>PYTORCH<|>TRANSFORMERS LIBRARY<|>PyTorch was used alongside the Transformers library for the project's implementation<|>7)\n##\n(\"relationship\"<|>GUS-NET<|>NBias<|>GUS-Net was developed as an improvement over Nbias for token-level bias classification<|>8)\n##\n(\"relationship\"<|>GUS-NET<|>BABE DATASET<|>GUS-Net's performance was evaluated against the BABE dataset<|>6)\n<|COMPLETE|>"}, {"content": "MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n", "role": "user"}], "parameters": {"model": "gpt-4-turbo-preview", "frequency_penalty": 0.0, "max_tokens": 4000, "n": 1, "presence_penalty": 0.0, "temperature": 0.0, "top_p": 1.0}}}