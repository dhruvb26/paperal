Page 1
GUS-Net: Social Bias Classification in Text with Generalizations,
Unfairness, and Stereotypes
Maximus Powers∗ Umang Mavani∗† Harshitha Reddy Jonala∗† Ansh Tiwari∗†
Hua Wei†
Abstract
The detection of bias in natural language processing
(NLP) is a critical challenge, particularly with the
increasing use of large language models (LLMs) in
various domains. This paper introduces GUS-Net, an
innovative approach to bias detection that focuses on
three key types of biases: Generalizations, Unfairness,
and Stereotypes. GUS-Net leverages generative AI
and automated agents to create a comprehensive
synthetic dataset, enabling robust multi-label token
classification. Our methodology enhances traditional
bias detection methods by incorporating the contex-
tual embeddings of pre-trained models, resulting in
improved accuracy and depth in identifying biased
entities. Through extensive experiments, we demon-
strate that GUS-Net outperforms state-of-the-art
techniques, achieving superior performance in terms of
accuracy, F1-score, and Hamming Loss. The findings
highlight GUS-Net’s effectiveness in capturing a wide
range of biases across diverse contexts, making it a
valuable tool for social bias detection in text. This
study contributes to the ongoing efforts in NLP to
address implicit bias, providing a pathway for future
research and applications in various fields. The Jupyter
notebooks used to create the dataset and model are
available at https://github.com/Ethical-Spectacle/fair-
ly/tree/main/resources.
Warning: This paper contains examples of harm-
ful language, and reader discretion is recommended.
1 Introduction
The detection of bias in natural language processing
(NLP) [13] is an important task, particularly with the
increasing use of large language models (LLMs) [33] in
domains like education [18] and business [29]. Bias can
significantly influence public perception and decision-
making, often subtly reinforcing stereotypes or prop-
∗Ethical Spectacle Research.
†Arizona State University.
agating discriminatory practices. While explicit bias,
which refers to clearly expressed prejudice or favoritism,
is easy to define, implicit bias involves more subtle and
often unconscious associations or attitudes. Therefore,
identifying and mitigating implicit bias in the text is
challenging: what is perceived as biased can vary greatly
depending on the context, including the perspectives of
viewers and speakers. For example, consider the phrase
“hard-working immigrants”. To some, this phrase may
appear positive, acknowledging the effort and diligence
of immigrants. However, from another perspective, it
might be perceived as implicitly biased, suggesting that
immigrants are expected to work harder than others to
be valued or accepted. This subtle implication can be
seen as reinforcing a stereotype that separates immi-
grants from native citizens, placing an undue burden of
proof on their worthiness. This subjectivity underlines
the complexity of implicit bias detection, making it a
critical area of research within NLP [14, 28, 25].
While the implicit nature of bias can manifest in
subtle forms, such as the choice of words, framing of
narratives, or the omission of certain viewpoints, tradi-
tional approaches to bias detection have typically re-
lied on human annotators to label datasets [25, 26].
Although this method has been essential for creating
foundational resources, it is susceptible to the biases of
annotators, who may struggle to step outside their own
ideological frameworks. As a result, existing datasets of-
ten lack the diversity of viewpoints necessary to capture
implicit biases effectively. Additionally, many datasets,
such as the one utilized by the Dbias model [32], are
limited in scope, focusing on a narrow range of biases
and failing to generalize across different contexts. The
Nbias framework [25], while an advancement in incor-
porating named entity recognition (NER) tasks, still
emphasizes explicit biases and overlooks the structural
elements of implicit bias, such as generalizations and
stereotypes. Despite studies of robust annotations con-
ducted by trained experts [27], all the previous bias
datasets rely on human annotators, which means that
these datasets often lack the diversity of viewpoints nec-
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited
arXiv:2410.08388v2  [cs.CL]  17 Oct 2024

Page 2
essary to capture implicit bias.
In response to these challenges, this paper intro-
duces the Generalizations, Unfairness, and Stereotypes
Network (GUS-Net), which stands for three kinds of
biases that are commonly considered in legal and psy-
chological literature [1, 3, 12, 21]: Generalizations,
Unfairness, and Stereotypes. GUS-Net leverages gen-
erative AI and automated agents to build an optimal
dataset for bias detection. Using the synthetic data
generated by these automated agents, we further fine-
tune the pre-trained model BERT (Bidirectional En-
coder Representations from Transformers) [10] for the
task of multi-label token classification. This approach
improves upon traditional methods by combining LLM
reasoning and the powerful contextual embeddings pro-
vided by pre-trained models, resulting in more accu-
rate and comprehensive bias detection across various
types of text. Our extensive experiments demonstrate
that the proposed method outperforms state-of-the-art
techniques in terms of dataset diversity and annotation
depth. The fine-tuned model not only achieves superior
performance on traditional metrics, such as accuracy
and F1-score but also provides a more nuanced under-
standing of the biases present in various texts. The main
contributions of this paper are:
• We generate a corpus containing biases in varied
domains, annotated by a team of LLM agents.
• We train an NLP model for multi-label named-
entity recognition, enhancing bias detection speci-
ficity and insight. To the best of our knowledge,
this is the first work to provides token-level multi-
label bias detection.
• We conduct experiments to demonstrate the con-
tributions of our methods in relation to existing
approaches, showcasing improvements in accuracy,
F1-score, and the depth of bias detection.
2 Related Works
The detection of bias in natural language processing
(NLP) is a critical area of research, particularly given
the increasing use of large language models (LLMs)
across various domains [7, 9, 33, 18, 8, 29, 30]. Tradi-
tional techniques for bias detection often rely on human
annotators to label datasets. While this approach has
been instrumental in creating foundational resources,
it is inherently limited by the annotators’ subjective
perspectives, which can introduce their own biases into
the annotation process [25, 26]. This limitation often
leads to a narrow understanding of bias, especially in
regard to implicit biases that are subtle and context-
dependent [13, 14, 28].
2.1 Ethical Dataset Construction The construc-
tion of ethical datasets for bias detection is essential
for ensuring comprehensive and fair analyses. Existing
datasets often suffer from limitations in scope, failing
to encompass the broad spectrum of biases and per-
spectives necessary for effective bias detection. For ex-
ample, the Dbias model [26] utilized the MBIC dataset,
which consists of a relatively small number of sentences,
restricting the model’s ability to generalize across dif-
ferent domains and types of bias. Although the NBias
framework [25] expanded the use of named-entity recog-
nition (NER) by introducing the entity “BIAS” it still
primarily addressed explicit biases and overlooked the
structural elements of implicit bias, such as stereotypes
and generalizations.
Moreover, studies that emphasize robust annota-
tions often rely on human judgment, which can lead
to a lack of diversity in viewpoints necessary to capture
the nuance of implicit bias [27]. This reliance on hu-
man annotators may also perpetuate the biases present
in society, resulting in datasets that do not adequately
represent the full range of perspectives. Thus, there
is a pressing need for more diverse and comprehensive
datasets that can capture implicit biases in language.
2.2 Bias Detection Traditional methods typically
focus on explicit bias, which is easier to define and iden-
tify, while neglecting the subtler forms of bias that may
influence public perception and decision-making. Im-
plicit bias can manifest through word choice, framing,
and the omission of certain viewpoints, making it chal-
lenging to detect using conventional approaches [13, 14].
Existing frameworks, such as Dbias and Nbias, have
made strides in bias detection but still focus primarily
on explicit biases, leaving a gap in the understanding of
how implicit biases operate [26, 25]. Additionally, the
datasets used for these frameworks often lack the neces-
sary diversity of perspectives, limiting their effectiveness
in identifying implicit biases. In contrast, our proposed
approach leverages generative AI and automated agents
to construct a more comprehensive dataset. By utilizing
synthetic data generated by these agents, we enhance
the training of the pre-trained model BERT for multi-
label token classification. This innovative methodology
not only improves the specificity and depth of bias de-
tection but also addresses the limitations of existing
datasets, paving the way for more accurate and nuanced
understanding of biases in various texts.
3 Methodology
3.1 Dataset Generation With modern synthetic
training data labeling techniques, we can create a
comprehensive dataset encapsulating our novel entities
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 3
Table 1: Detailed list of arguments for corpus creation.
Type Of Bias Target Statement TypeSentiment
Racial [white people, black people, asian people, hispanic people, indigenous people, ...]
Religious [christians, muslims, jewish people, hindus, buddhists, sikhs, atheists, agnostics
Gender [men, women, boys, girls, females, males, non-binary people, ...]
Age [children, teenagers, young people, middle aged people, old people, ...]
Nationality[immigrants, refugees, people from developing countries, people from Western countries, ...]
Sexuality [straight people, gay people, bisexual people, asexual people, LGBTQIA+ people, ...]
Socioeconomic[working class people, middle class people, upper class people, poor people, rich people, ...]
Educational [uneducated people, highly educated people, people with non-traditional education, ...]
Disability [people with physical disabilities, people in wheelchairs, people with mental disabilities, ...]
Political [republicans, democrats, independents, conservatives, liberals, progressives, ...]
Physical [tall people, short people, fat people, skinny people, ugly people, hot people, ...]
Stereotypes,
Unfair Generalizations,
False Assumptions,
Discriminatory Language,
Offensive Implications
Positive,
Negative
Summarize
You are a biased writer. Your task is to write exactly {num_results} {sentiment} statements, containing {type_of_bias} bias about {target}. The statements should contain {statement_type} targeting {target}. Biased statements should emphasize sentence structures that target people, make generalizations, imply discrimination, and make unfair assumptions. 
(b) Multi-agent Annotation(a) Corpus Generation
GEN Agent
UNFAIR Agent
STEREO Agent
The immigrants are hard working
Input sample Annotationthe immigrants are hard working
Figure 1: Overview of dataset generation pipeline, which includes (a) corpus generation with Mistral [15] though
specifications on different arguments, and (b) multi-agent annotation with DSPy [19].
while avoiding the labor-intensive and potentially sub-
jective human annotation process [34, 31, 2]. In addition
to a synthetic data annotation pipeline, we also use a
language model to synthetically generate the underlying
corpus for better coverage and structural consistency of
the entities we aim to classify [17, 20, 11]. The overall
dataset generation pipeline can be found in Figure 1.
3.1.1 Corpus Creation Since no existing dataset
includes the specific entities of generalizations, unfair-
ness, and stereotypes, we created a synthetic corpus of
statements and questions. To ensure coverage across
multiple domains, we developed four lists to utilize in
prompting: Type of bias, target group, statement type,
and sentiment, as shown in Table 1.
Using Mistral-7B [15], selected for its lack of
guardrails, we generated prompts by combining values
from each list. This process was applied with different
prompt templates and types of statements for both bi-
ased and fair statements. The prompt template used for
biased statement generation is illustrated in Figure 1(a).
For the generation of 1,294 fair statements, the
Sentiment was modified to indicate either “slightly pos-
itive yet fair” or “slightly negative yet fair.” Responses
were formatted in JSON for easier parsing and storage.
While an authentic corpus could be used for annotation
and training, the synthetic corpus created offers two key
advantages: a broader scope that balances domains and
a rich density of targeted parts of speech.
3.1.2 Data Annotation The data annotation pro-
cess involves several systematic steps to ensure accuracy
and consistency in labeling the generated sentences. We
utilized GPT-4o and the Stanford DSPy framework [19]
to annotate the generated sentences with entity labels,
following methodologies in recent literature for anno-
tation with agents [24]. As shown in Figure 1(b), the
annotation can be summarized in the following steps:
• Preparation of annotation : We recompiled a
DSPy agent for each entity type (e.g., generalization,
unfairness, stereotype) as an agent to streamline the
annotation process. Each agent contains the entity
definition and four examples of correct annotations.
The prompts sent to an agent include the definition
of the target entity along with the curated examples.
Using few-shot learning helps the agent understand the
context and provides guidance for accurate labeling [5,
4, 6, 16]. Their definitions are provided in Table 2.
• Annotation by agents : Each generated sentence
is processed by an agent supported by a language
model (LLM) for a single type of entity. We included
a Suggestions feature to ensure correct word/label
alignment, that allows for backtracking and corrections.
The agent evaluates the sentence for the presence of
the specified entity and assigns appropriate labels. The
agent produces a list of NER tags for each sentence,
indicating the presence or absence of the entity types.
• Summarizing module: After annotating for one en-
tity type, we aggregate the labels into a comprehensive
two-dimensional list. Each sub-list contains one or mul-
tiple tags corresponding to each token in the sentence.
• Final compilation: The individual entity labels from
each annotation are systematically combined to create
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 4
Table 2: Definitions used by annotator agents, and the corresponding entity labels.
Class Prompt Definition Entity Label
Generalizations Any broad generalization of a group or ubiquitous classifiers,
including adjectives/descriptors.
B-GEN, I-GEN
Unfairness Any harsh or unjust characterization or offensive language.B-UNFAIR, I-UNFAIR
Stereotypes Any statement (multiple words) that contains a stereotype
targeting a group of people, both explicitly and unconsciously.
B-STEREO, I-STEREO
Neutral - O
Racial
239
Religious
244
Gender
392
Age
275
Nationality
359
Sexuality
201
Socioeconomic
310
Educational
209
Disability
182
Political
282 Physical
152
Figure 2: Types of Bias in GUS Dataset. Note: Some
sentences contain multiple types of bias.
a final annotated dataset, with each token categorized
into one or multiple semantic part-of-speech categories
using B/I/O (Beginning, Inside, Outside) labels, as
shown in Table 2.
In total, we annotated 3,739 sentences, each labeled
for multi-label token classification training. In Figure 2
we depict the proportions of each type of bias repre-
sented in our dataset, as labeled post-generation by
gpt-4o to identify cases where more than one type of
bias is represented by a sentence. In Figure 3, we de-
pict the distribution of labels in the annotated GUS
dataset. It is important to note that each token can
be classified with more than one label, so the sum of
all labels is greater than the total number of tokens in
the GUS dataset (69,679 tokens). The GUS dataset is
54.7% statements, and 45.3% questions.
3.2 Proposed Model To efficiently and accu-
rately identify social biases in text, we propose a
multi-label token classification model. As shown
in Figure 4, we fine-tune the pre-trained model
bert-base-uncased [10] for multi-label classifica-
tion [23].
Rather than implementing a single entity to cap-
ture all definitions and nuances of “bias,” our model
achieves more granular and accurate insights with en-
tities chosen for their individual semantic clarity and
collective comprehensiveness of social bias. By utiliz-
67.9%
9.6%
4.7%
3.4%
17.8%
2.7%
3.1%
O
B-GEN
I-GEN
B-STEREO
I-STEREO
B-UNFAIR
I-UNFAIR
Figure 3: Token-Level Label Distribution in GUS
Dataset (Total Tokens: 69,679). Note: Some tokens
have multiple labels.
BERT Encoder
[CLS]Theimmigrantsarehard [SEP]
H H H H H H
Linear
O B-GENB-STEREOI-STEREOI-STEREOB-UNFAIR
The immigrants are hard working
working
I-STEREOI-UNFAIR
Figure 4: Overview of our proposed model with multi-
label classification.
ing a multi-label B/I/O format to represent token-level
annotations, the model can predict nested entities that
span multiple words. For instance, stereotypes often
span a full sentence beginning with a generalization, to
which some unfairness is assigned.
3.2.1 Model Architecture The GUS-Net model is
a multi-label token classification system designed to
identify social bias across three categories: generaliza-
tions, unfairness, and stereotypes. It outputs seven la-
bels, allowing the model to capture the nuanced struc-
ture of biased language. These labels facilitate the iden-
tification of individual bias categories, and provide flex-
ibility for overlapping and nested biases.
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 5
Input Processing. We tokenize all sentences us-
ing the pre-trained BERT tokenizer, ensuring that token
splits (such as sub-words) inherit the correct entity la-
bels from the parent word. Each text sequence is padded
to a maximum length of 128 tokens to ensure consistent
input size. Since sentences are rarely longer than 128
tokens, we reduced the BERT input size from the de-
fault 512 tokens, representing a 16x reduction in self-
attention elements to be processed. Correspondingly,
the NER tags were converted into a (128, 7) dimensional
vector, where each of the seven elements represents a bi-
nary label (0 or 1) for the respective entity type. These
vectors were padded with −100 values up to the full se-
quence length of 128 tokens, with the −100 values being
ignored during the loss calculation.
Model Fine-Tuning. We fine-tune the
pre-trained transformer model, specifically
bert-base-uncased, due to its ability to capture
deep contextual relationships between words, which
is crucial for identifying implicit biases [10]. BERT’s
bidirectional nature allows it to process the entire
input sequence, ensuring that each token is evaluated
in the context of its surrounding words. This feature is
particularly valuable in detecting subtle and complex
forms of social bias.
The model is implemented using the Hugging Face
transformers library. Input text is tokenized with
the pre-trained BERT tokenizer, which breaks down
sentences into sub-word units while preserving word
boundaries. Each token sequence is padded to a length
of 128 tokens, and the corresponding labels are mapped
to ensure proper alignment. By reducing the input
size from the default 512 tokens to 128, we optimize
the model’s computational efficiency without sacrificing
performance for typical sentence lengths in the dataset.
3.2.2 Loss Function Given the significant class im-
balance in our dataset, where certain entities like
STEREO are underrepresented compared to frequent
entities like O, we employed focal loss to address this
challenge [22]. Standard binary cross-entropy (BCE)
tends to focus on the majority class, leading to poor
performance on rare classes. In this paper, we use fo-
cal loss calculated over all tokens, defined as:
FL(pt) = −αt(1 − pt)γ log(pt)
where pt is the predicted probability for the true class;
(1 −pt)γ reduces the impact of well-classified examples,
helping the model prioritize rare or difficult examples; α
balances the contribution of positive and negative sam-
ples, ensuring underrepresented entities receive more fo-
cus; γ down-weights well-classified examples, allowing
the model to concentrate on harder-to-predict instances.
4 Experiments
4.1 Settings
Task Description and Metrics. In this paper,
we performed a token-level multi-label classification
task, where each sentence is annotated with one or more
labels to facilitate the identification of biased entities
across token sequences. Token-level classification is es-
sential for bias detection because biases can often be
nuanced and context-dependent. By focusing on indi-
vidual tokens, we can capture subtle implications and
associations that may be overlooked in a sentence-level
analysis. Moreover, we opted for multi-label classifica-
tion instead of multi-class classification to better reflect
the complex nature of biases, that may fall into one or
many entity classes. A single-class approach, like the
one proposed for Nbias, would limit our ability to cap-
ture the diversity of biases present in the text, as a
statement may contain multiple biases simultaneously.
Metrics. To evaluate the model’s performance, we
utilized a variety of metrics to assess its ability to
accurately identify biased entities:
• Hamming Loss: This metric measures the frac-
tion of incorrect labels over all tokens in the se-
quence, accounting for multi-label classification. It
is defined as:
Hamming Loss = 1
L
LX
i=1
1 (yi ̸= ˆyi)
where L is the total number of tokens, yi is the
true label for the i-th token, ˆyi is the predicted
label, and 1 is an indicator function that evaluates
whether the true label differs from the prediction.
• Precision, Recall, and F1-Score: These metrics
were calculated at two levels: individually for
each entity class and as a macro-average across all
classes. They are defined as follows:
Precision = T P
T P+ F P, Recall = T P
T P+ F N
F1-Score = 2 · Precision · Recall
Precision + Recall
where TP denotes true positives, FP denotes false
positives, and FN denotes false negatives.
Given the imbalanced class distribution in our
dataset, we evaluated both the macro-average perfor-
mance of the model and individual entity-type-level
metrics. By treating B- and I- tags as a single entity
(e.g., combining B-GEN and I-GEN predictions), we en-
hance our evaluation of the model’s ability to detect the
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 6
Table 3: Baseline Comparison of GUS-Net over Nbias w.r.t. both overall and entity-type-based F1, Precision and
Recall. GUS-Net outperforms Nbias in most cases.
Metrics Macro Entity-type-based
GUS-Net
Hamming Loss 0.0528 Generalizations Unfairness Stereotypes Neutral
F1 0.8 0.74 0.61 0.90 0.95
Precision 0.82 0.78 0.69 0.89 0.93
Recall 0.77 0.72 0.49 0.90 0.97
Nbias
Hamming Loss 0.06 Generalizations Unfairness Stereotypes Neutral
F1 0.68 0.70 0.19 0.89 0.95
Precision 0.93 0.87 0.83 0.94 0.95
Recall 0.63 0.56 0.11 0.86 0.96
presence of each biased entity, rather than merely as-
sessing the boundaries. This approach allows us to gain
deeper insights into the model’s performance across the
diverse classes of bias present in the data.
4.1.1 Hardware and Environment All experi-
ments were conducted on a single NVIDIA T4 GPU
with 16GB of memory, hosted on Google Colab, uti-
lizing under 10GB of RAM. The codebase was imple-
mented using PyTorch and the Transformers library,
and executed on Ubuntu 20.04 with Python 3.8. We
employed pytorch-lightning to streamline the train-
ing loops and logging mechanisms.
4.1.2 Hyperparameters We trained our BERT-
based multi-label token classification model with seven
output classes over 17 epochs. The training process uti-
lized a batch size of 16 and an initial learning rate of
5×10−5. The AdamW optimizer with weight decay was
implemented, along with a linear learning rate sched-
uler featuring a warm-up ratio of 0.1. To handle class
imbalance, we employed focal loss with α = 0 .65 for
the I-GEN label. The classification threshold for all
labels was set at 0.5. The original dataset was parti-
tioned into training (70%), validation (15%), and test
(15%) splits, ensuring similar distributions of biased en-
tity types across these splits.
4.2 Results
4.2.1 Overall Performance on Multi-Label
Classification for Token-level Bias The primary
goal of this experiment is to evaluate the performance
of our model in accurately identifying biased entities at
the token-level within the GUS dataset. By focusing
on token-level classification, we aimed to capture occur-
rences of social bias at the level of individual words and
phrases, rather than merely at the sentence level.
Due to the absence of directly comparable exist-
ing token-level work as a baseline for our model, we
opted to implement a variant of Nbias, which was the
state-of-the-art (SOTA) method designed for token-level
classification. We modified Nbias into a multi-label
framework and fine-tuned it on the GUS dataset. This
adaptation allowed us to examine how well the Nbias
architecture could perform in at multi-label classifica-
tion, even though it was designed for multi-class classi-
fication, while also addressing the challenges associated
with imbalanced class distributions. In our implemen-
tation, we employed focal loss instead of binary cross-
entropy to better manage class imbalance. This choice
was critical, as our dataset exhibited significant dispari-
ties in the representation of biased entities, particularly
with respect to the Unfairness class. From the results
in Table 3, we have the following observations:
• The Hamming Loss observed for GUS-Net (0.0528)
was on par with Nbias (0.564), meaning they clas-
sified a similar overall number of labels correctly.
• GUS-Net demonstrates better overall F1 (0.80) and
Recall (0.77) metrics compared to the F1 (0.68)
and Recall (0.63) observed with Nbias. The su-
perior F1 Score and Recall for GUS-Net highlight
its effectiveness in identifying presences rather than
absences. This could be due to the incorpora-
tion of focal loss during training, which allows the
model to focus more on difficult-to-classify exam-
ples, thereby improving its overall sensitivity to the
presence of bias. Conversely, while Nbias achieves
higher Precision (0.93) than GUS-Net (0.82), its
lower Recall indicates that it may not be capturing
all relevant instances of bias.
• The detailed F1 scores for each entity type in
GUS-Net show strong performance, particularly in
the Stereotypes (0.90) and Generalizations (0.74)
classes, without sacrificing performance on the
Neutral (0.95) class. Our model balances perfor-
mance across entity types, which suggests the ef-
fectiveness of focal loss to encapsulate imbalance of
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 7
Figure 5: Scatter plot showing the minimum normalized
biased entities versus normalized number of biased
words, along with the trend line. The understanding of
bias given by GUS-Net aligns well with the definitions
established in the BABE dataset.
biased entities.
4.2.2 Comparison with Human Annoation The
BABE (Bias Annotations By Experts) dataset [27] is a
well-established resource in bias detection, containing a
diverse range of biased statements annotated by trained
experts. This dataset is valuable as it provides insights
into various forms of bias across different demograph-
ics and contexts, making it a relevant benchmark for
evaluating our model’s performance.
In this analysis, we aimed to compare the normal-
ized number of biased words per sentence in the
BABE dataset with the number of positive (non-‘O’) la-
bel classifications made by our model (GUS-Net). The
normalized number of biased words refers to the count
of biased words adjusted for sentence length, allowing
for a fair comparison across sentences of varying lengths.
To obtain the normalized number of biased words,
we first filtered the training split of the BABE dataset
to include only sentences classified as biased. Since our
model labels multiple entity types (GEN, UNFAIR, and
STEREO) and the BABE dataset does not distinguish
between different forms of bias, we adjusted for imbal-
ance by binning the results and using the minimum
number of GUS entities found in each bin. The num-
ber of biased words from BABE was then normalized
by dividing by the sentence length.
The scatter plot in Figure 5 reveals a positive corre-
lation between the normalized number of biased words
from the BABE dataset and the normalized minimum
number of biased entities predicted by our model. This
trend suggests that our model’s understanding of bias
aligns well with the definitions established in the BABE
dataset, indicating that GUS-Net effectively captures
Table 4: Ablation study by comparing the influence of
GUS dataset and focal loss.
Metrics GUS-Net GUS-Net w.o.
GUS dataset
GUS-Net w.o.
focal loss
Precision 0.82 0.02 0.93
Recall 0.77 0.22 0.63
F1-Score 0.80 0.05 0.68
Hamming Loss 0.05 0.26 0.06
and represents social biases present in the text.
4.2.3 Ablation Study We conducted an ablation
study to evaluate the impact of different configura-
tions on the model’s performance. Table 4 presents the
macro-average Precision, Recall, F1-score, and Ham-
ming Loss for the following settings: (1) Our pro-
posed GUS-Net model; (2) GUS-Net without GUS
dataset: This configuration relies on an existing cor-
pus, BABE [27]. Since there are no token-level annota-
tions for BABE, we used the same annotation pipeline
outlined in this paper. (3) GUS-Net without focal
loss: In this configuration, we trained the model using
the Binary Cross-Entropy (BCE) loss function.
From the results in Table 4, we have the following
observations:
• Our proposed architecture, GUS-Net, outper-
forms the other configurations across nearly all
key performance metrics. Specifically, GUS-Net
achieves the highest macro-average Precision (0.82)
and F1-Score (0.80), along with the lowest Ham-
ming Loss (0.05), indicating its superior ability to
accurately identify and classify entities with min-
imal misclassifications. The high Precision and
F1-Score suggest that GUS-Net effectively reduces
false positives while maintaining a strong balance
between Precision and Recall.
• In contrast, substituting focal loss for BCE resulted
in a moderate Precision of 0.65. Upon further in-
spection of the metrics for each entity individually,
we found that the macro-average metrics were dis-
torted by the class imbalance of the ‘O’ tags. Es-
sentially, the model learns to prioritize predicting
‘O’ tags correctly, which detracts from its focus on
the new classes of interest. This observation em-
phasizes the importance of employing a loss func-
tion and architecture specifically designed to han-
dle class imbalance, as seen in GUS-Net, ensuring
more accurate and reliable model performance.
• Interestingly, using the BABE dataset as the un-
derlying corpus for annotation and training yielded
poor results. This is likely due to the nature of
our test set, which was designed to span various
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 8
Table 5: F1-Scores at varying α values, while γ = 2.
α 0.1 0.2 0.4 0.65 0.8
Generalizations F1 0.19 0.40 0.56 0.74 0.71
Unfairness F1 0.01 0.14 0.35 0.61 0.54
Stereotypes F1 0.60 0.81 0.83 0.90 0.83
Neutral F1 0.87 0.91 0.94 0.95 0.91
Macro Average F1 0.42 0.57 0.67 0.80 0.75
Hamming Loss 0.09 0.08 0.07 0.05 0.09
Table 6: F1-Scores at varying γ values, while α = 0.65.
γ 0.5 1 2 3 4
Generalizations F1 0.74 0.73 0.74 0.74 0.71
Unfairness F1 0.55 0.48 0.61 0.57 0.57
Stereotypes F1 0.90 0.89 0.90 0.88 0.87
Neutral F1 0.95 0.95 0.95 0.94 0.94
Macro Average F1 0.78 0.76 0.80 0.78 0.77
Hamming Loss 0.05 0.05 0.05 0.06 0.06
domains, whereas the BABE corpus was gathered
specifically from news articles. The domain-specific
nature of BABE may limit its effectiveness for gen-
eralizing across a broader range of biases.
4.3 Parameter Sensitivity Study To identify the
optimal focal loss parameters, α and γ, we conducted
a sensitivity study by testing various values for each
parameter while holding the other constant. As shown
in Table 5, we evaluated the performance of the model
at different α values while keeping γ fixed at 2. The
results indicate that the best-performing value for α
was 0.65, which resulted in improved F1-Scores across
all entity types. Table 6 shows the influence of γ
parameter while maintaining α at 0 .65. The results
reveal that the macro-average F1-Score remained at
0.80, indicating that this combination of parameters
effectively balances sensitivity and specificity across
entity types. Overall, the sensitivity study highlights
the importance of tuning the focal loss parameters to
improve the model’s performance in identifying various
biases. The optimal values used in this paper ( α = 0.65
and γ = 2) demonstrate the model’s ability to adapt
to class imbalances and enhance its performance in
detecting biased entities.
4.4 Case Study To demonstrate our model’s label-
ing capabilities and generalizability, we present a case
study involving religious bias from the GUS dataset.
In Figure 6(a), we provide an example of a statement
All Christians believe in an outdated fairy tale called the BibleStereotypesUnfairnessGeneralizations
B-STEREO, B-GEN I-STEREO, I-GEN I-STEREOI-STEREOI-STEREOI-STEREO, B-UNFAIRI-STEREO, I-UNFAIRI-STEREO, I-UNFAIRI-STEREOI-STEREOI-STEREO
(a) Example annotations in GUS Dataset and its corresponding meaning 
Atheists are so close mindedStereotypesUnfairnessGeneralizations
(b) Example predictions given by GUS-Net on thetestdata
B-STEREO, B-GEN I-STEREOI-STEREOI-STEREO, B-UNFAIRI-STEREO, I-UNFAIR
Figure 6: Example of GUS dataset and GUS-Net
Predictions.
that exhibits religious bias, along with the correspond-
ing labels generated by GUS-Net. Figure 6(b) show-
cases GUS-Net’s outputs for this case study, illustrating
its ability to accurately identify and classify instances
of religious bias. The outputs are represented visually,
highlighting how the model distinguishes between differ-
ent types of bias, including Generalizations, Unfairness,
and Stereotypes. This example indicates the effective-
ness of GUS-Net in generalizing across various forms of
bias, reinforcing its potential as a robust tool for bias
detection in diverse contexts.
5 Conclusion and Discussion
The proposed GUS-Net model addresses limitations in
existing bias detection methods by focusing on the nu-
anced identification of social biases with semantic cat-
egories of generalizations, unfairness, and stereotypes.
Moreover, GUS-Net uses a multi-label token classifi-
cation architecture, based on bert-base-uncased, that
allows entities to span multiple tokens and be nested
within each other. GUS-Net approaches bias with three
detailed entities, offering a more granular and precise
detection of social biases. This enables better insights
into the structural components of biased language. Our
results demonstrate that GUS-Net performs well at clas-
sifying tokens as each of the entities, with a notable
strength in detecting stereotypes. In sum, GUS-Net
contributes the field of bias detection in NLP by incor-
porating a fine-grained and multi-faceted view of biased
language.
Acknowledgements
We thank OpenAI for providing us with API credits
under the Researcher Access program.
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 9
References
[1] L. Alexander, What makes wrongful discrimination
wrong? biases, preferences, stereotypes, and proxies ,
University of Pennsylvania Law Review, 141 (1992),
pp. 149–219.
[2] A. Amalvy and V. Labatut , Annotation guidelines
for corpus novelties: Part 1 – named entity recognition,
2024, https://arxiv.org/abs/2410.02281, https://
arxiv.org/abs/2410.02281.
[3] R. D. Arvey , Unfair discrimination in the employ-
ment interview: Legal and psychological aspects. , Psy-
chological bulletin, 86 (1979), p. 736.
[4] R. Bommasani, D. A. Hudson, E. Adeli, R. Alt-
man, S. Arora, S. von Arx, M. S. Bernstein,
J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolf-
sson, S. Buch, D. Card, R. Castellon, N. Chat-
terji, A. Chen, K. Creel, J. Q. Davis, D. Dem-
szky, C. Donahue, M. Doumbouya, E. Dur-
mus, S. Ermon, J. Etchemendy, K. Ethayarajh,
L. Fei-Fei, C. Finn, T. Gale, L. Gillespie,
K. Goel, N. Goodman, S. Grossman, N. Guha,
T. Hashimoto, P. Henderson, J. Hewitt, D. E.
Ho, J. Hong, K. Hsu, J. Huang, T. Icard,
S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti,
G. Keeling, F. Khani, O. Khattab, P. W. Koh,
M. Krass, R. Krishna, R. Kuditipudi, A. Kumar,
F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Lev-
ent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Man-
ning, S. Mirchandani, E. Mitchell, Z. Munyikwa,
S. Nair, A. Narayan, D. Narayanan, B. New-
man, A. Nie, J. C. Niebles, H. Nilforoshan,
J. Nyarko, G. Ogut, L. Orr, I. Papadimitriou,
J. S. Park, C. Piech, E. Portelance, C. Potts,
A. Raghunathan, R. Reich, H. Ren, F. Rong,
Y. Roohani, C. Ruiz, J. Ryan, C. R ´e, D. Sadigh,
S. Sagawa, K. Santhanam, A. Shih, K. Srini-
vasan, A. Tamkin, R. Taori, A. W. Thomas,
F. Tram `er, R. E. Wang, W. Wang, B. Wu,
J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You,
M. Zaharia, M. Zhang, T. Zhang, X. Zhang,
Y. Zhang, L. Zheng, K. Zhou, and P. Liang ,
On the opportunities and risks of foundation models ,
2022, https://arxiv.org/abs/2108.07258, https://
arxiv.org/abs/2108.07258.
[5] T. B. Brown, B. Mann, N. Ryder, M. Sub-
biah, J. Kaplan, P. Dhariwal, A. Neelakan-
tan, P. Shyam, G. Sastry, A. Askell, S. Agar-
wal, A. Herbert-Voss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. M. Ziegler, J. Wu,
C. Winter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess, J. Clark,
C. Berner, S. McCandlish, A. Radford,
I. Sutskever, and D. Amodei , Language models
are few-shot learners , 2020, https://arxiv.org/abs/
2005.14165, https://arxiv.org/abs/2005.14165.
[6] Y.-C. Chan, G. Pu, A. Shanker, P. Suresh,
P. Jenks, J. Heyer, and S. Denton , Balancing cost
and effectiveness of synthetic data generation strategies
for llms , 2024, https://arxiv.org/abs/2409.19759,
https://arxiv.org/abs/2409.19759.
[7] L. Da, T. Chen, L. Cheng, and H. Wei , Llm un-
certainty quantification through directional entailment
graph and claim level response augmentation , arXiv
preprint arXiv:2407.00994, (2024).
[8] L. Da, M. Gao, H. Mei, and H. Wei , Prompt to
transfer: Sim-to-real transfer for traffic signal control
with prompt learning , in Proceedings of the AAAI
Conference on Artificial Intelligence, vol. 38, 2024,
pp. 82–90.
[9] L. Da, K. Liou, T. Chen, X. Zhou, X. Luo,
Y. Yang, and H. Wei , Open-ti: Open traffic intel-
ligence with augmented language model , International
Journal of Machine Learning and Cybernetics, (2024),
pp. 1–26.
[10] J. Devlin, M.-W. Chang, K. Lee, and
K. Toutanova , Bert: Pre-training of deep bidi-
rectional transformers for language understanding , in
Proceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1
(Long and Short Papers), 2019, pp. 4171–4186.
[11] M. Doh, , and A. Karagianni, ”my kind of woman”:
Analysing gender stereotypes in ai through the av-
erageness theory and eu law , 2024, https://arxiv.
org/abs/2407.17474, https://arxiv.org/abs/2407.
17474.
[12] M. E. Heilman , Gender stereotypes and workplace
bias, Research in organizational Behavior, 32 (2012),
pp. 113–135.
[13] D. Hovy and S. Prabhumoye, Five sources of bias in
natural language processing , Language and linguistics
compass, 15 (2021), p. e12432.
[14] A. Z. Jacobs, S. L. Blodgett, S. Barocas,
H. Daum ´e III, and H. Wallach , The meaning
and measurement of bias: lessons from natural lan-
guage processing, in Proceedings of the 2020 conference
on fairness, accountability, and transparency, 2020,
pp. 706–706.
[15] A. Q. Jiang, A. Sablayrolles, A. Mensch,
C. Bamford, D. S. Chaplot, D. de las
Casas, F. Bressand, G. Lengyel, G. Lample,
L. Saulnier, L. R. Lavaud, M.-A. Lachaux,
P. Stock, T. L. Scao, T. Lavril, T. Wang,
T. Lacroix, and W. E. Sayed , Mistral 7b ,
2023, https://arxiv.org/abs/2310.06825, https://
arxiv.org/abs/2310.06825.
[16] I. Joshi, I. Gupta, A. Dey, and T. Parikh , ’since
lawyers are males..’: Examining implicit gender bias
in hindi language generation by llms , 2024, https:
//arxiv.org/abs/2409.13484, https://arxiv.org/
abs/2409.13484.
[17] M. Kamruzzaman, A. A. Monsur, S. Das, E. Has-
san, and G. L. Kim , Banstereoset: A dataset to
measure stereotypical social biases in llms for bangla ,
2024, https://arxiv.org/abs/2409.11638, https://
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

Page 10
arxiv.org/abs/2409.11638.
[18] E. Kasneci, K. Seßler, S. K ¨uchemann, M. Ban-
nert, D. Dementieva, F. Fischer, U. Gasser,
G. Groh, S. G¨unnemann, E. H¨ullermeier, et al.,
Chatgpt for good? on opportunities and challenges of
large language models for education , Learning and in-
dividual differences, 103 (2023), p. 102274.
[19] O. Khattab, A. Singhvi, P. Maheshwari,
Z. Zhang, K. Santhanam, S. Haq, A. Sharma,
T. T. Joshi, H. Moazam, H. Miller, M. Zaharia,
and C. Potts , Dspy: Compiling declarative lan-
guage model calls into self-improving pipelines , arXiv
preprint arXiv:2310.03714, (2023), https://arxiv.
org/abs/2310.03714.
[20] T. King, Z. Wu, A. Koshiyama, E. Kazim, and
P. Treleaven, Hearts: A holistic framework for ex-
plainable, sustainable and robust text stereotype de-
tection, 2024, https://arxiv.org/abs/2409.11579,
https://arxiv.org/abs/2409.11579.
[21] F. J. Landy , Stereotypes, bias, and personnel deci-
sions: Strange and stranger , Industrial and Organiza-
tional Psychology, 1 (2008), pp. 379–392.
[22] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and
P. Doll ´ar, Focal loss for dense object detection ,
2018, https://arxiv.org/abs/1708.02002, https://
arxiv.org/abs/1708.02002.
[23] G. Mou and K. Lee , An effective, robust and
fairness-aware hate speech detection framework , in
2021 IEEE International Conference on Big Data
(Big Data), IEEE, Dec. 2021, https://doi.org/10.
1109/bigdata52589.2021.9672022, http://dx.doi.
org/10.1109/BigData52589.2021.9672022.
[24] S. S. Rambhatla and I. Misra , Selfeval: Lever-
aging the discriminative nature of generative models
for evaluation , 2023, https://arxiv.org/abs/2311.
10708, https://arxiv.org/abs/2311.10708.
[25] S. Raza, M. Garg, D. J. Reji, S. R. Bashir, and
C. Ding, Nbias: A natural language processing frame-
work for bias identification in text , Expert Systems
with Applications, 237 (2024), p. 121542.
[26] S. Raza, D. J. Reji, and C. Ding , Dbias: detect-
ing biases and ensuring fairness in news articles , In-
ternational Journal of Data Science and Analytics, 17
(2024), pp. 39–59.
[27] T. Spinde, M. Plank, J.-D. Krieger, T. Ruas,
B. Gipp, and A. Aizawa, Neural media bias detection
using distant supervision with babe–bias annotations by
experts, arXiv preprint arXiv:2209.14557, (2022).
[28] T. Sun, A. Gaut, S. Tang, Y. Huang, M. ElSh-
erief, J. Zhao, D. Mirza, E. Belding, K.-W.
Chang, and W. Y. Wang , Mitigating gender bias in
natural language processing: Literature review , arXiv
preprint arXiv:1906.08976, (2019).
[29] M. Vidgof, S. Bachhofner, and J. Mendling ,
Large language models for business process man-
agement: Opportunities and challenges , in Interna-
tional Conference on Business Process Management,
Springer, 2023, pp. 107–123.
[30] Y. Wu, B. Shi, J. Chen, Y. Liu, B. Dong,
Q. Zheng, and H. Wei, Rethinking sentiment analysis
under uncertainty, in Proceedings of the 32nd ACM In-
ternational Conference on Information and Knowledge
Management, 2023, pp. 2775–2784.
[31] J. Xu, M. Theune, and D. Braun , Leverag-
ing annotator disagreement for text classification ,
2024, https://arxiv.org/abs/2409.17577, https://
arxiv.org/abs/2409.17577.
[32] S. Zhang, Y. Shen, Z. Tan, Y. Wu, and W. Lu ,
De-bias for generative extraction in unified ner task ,
in Proceedings of the 60th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), 2022, pp. 808–818.
[33] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang,
Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong,
et al. , A survey of large language models , arXiv
preprint arXiv:2303.18223, (2023).
[34] I. Ziegler, A. K ¨oksal, D. Elliott, and
H. Sch ¨utze, Craft your dataset: Task-specific syn-
thetic dataset generation through corpus retrieval and
augmentation, 2024, https://arxiv.org/abs/2409.
02098, https://arxiv.org/abs/2409.02098.
Copyright © 2025 by SIAM
Unauthorized reproduction of this article is prohibited

