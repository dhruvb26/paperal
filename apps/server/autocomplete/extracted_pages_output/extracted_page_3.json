{
  "sections": [
    {
      "title": "Introduction to Dataset Inference",
      "content": "sequences or their clusters naturally occur. For instance, consider the Harry Potter series written by J.K. Rowling. Dataset inference tests whether a \u2018dataset\u2019 or a collection of paragraphs from her books was used for training a language model, rather than testing the membership of individual sentences alone. We also outline the specific framework required to operationalize dataset inference, including the necessary assumptions for the same.",
      "references": []
    },
    {
      "title": "Analysis on Known Data",
      "content": "We carry out our analysis of dataset inference using LLMs with known training and validation data. Specifically, we leverage the Pythia suite of models Biderman et al. [2023] trained on the Pile dataset Gao et al. [2020] (Section 5). This controlled experimental setup allows us to precisely analyze the model behavior on members and non-members when they occur IID (without any temporal shift) as the training and validation splits of PILE are publicly accessible. Across all subsets, dataset inference achieves p-values less than 0.1 in distinguishing between training and validation splits. At the same time, our method shows no false positives, with our statistical test producing p-values larger than 0.5 in all cases when comparing two subsets of validation data. To its practical merit, dataset inference requires only 1000 text sequences to detect whether a given suspect dataset was used to train an LLM.",
      "references": [
        {
          "in_text": "Biderman et al. [2023]",
          "complete_reference": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023."
        },
        {
          "in_text": "Gao et al. [2020]",
          "complete_reference": "Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020."
        }
      ]
    },
    {
      "title": "Background and Baselines",
      "content": "Membership Inference (MI) [Shokri et al., 2017]. The central question is: Given a trained model and a particular data point, can we determine if the data point was in the model\u2019s training set? Applications of MI methods span across detecting contamination in benchmark datasets [Oren et al., 2024, Shi et al., 2024], auditing privacy [Steinke et al., 2023], and identifying copyrighted texts within pre-training data [Shafran et al., 2021]. The field has been studied extensively in the realm of ML models trained via supervised learning on small datasets. The ability of membership inference in the context of large-scale language models (LLMs) remains an open problem. Recently, new methods [Mattern et al., 2023, Shi et al., 2024] have been proposed to close the gap and we present them in \u00a7 2.1.",
      "references": [
        {
          "in_text": "Shokri et al., 2017",
          "complete_reference": "R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3\u201318, Los Alamitos, CA, USA, may 2017. IEEE Computer Society."
        },
        {
          "in_text": "Oren et al., 2024",
          "complete_reference": "Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test set contamination for black-box language models. In The Twelfth International Conference on Learning Representations, 2024."
        },
        {
          "in_text": "Shi et al., 2024",
          "complete_reference": "Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024."
        },
        {
          "in_text": "Steinke et al., 2023",
          "complete_reference": "Thomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. In Thirty-seventh Conference on Neural Information Processing Systems, 2023."
        },
        {
          "in_text": "Shafran et al., 2021",
          "complete_reference": "Avital Shafran, Shmuel Peleg, and Yedid Hoshen. Membership inference attacks are easier on difficult problems. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 14820\u201314829, October 2021."
        },
        {
          "in_text": "Mattern et al., 2023",
          "complete_reference": "Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 11330\u201311343, Toronto, Canada, July 2023. Association for Computational Linguistics."
        }
      ]
    },
    {
      "title": "Dataset Inference",
      "content": "Dataset Inference [Maini et al., 2021] provides a strong statistical claim that a given model is a derivative of its own private training data. The key intuition behind the original method proposed for supervised learning is that classifiers maximize the distance of training examples from the model\u2019s decision boundaries, while the test examples are closer to the decision boundaries since they have no impact on the model weights. Subsequently, dataset inference was extended from supervised learning to the self-supervised learning (SSL) models [Dziedzic et al., 2022] based on the observation that representations of the training data points induce a significantly different distribution than the representation of the test data points. We introduce dataset inference for large language models to detect datasets used for training.",
      "references": [
        {
          "in_text": "Maini et al., 2021",
          "complete_reference": "Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution in machine learning. In International Conference on Learning Representations, 2021."
        },
        {
          "in_text": "Dziedzic et al., 2022",
          "complete_reference": "Adam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, Franziska Boenisch, and Nicolas Papernot. Dataset inference for self-supervised models. In NeurIPS (Neural Information Processing Systems), 2022."
        }
      ]
    },
    {
      "title": "Metrics for LLM Membership Inference",
      "content": "This section explores various metrics used to assess Membership Inference Attacks (MIAs) against LLMs. We study MIAs under gray-box access (which assumes access to the model loss, but not to parameters or gradients). The adversary aims to learn an attack function Af\u03b8 : X \u2192 {0, 1} that takes an input x from distribution X and determines whether x was in the training set Dtrain of the LM f\u03b8 or not. Let us now describe the MIAs we use in our work.",
      "references": []
    },
    {
      "title": "Thresholding Based Membership Inference Attacks",
      "content": "Thresholding Based. These MIAs leverage loss [Yeom et al., 2018] or perplexity [Carlini et al., 2021] as scores and then threshold them to classify samples as members or non-members. Specifically, the decision rule for membership is: Af\u03b8 (x) = 1[L(f\u03b8, x) < \u03b3], where \u03b3 is a selected pre-defined threshold. However, MIAs based solely on perplexity suffer from many false positives, where simple and predictable sequences that never occur in the training set can be labeled as members.",
      "references": [
        {
          "in_text": "Yeom et al., 2018",
          "complete_reference": "S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 268\u2013282, Los Alamitos, CA, USA, jul 2018. IEEE Computer Society."
        },
        {
          "in_text": "Carlini et al., 2021",
          "complete_reference": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24-3."
        }
      ]
    },
    {
      "title": "Conclusion",
      "content": "3",
      "references": []
    }
  ]
}