{
  "sections": [
    {
      "title": "Min-k% Prob",
      "content": "As a remedy to the problem of predictability, Shi et al. [2024] proposed the Min-k% Prob metric which evaluates the likelihood of the K% of tokens in x that have the lowest probability, conditioned on the preceding tokens. Hence, this MIA ignores highly predictable tokens in the suspect sequence. The membership prediction is made by thresholding the average negative log-likelihood of these tokens. The input sentence x is marked as included in pretraining data simply by thresholding the Min-k% Prob result: Af\u03b8 (x) = 1[Min-k% Prob(x) < \u03b3].",
      "references": [
        {
          "in_text": "[2024]",
          "complete_reference": "Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International Conference on Learning Representations, 2024."
        }
      ]
    },
    {
      "title": "Perturbation Based",
      "content": "The central hypothesis behind Perturbation-based MIAs is that a sample that an LLM saw during training should have a lower perplexity on its original version (x), as opposed to a perturbed version of the same (\u02dcx). Formally, the membership attack is defined as Af\u03b8 (x) = 1 [Pf\u03b8(x)/Pf\u03b8(\u02dcx) < \u03b3], for a threshold \u03b3. In our work, we investigate various forms of perturbations such as (1) white-space perturbation, (2) synonym substitution [Mattern et al., 2023], (3) character-level typos, (4) random deletion, and (5) changing character case.",
      "references": [
        {
          "in_text": "[Mattern et al., 2023]",
          "complete_reference": "Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Schoelkopf, Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. Membership inference attacks against language models via neighbourhood comparison. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 11330\u201311343, Toronto, Canada, July 2023. Association for Computational Linguistics."
        }
      ]
    },
    {
      "title": "DetectGPT",
      "content": "This is a special case of perturbation-based MIAs, originally designed to detect machine-generated text [Mitchell et al., 2023]. The key difference is that perturbations to the input are made using an external language model that infills randomly masked-out spans of the original input. It then compares the log-probability of x with expected value of the same from multiple infilled neighbors \u02dcxi.",
      "references": [
        {
          "in_text": "[Mitchell et al., 2023]",
          "complete_reference": "Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: zero-shot machine-generated text detection using probability curvature. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023."
        }
      ]
    },
    {
      "title": "Reference Model Based",
      "content": "These methods compare the perplexity ratio between a suspect model and a reference model on a given string. The suspect model may have seen the string during training, while the reference model has not. The corresponding MIA is: Af\u03b8 (x) = 1[L(f\u03b8, x) < L(f \u2032 \u03b8, x)], where f \u2032 reference model. In our work, we use the SILO [Min et al., 2023], Tinystories-33M [Eldan and Li, 2023], Tinystories-1M [Eldan and Li, 2023], and Phi-1.5 [Li et al., 2023] models as reference models. Notably, these models were not trained on general web data. In particular, the Phi-1.5 and Tinystories models were trained on synthetic data generated by GPT models, and the SILO model was trained on data that is freely licensed for training. \u03b8 is the",
      "references": [
        {
          "in_text": "[Min et al., 2023]",
          "complete_reference": "Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk in a nonparametric datastore. arXiv preprint arXiv:2308.04430, 2023."
        },
        {
          "in_text": "[Eldan and Li, 2023]",
          "complete_reference": "Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759, 2023."
        },
        {
          "in_text": "[Li et al., 2023]",
          "complete_reference": "Yuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023."
        }
      ]
    },
    {
      "title": "zlib Ratio",
      "content": "Another simple MI baseline uses the zlib library [Gailly and Adler, 2004], where a potential member has a low ratio of the model\u2019s perplexity to the entropy of the text, which is computed as the number of bits for the sequence when compressed with the zlib library: Af\u03b8 (x) = 1[Pf\u03b8 (x)/zlib(x) < \u03b3] [Carlini et al., 2021]. The idea is that a model trained on a dataset will have low perplexity for its members because it was optimized for them, unlike the zLib algorithm, which was not tailored to the training data.",
      "references": [
        {
          "in_text": "[Gailly and Adler, 2004]",
          "complete_reference": "Jean-loup Gailly and Mark Adler. zlib compression library. 2004."
        },
        {
          "in_text": "[Carlini et al., 2021]",
          "complete_reference": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24-3."
        }
      ]
    },
    {
      "title": "Problem Setup",
      "content": "LLMs train on trillions of tokens, and the sizes of the training sets are only likely to increase [Met, dbr]. To increase training efficiency (in terms of time, financial costs, and environmental impact), improve performance, and decrease the risk of privacy leakage, many LLM practitioners deduplicate their pre-training data [Biderman et al., 2023, Carlini et al., 2021, Lee et al., 2022]. In our work, we ask this question: How to detect if a given dataset was used to train an LLM? and propose the idea of dataset inference for LLMs.",
      "references": [
        {
          "in_text": "[Met, dbr]",
          "complete_reference": "Introducing meta llama 3: The most capable openly available llm to date, https://ai.meta.com/blog/meta- llama-3/."
        },
        {
          "in_text": "[Biderman et al., 2023]",
          "complete_reference": "Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML\u201923. JMLR.org, 2023."
        },
        {
          "in_text": "[Carlini et al., 2021]",
          "complete_reference": "Nicholas Carlini, Florian Tram\u00e8r, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, \u00dalfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633\u20132650. USENIX Association, August 2021. ISBN 978-1-939133-24-3."
        },
        {
          "in_text": "[Lee et al., 2022]",
          "complete_reference": "Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. In Smaranda Mure- san, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445, Dublin, Ire- land, May 2022. Association for Computational Linguistics."
        }
      ]
    },
    {
      "title": "Access Levels",
      "content": "In the black-box setting, we assume an input-output access to an LLM along with access to model loss, hence we are not allowed to inspect individual weights or hidden states (e.g., attention layer parameters) of the language model. This threat model is realistic in the case of LLM\u2019s users since many language models can be accessed through APIs that provide limited visibility into their inner workings. For instance, OpenAI [Ope] offers API access to GPT-3 and GPT-4, while Google [Gem] offers Gemini, without revealing the full architecture of the models or training methodology. The gray-box access, commonly assumed for MIAs, additionally assumes that we can obtain the perplexity or the loss values from an LLM,",
      "references": [
        {
          "in_text": "[Ope]",
          "complete_reference": "Openai, https://openai.com. URL https://openai.com/."
        },
        {
          "in_text": "[Gem]",
          "complete_reference": "Germini, https://gemini.google.com/. URL https://gemini.google.com/."
        }
      ]
    }
  ]
}