["Min-k% Prob. As a remedy to the problem of predictability, Shi et al. [2024] proposed the Min-k% Prob metric which evaluates the likelihood of the K% of tokens in x that have the lowest probability, conditioned on the preceding tokens. Hence, this MIA ignores highly predictable tokens in the suspect sequence. The membership prediction is made by thresholding the average negative log-likelihood of these tokens. The input sentence x is marked as included in pretraining data simply by thresholding the Min-k% Prob result: Af\u03b8 (x) = 1[Min-k% Prob(x) < \u03b3].", "Perturbation Based. The central hypothesis behind Perturbation-based MIAs is that a sample that an LLM saw during training should have a lower perplexity on its original version (x), as opposed to a perturbed version of the same (\u02dcx). Formally, the membership attack is defined as Af\u03b8 (x) = 1 [Pf\u03b8(x)/Pf\u03b8(\u02dcx) < \u03b3], for a threshold \u03b3. In our work, we investigate various forms of perturbations such as (1) white-space perturbation, (2) synonym substitution [Mattern et al., 2023], (3) character-level typos, (4) random deletion, and (5) changing character case.", "DetectGPT. This is a special case of perturbation-based MIAs, originally designed to detect machine- generated text [Mitchell et al., 2023]. The key difference is that perturbations to the input are made using an external language model that infills randomly masked-out spans of the original input. It then compares the log-probability of x with expected value of the same from multiple infilled neighbors \u02dcxi.", "Reference Model Based. These methods compare the perplexity ratio between a suspect model and a reference model on a given string. The suspect model may have seen the string during training, while the reference model has not. The corresponding MIA is: Af\u03b8 (x) = 1[L(f\u03b8, x) < L(f \u2032 \u03b8, x)], where f \u2032 reference model. In our work, we use the SILO [Min et al., 2023], Tinystories-33M [Eldan and Li, 2023], Tinystories-1M [Eldan and Li, 2023], and Phi-1.5 [Li et al., 2023] models as reference models. Notably, these models were not trained on general web data. In particular, the Phi-1.5 and Tinystories models were trained on synthetic data generated by GPT models, and the SILO model was trained on data that is freely licensed for training. \u03b8 is the", "zlib Ratio. Another simple MI baseline uses the zlib library [Gailly and Adler, 2004], where a potential member has a low ratio of the model\u2019s perplexity to the entropy of the text, which is computed as the number of bits for the sequence when compressed with the zlib library: Af\u03b8 (x) = 1[Pf\u03b8 (x)/zlib(x) < \u03b3] [Carlini et al., 2021]. The idea is that a model trained on a dataset will have low perplexity for its members because it was optimized for them, unlike the zLib algorithm, which was not tailored to the training data.", "3 Problem Setup", "LLMs train on trillions of tokens, and the sizes of the training sets are only likely to increase [Met, dbr]. To increase training efficiency (in terms of time, financial costs, and environmental impact), improve performance, and decrease the risk of privacy leakage, many LLM practitioners deduplicate their pre-training data [Biderman et al., 2023, Carlini et al., 2021, Lee et al., 2022]. In our work, we ask this question: How to detect if a given dataset was used to train an LLM? and propose the idea of dataset inference for LLMs.", "Access Levels. In the black-box setting, we assume an input-output access to an LLM along with access to model loss, hence we are not allowed to inspect individual weights or hidden states (e.g., attention layer parameters) of the language model. This threat model is realistic in the case of LLM\u2019s users since many language models can be accessed through APIs that provide limited visibility into their inner workings. For instance, OpenAI [Ope] offers API access to GPT-3 and GPT-4, while Google [Gem] offers Gemini, without revealing the full architecture of the models or training methodology. The gray-box access, commonly assumed for MIAs, additionally assumes that we can obtain the perplexity or the loss values from an LLM,", "4"]