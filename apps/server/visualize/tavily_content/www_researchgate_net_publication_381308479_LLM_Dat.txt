LLM Dataset Inference: Did you train on my dataset?
PreprintLLM Dataset Inference: Did you train on my dataset?June 2024DOI:10.48550/arXiv.2406.06443Authors: Pratyush MainiPratyush MainiThis person is not on ResearchGate, or hasn't claimed this research yet. Hengrui JiaHengrui JiaThis person is not on ResearchGate, or hasn't claimed this research yet. Nicolas PapernotNicolas PapernotThis person is not on ResearchGate, or hasn't claimed this research yet. Adam DziedzicAdam DziedzicThis person is not on ResearchGate, or hasn't claimed this research yet. Request file PDFTo read the file of this research, you can request a copy directly from the authors.Preprints and early-stage research may not have been peer reviewed yet.Request fileDownload citation Copy link Link copied Request file Download citation Copy link Link copiedTo read the file of this research, you can request a copy directly from the authors.Citations (1)References (29)AbstractThe proliferation of large language models (LLMs) in the real world has come with a rise in copyright cases against companies for training their models on unlicensed data from the internet. Recent works have presented methods to identify if individual text sequences were members of the model's training data, known as membership inference attacks (MIAs). We demonstrate that the apparent success of these MIAs is confounded by selecting non-members (text sequences not used for training) belonging to a different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared with ones used to train the model). This distribution shift makes membership inference appear successful. However, most MIA methods perform no better than random guessing when discriminating between members and non-members from the same distribution (e.g., in this case, the same period of time). Even when MIAs work, we find that different MIAs succeed at inferring membership of samples from different distributions. Instead, we propose a new dataset inference method to accurately identify the datasets used to train large language models. This paradigm sits realistically in the modern-day copyright landscape, where authors claim that an LLM is trained over multiple documents (such as a book) written by them, rather than one particular paragraph. While dataset inference shares many of the challenges of membership inference, we solve it by selectively combining the MIAs that provide positive signal for a given distribution, and aggregating them to perform a statistical test on a given dataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile with statistically significant p-values < 0.1, without any false positives. Discover the world's research25+ million members160+ million publication pages2.3+ billion citationsJoin for freeNo file available To read the file of this research, you can request a copy directly from the authors.Request file PDFCitations (1)References (29)... The application of MIAs on LLMs has faced discouraging results so far. Initial claims of success Meeus et al., 2024a;Carlini et al., 2021;Mattern et al., 2023) were later disproven by Duan et al. (2024); Das et al. (2024); Maini et al. (2024); Meeus et al. (2024b). Before their work, it was common practice to select true non-members from documents created after the LLM's cut-off date.  ...... They showed that this approach allowed MIA methods to exploit temporal cues, rather than identifying membership through the model's inherent response characteristics. Duan et al. (2024); Das et al. (2024); Maini et al. (2024) introduced a new evaluation method based on an independent, identically distributed (IID) split between true members and non-members. Since adopting this method, no further MIA studies on LLMs have shown performance significantly better than random.  ...... Since adopting this method, no further MIA studies on LLMs have shown performance significantly better than random. Reported membership detection has remained below 60% AUROC, close to the random-chance level of 50% AUROC (Duan et al., 2024;Das et al., 2024;Maini et al., 2024;Xie et al., 2024;Zhang et al., 2024b).  ...Scaling Up Membership Inference: When and How Attacks Succeed on Large Language ModelsPreprintFull-text availableOct 2024Haritz Puerto Martin Gubri Sangdoo Yun Seong Joon OhMembership inference attacks (MIA) attempt to verify the membership of a given data sample in the training set for a model. MIA has become relevant in recent years, following the rapid development of large language models (LLM). Many are concerned about the usage of copyrighted materials for training them and call for methods for detecting such usage. However, recent research has largely concluded that current MIA methods do not work on LLMs. Even when they seem to work, it is usually because of the ill-designed experimental setup where other shortcut features enable "cheating." In this work, we argue that MIA still works on LLMs, but only when multiple documents are presented for testing. We construct new benchmarks that measure the MIA performances at a continuous scale of data samples, from sentences (n-grams) to a collection of documents (multiple chunks of tokens). To validate the efficacy of current MIA approaches at greater scales, we adapt a recent work on Dataset Inference (DI) for the task of binary membership detection that aggregates paragraph-level MIA features to enable MIA at document and collection of documents level. This baseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.ViewShow abstractUnveiling Security, Privacy, and Ethical Concerns of ChatGPTArticleOct 2023Xiaodong WuRan DuanJianbing NiViewMembership Inference Attacks against Language Models via Neighbourhood ComparisonConference PaperJan 2023Justus Mattern Fatemehsadat MireshghallahZhijing Jin Taylor Berg-KirkpatrickViewMembership Inference Attacks From First PrinciplesConference PaperMay 2022Nicholas CarliniSteve ChienMilad Nasr Florian TramerViewDeduplicating Training Data Makes Language Models BetterConference PaperJan 2022 Katherine LeeDaphne IppolitoAndrew NystromNicholas CarliniViewMembership Inference Attacks are Easier on Difficult ProblemsConference PaperOct 2021Avital Shafran Shmuel Peleg Yedid HoshenViewCombining P-Values Via AveragingArticleJan 2018Vladimir Vovk Ruodu WangViewzlib compression libraryArticleDec 2004Jean-loup GaillyMark Adler(taken from http://www.gzip.org/ on 2004-12-01)
zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library for use on virtually any computer hardware and operating system. The zlib data format is itself portable across platforms. Unlike the LZW compression method used in Unix compress(1) and in the GIF image format, the compression method currently used in zlib essentially never expands the data. (LZW can double or triple the file size in extreme cases.) zlib's memory footprint is also independent of the input data and can be reduced, if necessary, at some cost in compression. A more precise, technical discussion of both points is available on another page.
zlib was written by Jean-loup Gailly (compression) and Mark Adler (decompression). Jean-loup is also the primary author/maintainer of gzip(1), the author of the comp.compression FAQ list and the former maintainer of Info-ZIP's Zip; Mark is also the author of gzip's and UnZip's main decompression routines and was the original author of Zip. Not surprisingly, the compression algorithm used in zlib is essentially the same as that in gzip and Zip, namely, the `deflate' method that originated in PKWARE's PKZIP 2.x.
Greg, Mark and/or Jean-loup will add some more stuff here when they think of something to add. For now this page is mainly a pointer to zlib itself and to the official zlib and deflate documentation. Note that the specifications both achieved official Internet RFC status in May 1996, and zlib itself was adopted by JavaSoft in version 1.1 of the Java Development Kit (JDK), both as a raw class and as a component of the JAR archive format. 
See also:
http://java.sun.com/j2se/1.4.2/docs/api/java/util/zip/package-summary.htmlViewShow abstractA Method for Combining Non-Independent, One-Sided Tests of SignificanceArticleDec 1975Morton B. BrownLittell and Folks [1971, 1973] show that Fisher's method of combining independent tests of significance is asymptotically optimal among essentially all methods of combining independent tests. By assuming a joint multivariate normal density for the variables, we approximate the distribution of Fisher's statistic in order to combine one-sided tests of location when all the variables are not jointly independent. The probability associated with this test is simpler to evaluate than that of the equivalent likelihood ratio test.ViewShow abstractRandom Variables with Maximum SumsArticleSep 1982 Ludger RueschendorfMotivated by a problem in PERT networks we consider the question of construction of random variables with maximum sums when the marginals are fixed.ViewShow abstractPosterior Predictive p-ValuesArticleSep 1994ANN STATXiao-Li MengExtending work of D. B. Rubin [ibid. 12, 1151-1172 (1984; Zbl 0555.62010)] this paper explores a Bayesian counterpart of the classical p-value, namely, a tail-area probability of a “test statistic” under a null hypothesis. The Bayesian formulation, using posterior predictive replications of the data, allows a “test statistic” to depend on both data and unknown (nuisance) parameters and thus permits a direct measure of the discrepancy between sample and population quantities. The tail- area probability for a “test statistic” is then found under the joint posterior distribution of replicate data and the (nuisance) parameters, both conditional on the null hypothesis. This posterior predictive p- value can also be viewed as the posterior mean of a classical p-value, averaging over the posterior distribution of (nuisance) parameters under the null hypothesis, and thus it provides one general method for dealing with nuisance parameters. Two classical examples, including the Behrens-Fisher problem, are used to illustrate the posterior predictive p-value and some of its interesting properties, which also reveal a new (Bayesian) interpretation for some classical p-values. An application to multiple-imputation inference is also presented. A frequency evaluation shows that, in general, if the replication is defined by new (nuisance) parameters and new data, then the Type I frequentist error of an α-level posterior predictive test is often close to but less than α and will never exceed 2α.ViewShow abstractCombining dependent P-valuesArticleNov 2002STAT PROBABIL LETTJames T. KostMichael P McDermottWe derive an approximation to the null distribution of Fisher's statistic for combining p-values when the underlying test statistics are jointly distributed as multivariate t with common denominator. Applications to testing problems involving order-restricted parameters are briefly discussed.ViewShow abstractFast-detectGPT: Efficient zero-shot detection of machine-generated text via conditional probability curvatureJan 2024Guangsheng BaoYanbin ZhaoZhiyang TengLinyi YangYue ZhangGuangsheng Bao, Yanbin Zhao, Zhiyang Teng, Linyi Yang, and Yue Zhang. Fast-detectGPT: Efficient 
zero-shot detection of machine-generated text via conditional probability curvature. In The Twelfth 
International Conference on Learning Representations, 2024. URL https://openreview.net/forum? 
id=Bpcgcr8E8Z.Pythia: a suite for analyzing large language models across training and scalingJan 2023Stella BidermanHailey SchoelkopfQuentin AnthonyHerbie BradleyO' KyleEric BrienMohammad Aflah HallahanShivanshu KhanPurohitEdward Usvsn Sai PrashanthAviya RaffLintang SkowronOskar SutawikaVan DerWalStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, 
Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, 
Lintang Sutawika, and Oskar Van Der Wal. Pythia: a suite for analyzing large language models across 
training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML'23. 
JMLR.org, 2023.Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin RaffelAug 20212633-2650Nicholas CarliniFlorian TramèrEric WallaceMatthew JagielskiAriel Herbert-VossKatherine LeeAdam RobertsTom BrownNicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, 
Adam Roberts, Tom Brown, Dawn Song, Úlfar Erlingsson, Alina Oprea, and Colin Raffel. Extracting 
training data from large language models. In 30th USENIX Security Symposium (USENIX Security 
21), pages 2633-2650. USENIX Association, August 2021. ISBN 978-1-939133-24-3. URL https: 
//www.usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.Flocks of stochastic parrots: Differentially private prompt learning for large language models2023Haonan DuanAdam DziedzicNicolas PapernotFranziska BoenischHaonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska Boenisch. Flocks of stochastic parrots: 
Differentially private prompt learning for large language models. In Thirty-seventh Conference on Neural 
Information Processing Systems (NeurIPS), 2023.Do membership inference attacks work on large language models?Jan 2024Michael DuanAnshuman SuriNiloofar MireshghallahSewon MinWeijia ShiLuke ZettlemoyerYulia TsvetkovYejin ChoiDavid EvansHannaneh HajishirziMichael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi, Luke Zettlemoyer, Yulia 
Tsvetkov, Yejin Choi, David Evans, and Hannaneh Hajishirzi. Do membership inference attacks work on 
large language models? 2024.Yannis Cattan, Franziska Boenisch, and Nicolas Papernot. Dataset inference for self-supervised models2022Adam DziedzicHaonan DuanMuhammad Ahmad KaleemNikita DhawanJonas GuanAdam Dziedzic, Haonan Duan, Muhammad Ahmad Kaleem, Nikita Dhawan, Jonas Guan, Yannis Cattan, 
Franziska Boenisch, and Nicolas Papernot. Dataset inference for self-supervised models. In NeurIPS 
(Neural Information Processing Systems), 2022.Tinystories: How small can language models be and still speak coherent english?Jan 2023Ronen EldanYuanzhi LiRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent 
english? arXiv preprint arXiv:2305.07759, 2023.The pile: An 800gb dataset of diverse text for language modelingJan 2020Leo GaoStella BidermanSid BlackLaurence GoldingTravis HoppeCharles FosterJason PhangHorace HeAnish ThiteNoa NabeshimaLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace 
He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. 
arXiv preprint arXiv:2101.00027, 2020.Jan 2023Yuanzhi LiSébastien BubeckRonen EldanAllie Del GiornoSuriya GunasekarYin Tat LeeYuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 
Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463, 2023.Dataset inference: Ownership resolution in machine learningJan 2021Pratyush MainiMohammad YaghiniNicolas PapernotPratyush Maini, Mohammad Yaghini, and Nicolas Papernot. Dataset inference: Ownership resolution 
in machine learning. In International Conference on Learning Representations, 2021. URL https: 
//openreview.net/forum?id=hvdKKV2yt7T.Silo language models: Isolating legal risk in a nonparametric datastoreJan 2023Sewon MinSuchin GururanganEric WallaceHannaneh HajishirziA NoahLuke SmithZettlemoyerSewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi, Noah A Smith, and Luke Zettlemoyer. 
Silo language models: Isolating legal risk in a nonparametric datastore. arXiv preprint arXiv:2308.04430, 
2023.Detectgpt: zero-shot machine-generated text detection using probability curvatureJan 2023Eric MitchellYoonho LeeAlexander KhazatskyChristopher D ManningChelsea FinnEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. Detectgpt: 
zero-shot machine-generated text detection using probability curvature. In Proceedings of the 40th 
International Conference on Machine Learning, ICML'23. JMLR.org, 2023.Proving test set contamination for black-box language modelsJan 2024Yonatan OrenNicole MeisterNiladri S ChatterjiFaisal LadhakTatsunori HashimotoYonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. Proving test 
set contamination for black-box language models. In The Twelfth International Conference on Learning 
Representations, 2024. URL https://openreview.net/forum?id=KS8mIvetg2.Beyond fair use: Legal risk evaluation for training llms on copyrighted textJan 2023Noorjahan RahmanEduardo SantacanaNoorjahan Rahman and Eduardo Santacana. Beyond fair use: Legal risk evaluation for training llms on 
copyrighted text. 2023. URL https://genlaw.org/CameraReady/57.pdf.Detecting pretraining data from large language modelsJan 2024Weijia ShiAnirudh AjithMengzhou XiaYangsibo HuangDaogao LiuTerra BlevinsDanqi ChenLuke ZettlemoyerWeijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and 
Luke Zettlemoyer. Detecting pretraining data from large language models. In The Twelfth International 
Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=zWqr3MQuNs.Membership inference attacks against machine learning modelsMay 20173-18R ShokriM StronatiC SongV ShmatikovR. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership inference attacks against machine learning 
models. In 2017 IEEE Symposium on Security and Privacy (SP), pages 3-18, Los Alamitos, CA, USA, 
may 2017. IEEE Computer Society. doi: 10.1109/SP.2017.41. URL https://doi.ieeecomputersociety. 
org/10.1109/SP.2017.41.Privacy auditing with one (1) training runJan 2023Thomas SteinkeMilad NasrMatthew JagielskiThomas Steinke, Milad Nasr, and Matthew Jagielski. Privacy auditing with one (1) training run. In 
Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview. 
net/forum?id=f38EY21lBw.Privacy risk in machine learning: Analyzing the connection to overfittingJul 2018268-282S YeomI GiacomelliM FredriksonS JhaS. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha. Privacy risk in machine learning: Analyzing the 
connection to overfitting. In 2018 IEEE 31st Computer Security Foundations Symposium (CSF), pages 
268-282, Los Alamitos, CA, USA, jul 2018. IEEE Computer Society. doi: 10.1109/CSF.2018.00027. URL 
https://doi.ieeecomputersociety.org/10.1109/CSF.2018.00027.Recommended publicationsDiscover morePreprintFull-text availableWard: Provable RAG Dataset Inference via LLM WatermarksOctober 2024Nikola JovanovićRobin StaabMaximilian Baader[...]Martin VechevRetrieval-Augmented Generation (RAG) improves LLMs by enabling them to incorporate external data during generation. This raises concerns for data owners regarding unauthorized use of their content in RAG systems. Despite its importance, the challenge of detecting such unauthorized usage remains underexplored, with existing datasets and methodologies from adjacent fields being ill-suited for its ... [Show full abstract] study. In this work, we take several steps to bridge this gap. First, we formalize this problem as (black-box) RAG Dataset Inference (RAG-DI). To facilitate research on this challenge, we further introduce a novel dataset specifically designed for benchmarking RAG-DI methods under realistic conditions, and propose a set of baseline approaches. Building on this foundation, we introduce Ward, a RAG-DI method based on LLM watermarks that enables data owners to obtain rigorous statistical guarantees regarding the usage of their dataset in a RAG system. In our experimental evaluation, we show that Ward consistently outperforms all baselines across many challenging settings, achieving higher accuracy, superior query efficiency and robustness. Our work provides a foundation for future studies of RAG-DI and highlights LLM watermarks as a promising approach to this problem.View full-textPreprintFull-text availableDataset Inference: Ownership Resolution in Machine LearningApril 2021Pratyush MainiMohammad Yaghini[...] Nicolas PapernotWith increasingly more data and computation involved in their training, machine learning models constitute valuable intellectual property. This has spurred interest in model stealing, which is made more practical by advances in learning with partial, little, or no supervision. Existing defenses focus on inserting unique watermarks in a model's decision surface, but this is insufficient: the ... [Show full abstract] watermarks are not sampled from the training distribution and thus are not always preserved during model stealing. In this paper, we make the key observation that knowledge contained in the stolen model's training set is what is common to all stolen copies. The adversary's goal, irrespective of the attack employed, is always to extract this knowledge or its by-products. This gives the original model's owner a strong advantage over the adversary: model owners have access to the original training data. We thus introduce dataset inference, the process of identifying whether a suspected model copy has private knowledge from the original model's dataset, as a defense against model stealing. We develop an approach for dataset inference that combines statistical testing with the ability to estimate the distance of multiple data points to the decision boundary. Our experiments on CIFAR10, SVHN, CIFAR100 and ImageNet show that model owners can claim with confidence greater than 99% that their model (or dataset as a matter of fact) was stolen, despite only exposing 50 of the stolen model's training points. Dataset inference defends against state-of-the-art attacks even when the adversary is adaptive. Unlike prior work, it does not require retraining or overfitting the defended model.View full-textPreprintFull-text availableBuilding pre-train LLM Dataset for the INDIC Languages: a case study on HindiJuly 2024 Shantipriya ParidaShakshi PanwarKusum Lata[...]Sambit SekharLarge language models (LLMs) demonstrated transformative capabilities in many applications that require automatically generating responses based on human instruction. However, the major challenge for building LLMs, particularly in Indic languages, is the availability of high-quality data for building foundation LLMs. In this paper, we are proposing a large pre-train dataset in Hindi useful for ... [Show full abstract] the Indic language Hindi. We have collected the data span across several domains including major dialects in Hindi. The dataset contains 1.28 billion Hindi tokens. We have explained our pipeline including data collection, pre-processing, and availability for LLM pre-training. The proposed approach can be easily extended to other Indic and low-resource languages and will be available freely for LLM pre-training and LLM research purposes.View full-textArticleFull-text available“Você precisa criar vínculos com quem você treina”"You need to bond with the ones you train": Mistur...June 2024 · Novos Debates Iris Wallenburg Jeannette Pols[...] Antoinette de BontEste artigo aborda a reforma contemporânea da educação médica de pós-graduação que visa padronizar o treinamento. As reformas são orientadas por intervenções de políticas públicas para aumentar a qualidade do atendimento, objetivar o desempenho e preparar os residentes para as mudanças nas necessidades de assistência médica. Este artigo se baseia em estudos etnográficos feitos nos Países Baixos, ... [Show full abstract] estudando como novos padrões de treinamento foram incorporados ao treinamento diário de residência em ginecologia e cirurgia. Percebendo a ciência da Educação como uma nova cultura epistêmica ao lado da cultura epistêmica tradicional, baseada na autoridade profissional, o artigo examina como ambas as culturas epistêmicas se entrelaçaram, fabricando uma nova cultura de treinamento que reúne tanto elementos tradicionais quanto “novos”.View full-textLast Updated: 11 Sep 2024Discover the world's researchJoin ResearchGate to find the people and research you need to help your work.Join for free ResearchGate iOS AppGet it from the App Store now.InstallKeep up with your stats and moreAccess scientific knowledge from anywhere orDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with GoogleWelcome back! Please log in.Email · HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with GoogleNo account? Sign upCompanyAbout usNewsCareersSupportHelp CenterBusiness solutionsAdvertisingRecruiting© 2008-2025 ResearchGate GmbH. All rights reserved.TermsPrivacyCopyrightImprintConsent preferences