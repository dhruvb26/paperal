OpenELM: An Efficient Language Model Family with Open Training and Inference Framework - Apple Machine Learning ResearchMachine Learning ResearchOpen MenuClose MenuOverviewResearchEventsWork with usresearch area Speech and Natural Language Processing | Workshop at ICMLcontent type paper | published April 2024OpenELM: An Efficient Language Model Family with Open Training and Inference FrameworkAuthorsSachin Mehta, Mohammad Sekhavat, Qingqing Cao, Max Horton, Yanzi Jin, Frank Sun, Iman Mirzadeh, Mahyar Najibikohnehshahri, Dmitry Belenko, Peter Zatloukal, Mohammad RastegariView publicationView source code (GitHub)Copy BibtexThis paper has been accepted at the Efficient Systems for Foundation Models workshop at ICML 2024.
The reproducibility and transparency of large language models are crucial for advancing open research, ensuring the trustworthiness of results, and enabling investigations into data and model biases, as well as potential risks. To this end, we release OpenELM, a state-of-the-art open language model. OpenELM uses a layer-wise scaling strategy to efficiently allocate parameters within each layer of the transformer model, leading to enhanced accuracy. For example, with a parameter budget of approximately one billion parameters, OpenELM exhibits a 2.36% improvement in accuracy compared to OLMo while requiring 2 times fewer pre-training tokens.
Diverging from prior practices that only provide model weights and inference code, and pre-train on private datasets, our release includes the complete framework for training and evaluation of the language model on publicly available datasets, including training logs, multiple checkpoints, and pre-training configurations. We also release code to convert models to MLX library for inference and fine-tuning on Apple devices. This comprehensive release aims to empower and strengthen the open research community, paving the way for future open research endeavors.Related readings and updates.Scaling Smart: Accelerating Large Language Model Pre-training with Small Model InitializationThis paper was accepted at the Efficient Natural Language and Speech Processing (ENLSP) Workshop at NeurIPS 2024.
The pre-training phase of language models often begins with randomly initialized parameters. With the current trends in scaling models, training their large number of parameters can be extremely slow and costly. In contrast, small language models are less expensive to train, but they often cannot achieve the accuracy of large models…See paper detailsPre-trained Language Models Do Not Help Auto-regressive Text-to-Image GenerationThis paper was accepted at the workshop I Can’t Believe It’s Not Better! (ICBINB) at NeurIPS 2023.
Recent advances in image tokenizers, such as VQ-VAE, have enabled text-to-image generation using auto-regressive methods, similar to language modeling. However, these methods have yet to leverage pre-trained language models, despite their adaptability to various downstream tasks. In this work, we explore this gap, and find that pre-trained language…See paper detailsDiscover opportunities in Machine Learning.Our research in machine learning breaks new ground every day.Work with usMachine Learning ResearchResearchOpenELM: An Efficient Language Model Family with Open Training and Inference FrameworkPrivacy PolicyTerms of UseLegalCopyright © 2024 Apple Inc. All rights reserved.