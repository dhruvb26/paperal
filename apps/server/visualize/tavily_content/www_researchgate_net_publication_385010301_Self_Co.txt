(PDF) Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Models
PreprintPDF AvailableSelf-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language ModelsOctober 2024DOI:10.48550/arXiv.2410.13088LicenseCC BY 4.0Authors: Jie RenJie RenThis person is not on ResearchGate, or hasn't claimed this research yet. Kangrui ChenKangrui ChenThis person is not on ResearchGate, or hasn't claimed this research yet. Chen ChenNanjing Tech University Vikash SehwagIndian Institute of Technology KharagpurShow all 7 authorsHide Download file PDFRead filePreprints and early-stage research may not have been peer reviewed yet.Download file PDFRead fileDownload citation Copy link Link copied Read file Download citation Copy link Link copiedReferences (53)Figures (5)Abstract and FiguresLarge Language Models (LLMs) and Vision-Language Models (VLMs) have made significant advancements in a wide range of natural language processing and vision-language tasks. Access to large web-scale datasets has been a key factor in their success. However, concerns have been raised about the unauthorized use of copyrighted materials and potential copyright infringement. Existing methods, such as sample-level Membership Inference Attacks (MIA) and distribution-based dataset inference, distinguish member data (data used for training) and non-member data by leveraging the common observation that models tend to memorize and show greater confidence in member data. Nevertheless, these methods face challenges when applied to LLMs and VLMs, such as the requirement for ground-truth member data or non-member data that shares the same distribution as the test data. In this paper, we propose a novel dataset-level membership inference method based on Self-Comparison. We find that a member prefix followed by a non-member suffix (paraphrased from a member suffix) can further trigger the model's memorization on training data. Instead of directly comparing member and non-member data, we introduce paraphrasing to the second half of the sequence and evaluate how the likelihood changes before and after paraphrasing. Unlike prior approaches, our method does not require access to ground-truth member data or non-member data in identical distribution, making it more practical. Extensive experiments demonstrate that our proposed method outperforms traditional MIA and dataset inference techniques across various datasets and models, including including public models, fine-tuned models, and API-based commercial models. An example of using DDI‚Ä¶¬† The change of distribution and √≠ ¬µ√≠¬±¬ù-values before and after √≠ ¬µ√≠¬±¬Ñ is paraphrased.‚Ä¶¬† Different memorization results between paraphrasing the whole sequence and half of the sequence.‚Ä¶¬† The framework of Self-Comparison‚Ä¶¬† Three regions for the threshold of sample-level MIAs Figure 6: F1 score of SMI on different sample sizes Figure 7: F1 score of SMI on different margin values Figure 8: √≠ ¬µ√≠¬±¬ù-values of SMI on different model sizes‚Ä¶¬†Figures - available via license: Creative Commons Attribution 4.0 InternationalContent may be subject to copyright. Discover the world's research25+ million members160+ million publication pages2.3+ billion citationsJoin for freePublic Full-text 1Available via license: CC BY 4.0Content may be subject to copyright. 
Self-Comparison for Dataset-Level Membership Inference inLarge (Vision-)Language ModelsJie Renrenjie3@msu.eduMichigan State UniversityKangrui Chenchenkan4@msu.eduMichigan State UniversityChen ChenChenA.Chen@sony.comSony AIVikash Sehwagvikash.sehwag@sony.comSony AIYue Xingxingyue1@msu.eduMichigan State UniversityJiliang Tangtangjili@msu.eduMichigan State UniversityLingjuan Lyulingjuan.lv@sony.comSony AIAbstractLarge Language Models (LLMs) and Vision-Language Models (VLMs)have made signiÓòõcant advancements in a wide range of naturallanguage processing and vision-language tasks. Access to largeweb-scale datasets has been a key factor in their success. How-ever, concerns have been raised about the unauthorized use ofcopyrighted materials and potential copyright infringement. Exist-ing methods, such as sample-level Membership Inference Attacks(MIA) and distribution-based dataset inference, distinguish memberdata (data used for training) and non-member data by leveragingthe common observation that models tend to memorize and showgreater conÓòõdence in member data. Nevertheless, these methodsface challenges when applied to LLMs and VLMs, such as the re-quirement for ground-truth member data or non-member data thatshares the same distribution as the test data. In this paper, we pro-pose a novel dataset-level membership inference method basedon Self-Comparison. We Óòõnd that a member preÓòõx followed by anon-member suÓòúx (paraphrased from a member suÓòúx) can fur-ther trigger the model‚Äôs memorization on training data. Instead ofdirectly comparing member and non-member data, we introduceparaphrasing to the second half of the sequence and evaluate howthe likelihood changes before and after paraphrasing. Unlike priorapproaches, our method does not require access to ground-truthmember data or non-member data in identical distribution, makingit more practical. Extensive experiments demonstrate that our pro-posed method outperforms traditional MIA and dataset inferencetechniques across various datasets and models, including includ-ing public models, Óòõne-tuned models, and API-based commercialmodels.1 IntroductionLarge Language Models (LLMs) [2,43,45] and Vision-LanguageModels (VLMs) [5,26,48] have demonstrated remarkable capa-bilities in understanding, reasoning, and generating both textualand visual data. These advancements have signiÓòõcantly impactednatural language processing (NLP) tasks such as machine trans-lation [42], text summarization [13], and sentiment analysis [32],as well as vision-language tasks like image captioning [17], visualquestion answering [28], and cross-modal retrieval [24]. The rapiddevelopment of these models has been fueled by the availability oflarge web-scale datasets, which have substantially improved theirperformance. However, concerns are raised about unauthorizeddata usage. Copyrighted materials may be included in trainingdatasets, potentially infringing upon the rights of content creatorsand causing Óòõnancial loss to them [27,36,50]. For example, theNew York Times sued OpenAI and Microsoft over the use of copy-righted work for training models1. These datasets often requiresubstantial resources to construct by the companies, e.g., the NewYork Times, and these companies typically do not disclose themfor model training purposes. Even when some datasets are open-sourced [14,35,38], their usage is typically restricted by licensesand is limited to educational and research purposes. Therefore, it iscrucial to safeguard these datasets against unauthorized use.To protect the datasets, it is crucial to detect their usage in train-ing. Membership Inference Attack (MIA) [39,40] is a widely usedmethod in evaluating training data leakage. However, most exist-ing research focuses on sample-level inference. When applied todataset-level inference, MIA faces two signiÓòõcant challenges. First,current large models typically train for only one or a few epochson extensive web-scale datasets [4,6,9,15,21,22,46]. As a result,each sample is encountered only a limited number of times, whichrestricts its contribution to the training process. Consequently, thismakes inference on individual samples quite challenging [12,29].Existing methods demonstrate limited eÓòùectiveness in sample-levelinference. Second, MIA often relies on a strong assumption: priorknowledge of a set of ground-truth member data. It is required ei-ther explicitly [40] or implicitly [39]. SpeciÓòõcally, in some methods,e.g., [39,49], the ground-truth data is used to determine the decisionthreshold for the proposed MIA method, implicitly imposing theprior knowledge assumption.To address aforementioned challenges in sample-level MIA, Mainiet al. [29]propose to aggregate the membership information of in-dividuals by using dataset distributions. They determine whether adataset is member data based on the assumption that the distribu-tion of MIA features, such as likelihood, for member data shouldsigniÓòõcantly diÓòùer from non-member data. However, this approachis eÓòùective only when a validation set, which is non-member and1https://www.nytimes.com/2023/12/27/business/media/new-york-times-open-ai-microsoft-lawsuit.htmlarXiv:2410.13088v1  [cs.LG]  16 Oct 2024
Trovato et al.has the identical distribution to the protected dataset, is available.In our preliminary study of Section 3.2, we Óòõnd that even a smalldistribution shift between the validation data and the protected datacan result in false positive detection. This highlights the necessityfor improvement when there is no access to the non-member datathat follows the same distribution as the protected data.To tackle the above challenges, we propose Self-ComparisonMembership Inference (SMI) for dataset-level inference. Instead ofdirectly comparing the distributions of two sets as in [29], we focuson comparing how these distributions change under paraphrasing.Intuitively, a model should be more conÓòõdent when generatingmember data than non-member data [10,31,39,49], reÓòûected inhigher likelihood scores. For non-member data, since the modelhas not seen either the original or paraphrased data during train-ing, the likelihood will remain relatively stable before and afterparaphrasing. In contrast, for member data, the model has encoun-tered the original data but not its paraphrased version, leading to amore signiÓòõcant likelihood change after paraphrasing. Our methoddoes not rely on the assumption of access to ground-truth memberdata or non-member data that follows the same distribution as theprotected data. Instead, it only requires an auxiliary non-memberset which is not necessary to be in the same distribution. It can beeasily obtained by synthesizing data or using data published afterthe release of the suspect model. In addition, our approach doesnot require white-box access to the model‚Äôs internal parametersor architecture. It only requires access to model outputs such aslogits or log probabilities, making it widely applicable even whenminimal information about the model is available.We conduct a comprehensive evaluation across various LLMsand LVMs, including publicly available model checkpoints withwell-documented training data, such as Pythia [6], GPT-Neo [7],LLaVA [26], and CogVLM [48]. We also validate our approach onmodels that we Óòõne-tuned. We further apply our method to verifythe membership of well-known books on API-based models likeGPT-4o [1]. Our extensive results demonstrate that our methodoutperforms existing techniques when no prior knowledge of theground-truth member data is available.2 Related worksSample-level MIA. MIA is Óòõrst proposed to evaluate the dataleakage of individual samples by Shokri et al. [40], and is extensivelystudied in classiÓòõcation models [19,33,37,47,52]. For LLMs, recentworks propose metrics based on a core assumption that the modelwould assign higher prediction conÓòõdence to the training data [10,31,39,49,51]. For example, Carlini et al. [10]demonstrate thatmember data usually has lower perplexity (greater conÓòõdence)than non-member data, and improve the detection using zlib ratio.Shi et al. [39]claim that non-member data would contain someoutlier tokens with extremely low probability, and use the tokensofùëò% smallest likelihood to infer the membership. Mattern et al.[31]substitute synonym to evaluate the conÓòõdence in the replacedtokens. Reference-based methods [49] compare the likelihood witha reference model that is not trained on the target data. However,sample-level MIA requires ground-truth member data to assist thedetection, such as training a shadow model [40] or determining athreshold [10, 31, 39, 49], which limits the practicality.Dataset inference. Dataset inference is designed for a diÓòùerentlevel of membership inference. It considers from the perspectiveof distribution. Maini et al. [30]propose that training data is moredistant from the decision boundaries in classiÓòõcation models. Mainiet al. [29]extend the dataset inference to LLMs. They assume thatthe training should bring a distribution shift between member dataand non-member data, and propose to use hypothesis testing formembership inference. However, their method requires ground-truth member and non-member data to train a regressor and avalidation set to infer membership.3 Preliminary StudiesAs mentioned above, models tend to produce more conÓòõdent pre-dictions on member data [40,50]. The more frequently a modelencounters a sample during training, the more conÓòõdent the pre-diction is. In extreme cases, this results in the memorization eÓòùectin generative models such as language models and diÓòùusion mod-els [10,54]. Particularly, if a data sample is duplicated many times,the model can memorize and re-generate it [20]. Even without fullmemorization, training on member samples can increase a model‚ÄôsconÓòõdence in those samples, which can be considered a form ofweak memorization [10, 29, 51].Based on this memorization, sample-level MIA and dataset in-ference are proposed to detect member data. In Section 3.2, we Óòõrstpresent the gap in the existing methods, such as distribution-baseddataset inference (DDI) [29], by comparing two datasets. We showthat a non-member dataset is also possible to be diÓòùerent from thevalidation set, leading to false positive detection in DDI. To furtherimprove dataset-level membership inference, we demonstrate thepossibility to Óòõll in the gap by comparing the change of distributionafter applying a paraphrase. Then in Section 3.3, we conduct anempirical study showing that a preÓòõx sequence can further triggerthe weak memorization and amplify the change of conÓòõdence.3.1 DeÓòõnitionsIn this subsection, we deÓòõne the problem and some key concepts.Problem deÓòõnition. Modelùëìis an auto-regressive generationmodel that takes a sequence of text and vision tokens as inputand outputs a probability distribution predicting the next token,which is a text token. For a given datasetùëÑ, the goal of dataset-levelmembership inference is to determine whetherùëÑwas included inthe training ofùëì. We nameùëÑas candidate set, andùëìas suspectmodel. ùëÑis composed of token sequences in varying lengths.Prediction conÓòõdence. The likelihood of the predicted tokencan reÓòûect the conÓòõdence of the model [10,31,39,49]. For a tokensequenceùëã=(ùë•1, ùë•2, . . . , ùë•ùëá), we can use average negative log-likelihood (A-NLL) to represent the conÓòõdence to generate it, whichis given byA-NLL(ùëã)=‚àí1ùëáùëáÓõïùë°=1log ùëÉ(ùë•ùë°|ùë•1, ùë•2, . . . , ùë•ùë°‚àí1).It measures how conÓòõdent a model predicts a sample of text byevaluating the probability it assigns to a sequence of tokens. LowerA-NLL indicates greater conÓòõdence. In our paper, we also use apreÓòõx sequence as input and calculate the A-NLL on the suÓòúx,ùëãùëñ:ùëá=(ùë•ùëñ,ùë• ùëñ+1, . . . , ùë•ùëá). In this case, A-NLL is only averaged on
Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Modelsthe suÓòúx, which isA-NLL(ùëãùëñ:ùëá|ùëã:ùëñ)=‚àí1ùëá‚àíùëñ+1ùëáÓõïùë°=ùëñlog ùëÉ(ùë•ùë°|ùë•1, ùë•2, . . . , ùë•ùë°‚àí1).A-NLL is also the major term of the loss of the training of LLMsand VLMs. It can be seen as a straightforward metric to reÓòûectthe conÓòõdence and test membership. Besides A-NLL, metrics basedon the intuition of conÓòõdence, such as Min-ùëò%Prob [39], Max-ùëò%Prob [29], perplexity [10], perturbation-based features [31], andzlib Ratio [10], are also proposed to measure membership.3.2Limitation of DDI when the validation set isnot availableIn this subsection, we use the DDI method proposed by [29] asan example to demonstrate the limitation when the non-memberdata which follows the same distribution as the protected set is notavailable. We begin by outlining the details of DDI [29] and thendiscuss the gap and the potential improvement.The method of DDI [29] Óòõrst employs a linear regressor to ag-gregate various sample-wise MIA metrics derived from predictionconÓòõdence, including Min-ùëò%Prob [39], Max-ùëò%Prob [29], per-plexity [10], perturbation-based features [31], and zlib Ratio [10].Trained on ground-truth member and non-member data, the lin-ear regressor produces a membership score for each sample. Eventhough the member and non-member data are in the same distri-bution, the distributions of their membership scores are diÓòùerent.Given a candidate setùëÑ,ùëÑvalis a validation set that is in the samedistribution asùëÑand is known to be non-member. DDI employsaùëß-test for hypothesis testing, with the null hypothesis (ùêª0) as-sumingùëÑwas not used in training. Thisùëß-test determines whetherthe distributions of membership scores ofùëÑandùëÑvalare diÓòùerent.IfùëÑwas not used for training, their distributions of membershipscores should be similar; otherwise, they should diÓòùer. By perform-ing aùëß-test between the membership scores ofùëÑandùëÑval, if theùëù-value falls below a signiÓòõcance threshold (such as 0.05 or 0.01),ùëÑis classiÓòõed as member data.Limitation of DDI. DDI relies on a strict assumption: access to anon-member set in the same distribution asùëÑ. To test whetherùëÑis amember, we must always prepare a separate set as the validation set.In practice, this is often impractical. IfùëÑis not a training member,even a small diÓòùerence betweenùëÑvalandùëÑcan result in a lowùëù-value, leading to a false positive prediction. SpeciÓòõcally, we Óòõnd thattheùëù-value decreases dramatically as the sizes of the two samplesets inùëß-test (such asùëÑandùëÑvalin DDI) increase, which is provedin the following Theory 1.Theory 1. Given two sample setsùê¥andùêµ,ùúáùê¥andùúéùê¥are themean and standard variance ofùê¥, andùúáùêµandùúéùêµareùêµ‚Äôs. Withoutloss of generality, we assume that the sample sizes ofùê¥andùêµareboth ùëõ. If ùúáùê¥‚â†ùúáùêµ, the ùëù-value of ùëß-test satisÓòões:ùëù-value ‚àùùëí‚àíùëêùëõ,where ùëêis a constant coeÓòúcient and ùëê>0.(a) Distribution (b) ùëù-valueFigure 1: An example of using DDIProof. In ùëß-test, the statistic ùëßis calculated asùëß=ùúáùê¥‚àíùúáùêµÓô≥Óòíùúé2ùê¥ùëõÓòì+Óòíùúé2ùêµùëõÓòì=ùúáùê¥‚àíùúáùêµÓô≤Óòêùúé2ùê¥Óòë+Óòêùúé2ùêµÓòë‚àöùëõ. (1)Then the ùëù-value for a one-tailed test isùëù-value =1‚àíŒ¶(|ùëß|),whereŒ¶(|ùëß|)is the cumulative distribution function (CDF) of thenormal distribution.Whenùëõincreases, the tail probability 1‚àíŒ¶(|ùëß|)can be calculatedusing an asymptotic approximation [8, 11, 18], which gives1‚àíŒ¶(|ùëß|) ‚âà 1|ùëß|‚àö2ùúãùëí‚àíùëß22So the logarithmic of ùëù-value islog(ùëù-value) ‚âà log Óòí1|ùëß|‚àö2ùúãùëí‚àíùëß22ÓòìFor largerùëõ(i.e. larger|ùëß|), the dominant term is‚àíùëß2, whichmeans log(ùëù-value ) ‚âà ‚àíùëß22. Substituting Eq. 1 into this, we havelog(ùëù-value) ‚àù ‚àíùëõ, i.e., ùëù-value ‚àùùëí‚àíùëêùëõ.‚ñ°Empirical results. We use the example in Figure 1a to demon-strate a case of false positive detection when the distributions ofùëÑvalandùëÑare not identical. SpeciÓòõcally, we useùëÑvalto test themembership of two sets,ùëÑmemandùëÑnon. In this scenario,ùëÑmemismember data, whileùëÑnonis non-member data. None of them hasan identical distribution to ùëÑval. We use Pythia-12B [6] as the sus-pect model, PILE [14] asùëÑmem, FineWeb [34] in 2024 asùëÑnonandBBC news in 2024 asùëÑval. In Figure 1a, whileùëÑmemdiÓòùers moresigniÓòõcantly fromùëÑval, there is also a small diÓòùerence betweenùëÑnonandùëÑval. By calculating theùëù-value in Figure 1b, we maketwo key observations. First, theùëù-value forùëÑnonis also very low,indicating a false positive detection. Second, the logarithm of theùëù-value decreases linearly as the sample sizeùëõincreases, whichaligns with Theory 1. The diÓòùerence betweenùëÑvalandùëÑnonmayarise from various factors, such as the lack of non-member data thatfollows the same distribution and the randomness of the samplingprocess.
Trovato et al.(a) The change of distribution (b) ùëù-valueFigure 2: The change of distribution andùëù-values before andafter ùëÑis paraphrased.Therefore, to avoid the false positive detection, we do not com-pareùëÑdirectly withùëÑval. Instead, we introduce changes throughparaphrasing the samples inùëÑand compare the diÓòùerences beforeand after paraphrasing. Figure 2a illustrates the distribution of A-NLL on the suspect model. After paraphrasing, the mean A-NLLofùëÑmemincreases more signiÓòõcantly than that ofùëÑnon. This is be-cause the model has encountered the member data during training,resulting in (weak) memorization. After paraphrasing, the verbatimmember data becomes non-member data, leading to an increase inA-NLL. In contrast, non-member data is less aÓòùected by paraphras-ing, as the model has never been trained on it. In Figure 2b, wealso conduct aùëß-test to compare the distributions before and afterparaphrasing. Denoteùúáorgas the mean of A-NLL of the sampleset before paraphrase andùúáparaas the mean of A-NLL of the sam-ple set after paraphrase. Then, the null hypothesis and alternativehypothesis (ùêª1) can be formulated asùêª0:ùúáorg ‚â•ùúápara;ùêª1:ùúáorg <ùúáparaLowerùëù-value indicates a more signiÓòõcant change after paraphras-ing. In Figure 2b, theùëù-value forùëÑmemdecreases much more rapidly,demonstrating thatùëÑmemundergoes greater change after para-phrasing compared toùëÑnon. In the following subsection, we furtherprovide a study on when will the change become more obvious.3.3 PreÓòõx Sequences can trigger memorizationon training DataAs we analyzed, the model exhibits greater conÓòõdence (i.e., lowerA-NLL) on data it has encountered during training. However, inthis subsection, we observe that paraphrasing the entire sequenceof member data does not always result in a high A-NLL, making itdiÓòúcult to distinguish from non-member data. In this subsection,we provide a detailed analysis of this phenomenon and point outhow to exemplify the change of A-NLL caused by the paraphraseintroduced in the previous subsection.In Figure 3a, we present the change in A-NLL after applyingtwo diÓòùerent paraphrasing methods: whole paraphrase and halfparaphrase. For the whole paraphrase (blue in Figure 3a), we para-phrase the whole text sequences. For each sample, we calculatethe NLLs of both the original and paraphrased text, then subtractthe original NLL from the paraphrased NLL and plot the results.(a) The change of distribution (b) ùëù-valueFigure 3: DiÓòùerent memorization results between paraphras-ing the whole sequence and half of the sequence.For the half paraphrase (red in Figure 3a), we leave the Óòõrst half ofthe text unchanged, paraphrasing only the second half, and reportthe change in NLL for the paraphrased portion. As shown, for thewhole paraphrase, the average change in NLL is slightly above 0,indicating that paraphrasing causes only a minor increase in NLL.In contrast, the half paraphrase leads to a much more signiÓòõcantincrease in NLL. Consequently, in Figure 3b, theùëù-value for the halfparaphrase decreases more rapidly, making it easier to distinguishfrom non-member data.We conjecture that half paraphrasing can introduce an abruptand unexpected change in the prediction sequence. When given apreÓòõx, the causal model relies on it to predict the likelihood distri-bution of the next token. If the preÓòõx comes from member data, themodel is likely to follow its memorization verbatim, even if thatmemorization is weak. Paraphrasing the second half can disruptthis expectation, leading to a higher NLL. Conversely, if the preÓòõxis from non-member data, the paraphrased text still appears rea-sonable, and the model does not have a strong tendency to predictthe original text verbatim. In contrast, the whole paraphrasing doesnot induce such unexpected changes, as no member preÓòõx exists totrigger a verbatim prediction based on memorization.For VLMs, the image tokens in the input can be viewed as thepreÓòõx. We will present the diÓòùerence of the processing betweenLLMs and VLMs in the following section.4 MethodBased on the observations from the preliminary studies, in thissection, we propose our method, Self-Comparison Membership In-ference (SMI). We Óòõrst introduce the framework of Self-Comparisonto get theùëù-values in Section 4.1. Then we explain how to use theùëù-values and its trend asùëõchanges to determine membership inSection 4.2. Lastly, we introduce a variant for a challenging casewhere only the probability of the predicted token is available inSection 4.3.4.1 Self-ComparisonBuilding on preliminary studies, we propose SMI to infer the mem-bership by analyzing the change of A-NLL distribution. To captionthis distribution shift, SMI uses theùëù-value derived from the hy-pothesis testing between the datasets before and after paraphrasing.
Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language ModelsFigure 4: The framework of Self-ComparisonIn this subsection, we provide a detailed explanation of the hypoth-esis testing process and how theùëù-values are calculated. We namethe process to get theùëù-values as Self-Comparison. The frameworkof Self-Comparison can be found in Figure 4, which is performedin three key steps. We begin by detailing these steps using textualdatasets, speciÓòõcally focusing on membership inference in LLMs.Following this, we discuss how the framework adapts to multi-modal datasets, such as VQA and captioning, to demonstrate theframework for VLMs.LLMs. As discussed in Section 3.3, to make the change of memberdata more obvious and easier to capture, SMI takes advantage oftriggered memorization of preÓòõx sequences. For a given setùëÑ, Self-Comparison processes it using the following steps.(1)Half paraphrase. We Óòõrst truncate all the samples ofùëÑintosequences with a length of 150 tokens, and then paraphrasethe second half of the sequence. To ensure the completenessof sentences, we remove the last sentence if it is brokenresulting from truncation. To get a complete second half, wesegment the sequence by sentences rather than tokens. Weuse Gemma 2 [44] for paraphrase.(2)Membership Metric calculation. In the proposed SMI, we usethe most straightforward A-NLL as the metric. We calculatethe average NLL on the tokens in the second half of thesequence. By inputting the original data and paraphraseddata into the LLMs, we obtain two sets of A-NLL values. (Itis worth noting that there is a challenge with certain models,such as ChatGPT-4, which only return the log probability forthe predicted token. We provide a comprehensive discussionof this case in Section 4.3.)(3)Comparison. Based on the preliminary studies, we know thatif the model is trained onùëÑ, the distribution of A-NLL shouldhave a signiÓòõcant change. We calculate theùëù-values of thehypothesis testing withùêª0thatùëÑis not member data. To getthe trend ofùëù-value as the sample size increases, we calculatea series ofùêæ ùëù-values,{ùëùùëñ|1‚â§ùëñ‚â§ùêæ}atùêæequal intervals,{ùëõùëñ|1‚â§ùëñ‚â§ùêæ}, whereùëõùëñ=ùëñùêæùëÅandùëÅis the total numberof samples inùëÑ. We use the slope of linear least-squaresregression forùëùùëñandùëõùëñ(such as the orange dash line inùëù-value ofùëÑmemin Figure 4) to represent the trend ofùëù-values.The slope can be denoted asùõΩ=√çùêæùëñ=1(ùëõùëñ‚àíùëõ)Óòêlog ùëùùëñ‚àílog ùëùÓòë√çùêæùëñ=1(ùëõùëñ‚àíùëõ)2,whereùëõis the mean of{ùëõùëñ|1‚â§ùëñ‚â§ùêæ}andlog ùëùis the meanof {log ùëùùëñ|1‚â§ùëñ‚â§ùêæ}.It is worth mentioning that, based on our experiments in Sec-tion 5, in SMI,ùëÅ=500 is large enough to provide a solid inference.This means that for web-scale large datasets, we do not need to testthe whole dataset. We can sample a set of sizeùëÅ, which is eÓòúcientand eÓòùective to verify the unauthorized dataset usage.VLMs. For datasets such as VQA and image captioning, imageis also a part of input. This leads to a small diÓòùerence in the stepof paraphrase. For such datasets, SMI keeps the image tokens andquestions unchanged and paraphrases the textual response. For thedatasets with multi-round chatting, we only use the Óòõrst round. Forstep (2) and (3), it is the same as LLMs.After obtaining the results ofùëù-values, in the following subsec-tion, we present the determination conditions for classifyingùëÑasa training member.4.2 Criteria for membershipAlthough we use hypothesis testing to getùëù-values, the traditionalsigniÓòõcance level likeùëù=0.01 is not applicable to our problem. Aswe mentioned in Section 3.2, a small diÓòùerence may lead to a smallùëù-value ofùëÑnonespecially whenùëõis large. The paraphrase mayalso bring in such small diÓòùerences. In this subsection, for a morerigorous membership inference, we propose to use an auxiliarydatasetùëÑauxto eliminate the impact of paraphrase and present thetwo criteria for membership using ùëÑaux.SMI uses a non-member set as the auxiliary set,ùëÑaux. ThisùëÑauxis non-member, but not necessary to be the same distribution toùëÑ.It is easy to obtain from synthesized data, unpublished data, and thedata released after the model training date. We denote theùëù-valuesfrom Self-Comparison ofùëÑauxasÓòàùëù‚Ä≤ùëñ|1‚â§ùëñ‚â§ùêæÓòâ, and its slope asùõΩ‚Ä≤. In the following experiments of Section 5, we observe thatùëù‚Ä≤ùëñisusually larger than or close to 0.01 andùõΩ‚Ä≤is close to 0. Based onùëÑaux, we say thatùëÑis the member data in the training set when itsatisÓòões the following two criteria:ùõΩ<ùõΩ‚Ä≤‚àíùúñ1,(2)log ùëùùêæ<log ùëù‚Ä≤ùêæ‚àíùúñ2.(3)Both of the criteria indicate the change of A-NLL ofùëÑshould bemore signiÓòõcant thanùëÑaux. Eq. 2 means the slope ofùëÑshould besmaller thanùëÑauxbyùúñ1, while Eq. 3 means theùëù-value ofùëÑshouldbe smaller thanùëÑauxbyùúñ2. The constants,ùúñ1andùúñ2, are two margin
Trovato et al.values to reduce randomness. They are not data-speciÓòõc since theùëß-test is not data-speciÓòõc for various datasets. This is diÓòùerent fromthe threshold in sample-wise MIA methods.Remarks. From the above pipeline of Self-Comparison and thecriteria, we can Óòõnd that SMI does not rely on ground-truth memberdata. This relaxes the strict assumptions and makes the dataset-levelinference much more practical.4.3A variant when not the logits/probability ofthe whole vocabulary are availableFor some models such as the API of Together AI platform2, thelogits for the whole vocabulary at each token position are available.For example, if the previous sequence is ‚ÄúToday is a sunny ...‚Äù andthe next token is predicted to be ‚Äúday‚Äù, these models will providenot only the probability of ‚Äúday‚Äù, but also the probability of thewhole vocabulary. However, for other models like GPT-4o, theyonly provide the probability of the predicted token. For example,ifùëÑhas a sample that is ‚ÄúToday is a sunny and warm day‚Äù, weneed the probability of ‚Äúand warm day‚Äù to calculate A-NLL. Butthe model prediction is ‚ÄúToday is a sunny day‚Äù. It only provides theprobability of ‚Äúday‚Äù, but we need the probability of ‚Äúand‚Äù. This is amore diÓòúcult scenario.To solve this challenge, we propose to use a constant as theprobability of the unavailable tokens. To get the probability of ‚Äúandwarm day‚Äù, we Óòõrst input the preÓòõx ‚ÄúToday is a sunny‚Äù, if the modelprediction on the next token is ‚Äúand‚Äù, we can have the probability of‚Äúand‚Äù. However, if the prediction is ‚Äúday‚Äù, it means the probabilityof ‚Äúand‚Äù should be low. Thus, we assign a low constant probabilityvalue (i.e., large NLL) to it. Then we add ‚Äúand‚Äù into the preÓòõx andcontinue to input ‚ÄúToday is a sunny and‚Äù to predict ‚Äúwarm‚Äù until weiterate through all the following tokens. With this improvement,we can also use SMI when only the probability of the predictedtoken is available.5 ExperimentsIn this section, we present the experiments to show the eÓòùective-ness of SMI. We conduct the experiments across diÓòùerent modelsand datasets in Section 5.2, and ablation studies including modelsize, sample size, and margin values in Section 5.3. Additionally, inAppendix B, we also conduct the experiment to demonstrate theeÓòùectiveness of SMI when only part ofùëÑis used for training. Next,we Óòõrst introduce the experimental settings.5.1 Experimental SettingsSuspect models and datasets. We conduct experiments on publicmodel checkpoints, models Óòõne-tuned by ourselves, and API-basedcommercial models. The details are as follows:(1) For public checkpoints, we include two LLMs, Pythia [6] andGPT-Neo [7], and two VLMs, LLaVA [26] and CogVLM [48]. Thetraining dataset of Pythia and GPT-Neo is PILE [14], and we use NewYorker Caption Contest (NY) [16] and FineWeb (F-CC) [34] crawledin 2024 asùëÑnonandùëÑaux. The training data of LLaVA includesTextVQA (TVQA) [41], Visual Genome (VG) [23], and MS COCO(COCO) [25]. The training data of CogVLM includes CogVLM-SFT2https://www.together.ai/(Cog) [48]. And we use NoCaps (NC) [3] and Flickr (Flkr) [53] asùëÑnon and ùëÑaux for public VLMs.(2) For Óòõne-tuned models, we train Pythia-1.4B on FineWeb, NY,or BBC news in 2024. We train LLaVA using Flickr or MS COCO.To simulate the real-world Óòõne-tuning, we add 118,000 unrelatedsamples into the training set of Pythia-1.4B and 20,000 into LLaVA.Each model is trained in one epoch.(3) For API-based models, we use GPT-4o. SinceùëÑmemis un-known, we use famous books, Bible,Pride and Prejudice (Pride), andHarry Potter (HP), and BookMIA containing member (B-Pos) andnon-member (B-Neg) samples of OpenAI models. The non memberin BookMIA (B-Neg) and BBC news in 2024 are ùëÑnon and ùëÑaux.To further validate SMI, we use diÓòùerent datasets asùëÑnonandùëÑaux alternatively.Baselines. To ensure the fairness of experiments, we assumeall the baselines and SMI have no access to the ground-truth mem-ber data, but have access toùëÑauxwhich is non-member and doesnot necessarily follow the same distribution as member data. Thebaseline methods include DDI and three sample-level MIAs, A-NLL,Min-ùëò% [39] and zlib ratio [10]. Each sample-level MIA calculatesa membership score and uses it to determine the membership ofone sample. We use the 45th percentile of the membership scoreofùëÑauxas the threshold (which is explained in Figure 5 and Sec-tion 5.2). In the three MIAs, membership scores lower than thethreshold will be classiÓòõed as member. In addition, to better matchthe sample-level MIAs to the dataset-level tasks, we create variantsof them by counting the positives inùëÑ. We Óòõrst use sample-levelMIAs to classify each sample inùëÑ. If more than 50% samples arepredicted as member by the sample-wise MIA, we classifyùëÑasmember. Otherwise, it is classiÓòõed as non-member.Evaluation metrics. We use F1 score, recall, and precision asevaluation metrics. In each original dataset, there are at least 900samples. For dataset-level inference, to increase the number ofdatasets and get more convincing results, we construct 300 sub-sets from the original datasets, with each sub-set consisting of 500randomly sampled sequences (ùëÅ=500) from the original dataset.The 300 sub-sets are composed of 100ùëÑmems, 100ùëÑnons and 100ùëÑauxs. The metrics are calculated by the label and prediction for thesub-sets. In addition, to better explain the variants of sample-levelMIA, we also calculate the evaluation metrics at the sample levelusing all the samples in the original datasets and use them as areference.Implementation details. For all the models, we only use theoutput probability for membership inference. For all the results ofSMI, we useùúñ1=0.01 andùúñ2=10. For GPT-4o, we use the chattemplate in Appendix A.1.5.2 Main resultsIn this subsection, we show the eÓòùectiveness of our method indiÓòùerent LLMs and VLMs. We reported the results of public modelcheckpoints, the model Óòõne-tuned by ourselves, and API-basedGPT-4o in Table 1. We get two observations from the results.1. SMI outperforms all the baseline methods across variousmodels and datasets. For public models and Óòõne-tuned models,Table 1 shows that the average F1 scores of SMI consistently exceed0.98, demonstrating that our method can accurately distinguish
Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language ModelsTable 1: Dataset-level membership inference on public and Óòõne-tuned models. The results are F1 score (recall/precision). Welabel the best average F1 score by bold fonts.Public LLM Pythia-1.4B Pythia-6.9B Pythia-12B GPT-Neo-1.3B GPT-Neo-2.7B AverageùëÑmem/ùëÑnon/ùëÑaux PILE/F-CC/NY PILE/NY/F-CC PILE/F-CC/NY PILE/NY/F-CC PILE/F-CC/NYA-NLL (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.800 (1.000/0.700)Min-ùëò%(Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.800 (1.000/0.700)zlib (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.800 (1.000/0.700)DDI 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500)SMI (ours) 0.958 (0.920/1.000) 1.000 (1.000/1.000) 0.995 (1.000/0.990) 0.980 (0.970/0.990) 0.985 (0.970/1.000) 0.984 (0.972/0.996)Public VLM LLaVA-v1.5 CogVLM-v1 CogVLM-v1-chat AverageùëÑmem/ùëÑnon/ùëÑaux TVQA/Flkr/NC VG/NC/Flkr COCO/Flkr/NC Cog/NC/Flkr Cog/Flkr/NCA-NLL (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.669 (1.000/0.503) 0.000 (0.000/0.000) 0.667 (1.000/0.500) 0.600 (0.800/0.501)Min-ùëò%(Dataset) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.995 (1.000/0.990) 0.000 (0.000/0.000) 0.667 (1.000/0.500) 0.732 (0.800/0.698)zlib (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.800 (1.000/0.700)DDI 0.667 (1.000/0.500) 0.873 (1.000/0.775) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.775 (1.000/0.655)SMI (ours) 0.980 (0.990/0.971) 1.000 (1.000/1.000) 0.980 (1.000/0.962) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.992 (0.998/0.986)Fine-tuned Pythia-1.4B LLaVA (initialized by Vicuna) AverageùëÑmem/ùëÑnon/ùëÑaux F-CC/NY/BBC NY/BBC/F-CC BBC/F-CC/NY Flkr/NC/TC COCO/Flkr/VGA-NLL (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.867 (1.000/0.800)Min-ùëò%(Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 0.867 (1.000/0.800)zlib (Dataset) 0.667 (1.000/0.500) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.800 (1.000/0.700)DDI 0.667 (1.000/0.500) 0.694 (1.000/0.532) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.672 (1.000/0.506)SMI (ours) 1.000 (1.000/1.000) 0.995 (1.000/0.990) 1.000 (1.000/1.000) 0.971 (1.000/0.943) 1.000 (1.000/1.000) 0.993 (1.000/0.987)API-based GPT-4o AverageùëÑmem/ùëÑnon/ùëÑaux Bible/B-Neg/BBC Pride/B-Neg/BBC HP/B-Neg/BBC B-Pos/B-Neg/BBCA-NLL (Dataset) 1.000 (1.000/1.000) 0.000 (0.000/0.000) 0.000 (0.000/0.000) 0.000 (0.000/0.000) 0.250 (0.250/0.250)Min-ùëò%(Dataset) 1.000 (1.000/1.000) 0.000 (0.000/0.000) 0.000 (0.000/0.000) 0.000 (0.000/0.000) 0.250 (0.250/0.250)zlib (Dataset) 0.000 (0.000/0.000) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.500 (0.750/0.375)DDI 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500) 0.667 (1.000/0.500)SMI (ours) 1.000 (1.000/1.000) 1.000 (1.000/1.000) 0.919 (0.850/1.000) 0.958 (0.920/1.000) 0.969 (0.943/1.000)member data. In contrast, while dataset-level MIA variants achievehigh F1 scores on some models, such as Pythia-6.9B and GPT-Neo-1.3B, they perform poorly on others like Pythia-1.4B and CogVLM-v1, resulting in signiÓòõcantly lower F1 scores and precision comparedto our method. This is because previous MIA approaches rely ondata-speciÓòõc thresholds that are not consistently applicable acrossdiÓòùerent models and datasets (which are detailed below). As for DDI,its performance is typically 0.667 (1.000/0.500), as it classiÓòões everyùëÑas member data regardless of its true label. While this allows itto recall all member sets, it misclassiÓòões all non-member sets asmember, leading to a precision of 0.5. For API-based GPT-4o, ourmethod signiÓòõcantly outperforms others, achieving an F1 score of0.969. The performance of dataset-level MIA variants is worse thantheir performance on public and Óòõne-tuned models. This diÓòùerenceis likely due to the lack of access to the probabilities of all tokens,which may aÓòùect the accuracy of their membership scores.2. The performance of sample-level MIAs and their dataset-level variants are highly dependent on the choice of thresh-old. From Table 1, it is evident that the dataset-level variants ofMIAs, such as A-NLL (Dataset), often exhibit three distinct modes ofF1 scores: 0.000, 0.667, and 1.000. These modes are a result of threediÓòùerent threshold choices. In Figure 5, we plot the distributions ofA-NLL ofùëÑmemandùëÑnonand their medians. We can see that thetwo medians split the x-axis into three regions. When the thresholdfalls within the middle region (between the medians), indicated bythe green arrow in Figure 5, more than 50% member samples willbe correctly classiÓòõed as member. Since all the samples inùëÑmemare member samples,ùëÑmemwill be classiÓòõed as member. Similarly,less than 50% non-member data will be classiÓòõed as member andùëÑnonwill be classiÓòõed as non-member. This leads to the correctclassiÓòõcation of everyùëÑ, and F1 score is 1.000. However, if thethreshold falls in the right region, all theùëÑs will be classiÓòõed asmember set. In this case, the F1 score drops to 0.667 (1.000/0.500),which is similar to DDI, i.e., all the member sets are recalled, but theprecision is only 0.5. On the other hand, if the threshold is in the leftregion, allùëÑs are classiÓòõed as non-member sets, meaning no mem-ber sets are recalled, leading to an F1 score of 0.000 (0.000/0.000).To choose the threshold from the middle region, we conjecture thatthe distribution ofùëÑauxis more similar toùëÑnonsince they are bothnon-member. Then the middle region is possible to locate at theleft of the median ofùëÑaux. Thus, we use the 45th percentile of themembership score of ùëÑaux as the threshold.
Trovato et al.Table 2: Sample-level MIAs. The results are F1 score (recall/precision).Public LLM Pythia-1.4B Pythia-6.9B Pythia-12B GPT-Neo-1.3B GPT-Neo-2.7B AverageùëÑmem/ùëÑnon/ùëÑaux PILE/F-CC/NY PILE/NY/F-CC PILE/F-CC/NY PILE/NY/F-CC PILE/F-CC/NYA-NLL 0.684 (0.877/0.561) 0.785 (0.690/0.910) 0.689 (0.898/0.559) 0.783 (0.698/0.893) 0.682 (0.882/0.556) 0.725 (0.809/0.696)Min-ùëò%0.645 (0.756/0.563) 0.720 (0.658/0.794) 0.666 (0.803/0.569) 0.702 (0.649/0.764) 0.661 (0.790/0.568) 0.679 (0.731/0.651)zlib 0.670 (0.886/0.539) 0.778 (0.659/0.951) 0.677 (0.906/0.541) 0.782 (0.659/0.961) 0.673 (0.892/0.540) 0.716 (0.800/0.706)Public VLM LLaVA-v1.5 CogVLM-v1 CogVLM-v1-chat AverageùëÑmem/ùëÑnon/ùëÑaux TVQA/Flkr/NC VG/NC/Flkr COCO/Flkr/NC Cog/NC/Flkr Cog/Flkr/NCA-NLL 0.701 (0.830/0.606) 0.822 (0.953/0.723) 0.780 (0.983/0.646) 0.371 (0.320/0.440) 0.737 (0.992/0.586) 0.682 (0.816/0.600)Min-ùëò%0.701 (0.787/0.632) 0.799 (0.961/0.683) 0.795 (0.962/0.677) 0.299 (0.251/0.371) 0.712 (0.881/0.598) 0.661 (0.768/0.592)zlib 0.671 (0.865/0.549) 0.765 (0.755/0.776) 0.729 (0.982/0.580) 0.861 (0.935/0.797) 0.720 (0.999/0.563) 0.749 (0.907/0.653)Figure 5: Three regions for thethreshold of sample-level MIAsFigure 6: F1 score of SMI on dif-ferent sample sizesFigure 7: F1 score of SMI on dif-ferent margin valuesFigure 8:ùëù-values of SMI ondiÓòùerent model sizesIn addition, it is important to note that there is a signiÓòõcantoverlap between the distributions ofùëÑmemandùëÑnon, which meansit is hard to use a threshold to distinguish the member and non-member samples. We conduct sample-level inference experimentsusing sample-level MIAs and present the results of sample-levelMIAs in Table 2. As shown, most of the sample-level MIAs achieveF1 scores of 0.65 to 0.7 on public models (which is very low sincerandom guess has F1 score of 0.5, and predicting all the samplesas member is 0.667). The observed overlap helps explain the poorperformance of existing sample-level MIAs.In summary, our method can achieve signiÓòõcantly better perfor-mance than baseline methods and does not rely on ground-truthdata to determine a data-speciÓòõc threshold.5.3 Ablation studiesIn this subsection, we conduct ablation studies on sample size,margin values and model size.Sample size. In some scenarios, models may restrict the numberof allowed queries. Verifying membership with fewer samples be-comes important. In Figure 6, we show F1 scores of SMI on publicmodels when fewer samples are available. Recall that we useùëÅtorepresent the number of the total available samples. The resultsdemonstrate that the performance is stable and high whenùëÅ‚â•300.Even withùëÅ‚â•100, SMI achieves F1 scores above 0.76 across allmodels, with most scores around 0.9. In summary, our methodmaintains strong performance even with a limited sample size.Margin values. In Figure 7, we demonstrate the eÓòùectivenessof the two margin values,ùúñ1andùúñ2. The results show that bothùúñ1andùúñ2improve performance compared to the case without marginvalues. Usingùúñ2alone yields a higher F1 score than usingùúñ1alone.Furthermore, combining both margin values leads to an even betterperformance in the F1 score. This improvement occurs because themargin values help reduce noise in the sampling process.Model size. Larger models usually have more parameters whichmight memorize larger datasets. In Figure 8, we present the trendofùëù-values for the Pythia series across diÓòùerent model sizes. Theresults indicate that as model size increases, theùëù-value decreasesfaster. Notably, this decrease is evident even at a model size of410M, demonstrating that our method can eÓòùectively distinguishmember data at this scale. In contrast, smaller models, such asPythia-70M, exhibit insuÓòúcient capacity to memorize large datasets.These smaller models do not raise signiÓòõcant concerns as theycannot eÓòùectively leverage the knowledge and corpus of the dataset.6 ConclusionIn this paper, we propose a novel dataset-level membership infer-ence method based on Self-Comparison. Instead of directly com-paring member and non-member data, our approach leveragesparaphrasing on the second half of the sequence and evaluateshow the likelihood changes before and after paraphrasing. Unlikeprior approaches, our method does not require access to ground-truth member data or non-member data in identical distribution,enhancing its practicality. Extensive experiments demonstrate thatour proposed method outperforms traditional MIA and datasetinference techniques across various datasets and models, includ-ing public models, Óòõne-tuned models, and API-based commercialmodels.
Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language ModelsReferences[1] 2024. Hello GPT-4o. https://openai.com/index/hello- gpt-4o/[2]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, ShyamalAnadkat, et al.2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774(2023).[3]Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark John-son, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. 2019. nocaps:novel object captioning at scale. In Proceedings of the IEEE International Conferenceon Computer Vision. 8948‚Äì8957.[4]Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,Wenbin Ge, Yu Han, Fei Huang, et al.2023. Qwen technical report. arXiv preprintarXiv:2309.16609 (2023).[5]Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, JunyangLin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 (2023).[6]Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley,Kyle O‚ÄôBrien, Eric Hallahan, Mohammad AÓòûah Khan, Shivanshu Purohit,USVSN Sai Prashanth, Edward RaÓòù, et al.2023. Pythia: A suite for analyzinglarge language models across training and scaling. In International Conference onMachine Learning. PMLR, 2397‚Äì2430.[7]Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-TensorÓòûow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it usingthese metadata..[8]Norman Bleistein and Richard A Handelsman. 1986. Asymptotic expansions ofintegrals. Courier Corporation.[9]Tom B Brown. 2020. Language models are few-shot learners. arXiv preprintarXiv:2005.14165 (2020).[10]Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,et al.2021. Extracting training data from large language models. In 30th USENIXSecurity Symposium (USENIX Security 21). 2633‚Äì2650.[11]Nicolaas Govert De Bruijn. 2014. Asymptotic methods in analysis. CourierCorporation.[12]Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, WeijiaShi, Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and HannanehHajishirzi. 2024. Do membership inference attacks work on large languagemodels? arXiv preprint arXiv:2402.07841 (2024).[13]Wafaa S El-Kassas, Cherif R Salama, Ahmed A Rafea, and Hoda K Mohamed.2021. Automatic text summarization: A comprehensive survey. Expert systemswith applications 165 (2021), 113679.[14]Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, CharlesFoster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al.2020. Thepile: An 800gb dataset of diverse text for language modeling. arXiv preprintarXiv:2101.00027 (2020).[15]Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Ja-cob Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al.2020. Scaling laws for autoregressive generative modeling. arXiv preprintarXiv:2010.14701 (2020).[16]Jack Hessel, Ana Marasoviƒá, Jena D. Hwang, Lillian Lee, JeÓòù Da, Rowan Zellers,Robert MankoÓòù, and Yejin Choi. 2023. Do Androids Laugh at Electric Sheep?Humor ‚ÄúUnderstanding‚Äù Benchmarks from The New Yorker Caption Contest. InProceedings of the ACL.[17]MD Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga.2019. A comprehensive survey of deep learning for image captioning. ACMComputing Surveys (CsUR) 51, 6 (2019), 1‚Äì36.[18]Eugene Isaacson and Herbert Bishop Keller. 2012. Analysis of numerical methods.Courier Corporation.[19]Bargav Jayaraman and David Evans. 2019. Evaluating diÓòùerentially private ma-chine learning in practice. In 28th USENIX Security Symposium (USENIX Security19). 1895‚Äì1912.[20]Nikhil Kandpal, Eric Wallace, and Colin RaÓòùel. 2022. Deduplicating trainingdata mitigates privacy risks in language models. In International Conference onMachine Learning. PMLR, 10697‚Äì10707.[21]Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,Rewon Child, Scott Gray, Alec Radford, JeÓòùrey Wu, and Dario Amodei. 2020.Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).[22]Aran Komatsuzaki. 2019. One epoch is all you need. arXiv preprintarXiv:1906.06669 (2019).[23]Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, JoshuaKravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.2017. Visual genome: Connecting language and vision using crowdsourced denseimage annotations. International journal of computer vision 123 (2017), 32‚Äì73.[24]Fengling Li, Lei Zhu, Tianshi Wang, Jingjing Li, Zheng Zhang, and Heng TaoShen. 2023. Cross-modal retrieval: a systematic review of methods and futuredirections. arXiv preprint arXiv:2308.14263 (2023).[25]Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, DevaRamanan, Piotr Doll√°r, and C Lawrence Zitnick. 2014. Microsoft coco: Commonobjects in context. In Computer Vision‚ÄìECCV 2014: 13th European Conference,Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13. Springer, 740‚Äì755.[26]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruc-tion tuning. Advances in neural information processing systems 36 (2024).[27]Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, PeterHase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al.2024. Rethinkingmachine unlearning for large language models. arXiv preprint arXiv:2402.08787(2024).[28]Siyu Lu, Mingzhe Liu, Lirong Yin, Zhengtong Yin, Xuan Liu, and Wenfeng Zheng.2023. The multi-modal fusion in visual question answering: a review of attentionmechanisms. PeerJ Computer Science 9 (2023), e1400.[29]Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic. 2024. LLMDataset Inference: Did you train on my dataset? arXiv preprint arXiv:2406.06443(2024).[30]Pratyush Maini, Mohammad Yaghini, and Nicolas Papernot. 2021. Dataset infer-ence: Ownership resolution in machine learning. arXiv preprint arXiv:2104.10706(2021).[31]Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, Bernhard Sch√∂lkopf,Mrinmaya Sachan, and Taylor Berg-Kirkpatrick. 2023. Membership inferenceattacks against language models via neighbourhood comparison. arXiv preprintarXiv:2305.18462 (2023).[32]Walaa Medhat, Ahmed Hassan, and Hoda Korashy. 2014. Sentiment analysisalgorithms and applications: A survey. Ain Shams engineering journal 5, 4 (2014),1093‚Äì1113.[33]Milad Nasr, Reza Shokri, and Amir Houmansadr. 2019. Comprehensive privacyanalysis of deep learning: Passive and active white-box inference attacks againstcentralized and federated learning. In 2019 IEEE symposium on security and privacy(SP). IEEE, 739‚Äì753.[34]Guilherme Penedo, Hynek Kydl√≠ƒçek, Anton Lozhkov, Margaret Mitchell, ColinRaÓòùel, Leandro Von Werra, Thomas Wolf, et al.2024. The Óòõneweb datasets:Decanting the web for the Óòõnest text data at scale. arXiv preprint arXiv:2406.17557(2024).[35]Colin RaÓòùel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limitsof transfer learning with a uniÓòõed text-to-text transformer. Journal of machinelearning research 21, 140 (2020), 1‚Äì67.[36]Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang,Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi Chang, et al.2024. Copyright pro-tection in generative ai: A technical perspective. arXiv preprint arXiv:2402.02333(2024).[37]Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz, andMichael Backes. 2018. Ml-leaks: Model and data independent membershipinference attacks and defenses on machine learning models. arXiv preprintarXiv:1806.01246 (2018).[38]Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, RossWightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, MitchellWortsman, et al.2022. Laion-5b: An open large-scale dataset for training nextgeneration image-text models. Advances in Neural Information Processing Systems35 (2022), 25278‚Äì25294.[39]Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, TerraBlevins, Danqi Chen, and Luke Zettlemoyer. 2023. Detecting pretraining datafrom large language models. arXiv preprint arXiv:2310.16789 (2023).[40]Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-bership inference attacks against machine learning models. In 2017 IEEE sympo-sium on security and privacy (SP). IEEE, 3‚Äì18.[41]Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, DhruvBatra, Devi Parikh, and Marcus Rohrbach. 2019. Towards VQA Models ThatCan Read. In Proceedings of the IEEE Conference on Computer Vision and PatternRecognition. 8317‚Äì8326.[42]Felix Stahlberg. 2020. Neural machine translation: A review. Journal of ArtiÓòõcialIntelligence Research 69 (2020), 343‚Äì418.[43]Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupati-raju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re, Mihir Sanjay Kale, JulietteLove, et al.2024. Gemma: Open models based on gemini research and technology.arXiv preprint arXiv:2403.08295 (2024).[44]Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, CassidyHardin, Surya Bhupatiraju, L√©onard Hussenot, Thomas Mesnard, Bobak Shahriari,Alexandre Ram√©, et al.2024. Gemma 2: Improving open language models at apractical size. arXiv preprint arXiv:2408.00118 (2024).[45]Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-AnneLachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, FaisalAzhar, et al.2023. Llama: Open and eÓòúcient foundation language models. arXivpreprint arXiv:2302.13971 (2023).
Trovato et al.[46]Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-ale, et al.2023. Llama 2: Open foundation and Óòõne-tuned chat models. arXivpreprint arXiv:2307.09288 (2023).[47]Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2019.Demystifying membership inference attacks in machine learning as a service.IEEE transactions on services computing 14, 6 (2019), 2073‚Äì2089.[48]Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, JunhuiJi, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al.2023. Cogvlm: Visual expert forpretrained language models. arXiv preprint arXiv:2311.03079 (2023).[49]Lauren Watson, Chuan Guo, Graham Cormode, and Alex Sablayrolles. 2021. Onthe importance of diÓòúculty calibration in membership inference attacks. arXivpreprint arXiv:2111.08440 (2021).[50]Johnny Tian-Zheng Wei, Ryan Yixiang Wang, and Robin Jia. 2024. Provingmembership in LLM pretraining data via data watermarks. arXiv preprintarXiv:2402.10892 (2024).[51]Roy Xie, Junlin Wang, Ruomin Huang, Minxing Zhang, Rong Ge, Jian Pei,Neil Zhenqiang Gong, and Bhuwan Dhingra. 2024. ReCaLL: Membership Infer-ence via Relative Conditional Log-Likelihoods. arXiv preprint arXiv:2406.15968(2024).[52]Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacyrisk in machine learning: Analyzing the connection to overÓòõtting. In 2018 IEEE31st computer security foundations symposium (CSF). IEEE, 268‚Äì282.[53]Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From imagedescriptions to visual denotations: New similarity metrics for semantic inferenceover event descriptions. TACL 2 (2014), 67‚Äì78.[54]Shenglai Zeng, Yaxin Li, Jie Ren, Yiding Liu, Han Xu, Pengfei He, Yue Xing,Shuaiqiang Wang, Jiliang Tang, and Dawei Yin. 2023. Exploring memorization inÓòõne-tuned language models. arXiv preprint arXiv:2310.06714 (2023).
Self-Comparison for Dataset-Level Membership Inference in Large (Vision-)Language Models(a) A-NLL (Dataset)(b) DDI(c) SMI (ours)Figure 9: Membership inference when part ofùëÑis used fortraining. The x-axis is the ratio of ùëÑused for training.A Details on the prompts used in SMIA.1 GPT-4oThe prompt for GPT-4o is :‚ÄúPlease complete the following sentence. Output the next wordsdirectly! The incomplete sentence is:‚ÄùB Additional experimentsWe conduct experiments to test the membership inference perfor-mance when part of data inùëÑis used for training, and plot theresults in Figure 9. To simulate the training with partialùëÑ, we donot directly re-train a model with part ofùëÑ. Instead, we use thepublic model checkpoints, but mix non-member data intoùëÑin themembership inference process. For instance, ifùëü% ofùëÑconsistsof non-member data, the result can be interpreted as the suspectmodel using only ùëü% of ùëÑfor training.From the results, we can see that when more than 40% is usedfor training, on most of models, our method can perform wellwith F1 score higher than 0.9. In contrast, A-NLL (Dataset) is stillsuÓòùering from the problem of threshold at all ratios. Without theprior knowledge to the ground-truth member data or data-speciÓòõcthreshold, the performance is inconsistent on diÓòùerent models. Asfor DDI, the membership inference still has a high false positiverate. In summary, our method still outperforms baseline methodswhen only part of ùëÑis used for training.Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009
Citations (0)References (53)ResearchGate has not been able to resolve any citations for this publication.The multi-modal fusion in visual question answering: a review of attention mechanismsArticleFull-text availableMay 2023 Siyu Lu Mingzhe Liu Lirong Yin Wenfeng ZhengVisual Question Answering (VQA) is a significant cross-disciplinary issue in the fields
of computer vision and natural language processing that requires a computer to output a natural language answer based on pictures and questions posed based on the pictures. This requires simultaneous processing of multimodal fusion of text features and visual features, and the key task that can ensure its success is the attention mechanism. Bringing in attention mechanisms makes it better to integrate text features and image features into a compact multi-modal representation. Therefore, it is necessary to clarify the development status of attention mechanism, understand the most advanced attention mechanism methods, and look forward to its future development direction. In this article, we first conduct a bibliometric analysis of the correlation through CiteSpace, then we find and reasonably speculate that the attention mechanism has great development potential in cross-modal retrieval. Secondly, we discuss the classification and application of existing attention mechanisms in VQA tasks, analysis their shortcomings, and summarize current improvement methods. Finally, through the continuous exploration of attention mechanisms, we believe that VQA will evolve in a smarter and more human direction.ViewShow abstractAutomatic Text Summarization: A Comprehensive SurveyArticleFull-text availableJul 2020EXPERT SYST APPL Wafaa Samy El-Kassas Cherif Salama Ahmed Rafea Hoda K. MohamedAutomatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.ViewShow abstractnocaps: novel object captioning at scaleConference PaperFull-text availableOct 2019 Harsh agrawal AgrawalPeter Anderson Karan Desai Stefan LeeViewComprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated LearningConference PaperFull-text availableMar 2019Milad Nasr Reza Shokri Amir HoumansadrDeep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.ViewShow abstractA Comprehensive Survey of Deep Learning for Image CaptioningArticleFull-text availableOct 2018 Md Zakir Hossain Ferdous Sohel Mohd Fairuz Shiratuddin Hamid LagaGenerating a description of an image is called image captioning. Image captioning requires to recognize the important objects, their attributes and their relationships in an image. It also needs to generate syntactically and semantically correct sentences. Deep learning-based techniques are capable of handling the complexities and challenges of image captioning. In this survey paper, we aim to present a comprehensive review of existing deep learning-based image captioning techniques. We discuss the foundation of the techniques to analyze their performances, strengths and limitations. We also discuss the datasets and the evaluation metrics popularly used in deep learning based automatic image captioning.ViewShow abstractDo Androids Laugh at Electric Sheep? Humor ‚ÄúUnderstanding‚Äù Benchmarks from The New Yorker Caption ContestConference PaperJan 2023Jack HesselAna MarasoviƒáJena D. Hwang Choi YejinViewML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning ModelsConference PaperJan 2019 Ahmed Salem Yang ZhangMathias HumbertMichael BackesViewNeural Machine Translation: A ReviewArticleOct 2020JAIRFelix StahlbergThe field of machine translation (MT), the automatic translation of written text from one natural language into another, has experienced a major paradigm shift in recent years. Statistical MT, which mainly relies on various count-based models and which used to dominate MT research for decades, has largely been superseded by neural machine translation (NMT), which tackles translation with a single neural network. In this work we will trace back the origins of modern NMT architectures to word and sentence embeddings and earlier examples of the encoder-decoder network family. We will conclude with a short survey of more recent trends in the field.ViewShow abstractTowards VQA Models That Can ReadConference PaperJun 2019Amanpreet SinghVivek NatarajanMeet Shah Marcus RohrbachViewDemystifying Membership Inference Attacks in Machine Learning as a ServiceArticleFeb 2019Stacey Truex Ling LiuMehmet Emre GursoyWenqi WeiMembership inference attacks seek to infer membership of individual training instances of a model to which an adversary has black-box access through a machine learning-as-a-service API. In providing an in-depth characterization of membership privacy risks against machine learning models, this paper presents a comprehensive study towards demystifying membership inference attacks from two complimentary perspectives. First, we provide a generalized formulation of the development of a black-box membership inference attack model. Second, we characterize the importance of model choice on model vulnerability through a systematic evaluation of a variety of machine learning models and model combinations using multiple datasets. Through formal analysis and empirical evidence from extensive experimentation, we characterize under what conditions a model may be vulnerable to such black-box membership inference attacks. We show that membership inference vulnerability is data-driven and its attack models are largely transferable. Though different model types display different vulnerabilities to membership inferences, so do different datasets. Our empirical results additionally show that (1) using the type of target model under attack within the attack model may not increase attack effectiveness and (2) collaborative learning exposes vulnerabilities to membership inference risks when the adversary is a participant. We also discuss countermeasure and mitigation strategies.ViewShow abstractShow moreRecommended publicationsDiscover morePreprintFull-text availableEnTruth: Enhancing the Traceability of Unauthorized Dataset Usage in Text-to-image Diffusion Models...June 2024Jie Ren Chen Chen Lingjuan Lyu[...]Yingqian CuiGenerative models, especially text-to-image diffusion models, have significantly advanced in their ability to generate images, benefiting from enhanced architectures, increased computational power, and large-scale datasets. While the datasets play an important role, their protection has remained as an unsolved issue. Current protection strategies, such as watermarks and membership inference, are ... [Show full abstract] either in high poison rate which is detrimental to image quality or suffer from low accuracy and robustness. In this work, we introduce a novel approach, EnTruth, which Enhances Traceability of unauthorized dataset usage utilizing template memorization. By strategically incorporating the template memorization, EnTruth can trigger the specific behavior in unauthorized models as the evidence of infringement. Our method is the first to investigate the positive application of memorization and use it for copyright protection, which turns a curse into a blessing and offers a pioneering perspective for unauthorized usage detection in generative models. Comprehensive experiments are provided to demonstrate its effectiveness in terms of data-alteration rate, accuracy, robustness and generation quality.View full-textPreprintAre Diffusion Models Vulnerable to Membership Inference Attacks?February 2023Jinhao DuanFei KongShiqi Wang[...]Kaidi XuDiffusion-based generative models have shown great potential for image synthesis, but there is a lack of research on the security and privacy risks they may pose. In this paper, we investigate the vulnerability of diffusion models to Membership Inference Attacks (MIAs), a common privacy concern. Our results indicate that existing MIAs designed for GANs or VAE are largely ineffective on diffusion ... [Show full abstract] models, either due to inapplicable scenarios (e.g., requiring the discriminator of GANs) or inappropriate assumptions (e.g., closer distances between synthetic images and member images). To address this gap, we propose Step-wise Error Comparing Membership Inference (SecMI), a black-box MIA that infers memberships by assessing the matching of forward process posterior estimation at each timestep. SecMI follows the common overfitting assumption in MIA where member samples normally have smaller estimation errors, compared with hold-out samples. We consider both the standard diffusion models, e.g., DDPM, and the text-to-image diffusion models, e.g., Stable Diffusion. Experimental results demonstrate that our methods precisely infer the membership with high confidence on both of the two scenarios across six different datasetsRead moreConference PaperResisting Membership Inference Attacks by Dynamically Adjusting Loss TargetsAugust 2023Xihua MaYouliang TianZehua DingRead morePreprintFine-tuning can Help Detect Pretraining Data from Large Language ModelsOctober 2024Hengxiang ZhangSongxin ZhangBingyi JingHongxin WeiIn the era of large language models (LLMs), detecting pretraining data has been increasingly important due to concerns about fair evaluation and ethical risks. Current methods differentiate members and non-members by designing scoring functions, like Perplexity and Min-k%. However, the diversity and complexity of training data magnifies the difficulty of distinguishing, leading to suboptimal ... [Show full abstract] performance in detecting pretraining data. In this paper, we first explore the benefits of unseen data, which can be easily collected after the release of the LLM. We find that the perplexities of LLMs perform differently for members and non-members, after fine-tuning with a small amount of previously unseen data. In light of this, we introduce a novel and effective method termed Fine-tuned Score Deviation (FSD), which improves the performance of current scoring functions for pretraining data detection. In particular, we propose to measure the deviation distance of current scores after fine-tuning on a small amount of unseen data within the same domain. In effect, using a few unseen data can largely decrease the scores of all non-members, leading to a larger deviation distance than members. Extensive experiments demonstrate the effectiveness of our method, significantly improving the AUC score on common benchmark datasets across various models.Read moreLast Updated: 20 Oct 2024Discover the world's researchJoin ResearchGate to find the people and research you need to help your work.Join for free ResearchGate iOS AppGet it from the App Store now.InstallKeep up with your stats and moreAccess scientific knowledge from anywhere orDiscover by subject areaRecruit researchersJoin for freeLoginEmail Tip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with GoogleWelcome back! Please log in.Email ¬∑ HintTip: Most researchers use their institutional email address as their ResearchGate loginPasswordForgot password? Keep me logged inLog inorContinue with GoogleNo account? Sign upCompanyAbout usNewsCareersSupportHelp CenterBusiness solutionsAdvertisingRecruiting¬© 2008-2025 ResearchGate GmbH. All rights reserved.TermsPrivacyCopyrightImprintConsent preferences