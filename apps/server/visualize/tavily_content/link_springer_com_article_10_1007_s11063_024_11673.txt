A Study of the State of the Art Approaches and Datasets for Multilingual Natural Language Inference | Neural Processing Letters
Skip to main content
Log in
Menu
                            Find a journal
                            Publish with us
                            Track your research
Search
Cart
Home
Neural Processing Letters
Article
A Study of the State of the Art Approaches and Datasets for Multilingual Natural Language Inference
Open access
                            Published: 25 October 2024
Volume 56, article number 243, (2024)
Cite this article
Download PDF
                            You have full access to this open access article
Neural Processing Letters
                                Aims and scope
                                Submit manuscript
                                A Study of the State of the Art Approaches and Datasets for Multilingual Natural Language Inference
Download PDF
Sara Renjit1 & Sumam Mary Idicula2 
588 Accesses
Explore all metrics 
AbstractNatural language inference is critical in Natural Language Processing where semantics is involved. Also known as textual entailment recognition, it defines a directional relationship between pair of sentences, namely the text and the hypothesis. Identifying entailment, contradiction, and neutrality in text pairs is necessary for various language processing applications to reduce text redundancy. An overview of the critical works in this aspect for all languages, including the Indian language perspective, is detailed here. There is a high volume of textual entailment and related attempts in English and foreign languages. In contrast, there are only a few attempts for low resource languages. This article presents the progress in textual entailment for various languages and the development of different datasets for textual entailment. Over these years, the datasets developed differ in size and kind. Most of the datasets are raw or generated, and a few of the latest are translated datasets. The article also points to observation notes on the progress that has happened throughout these years.
Similar content being viewed by others
Natural Language Inference for Portuguese Using BERT and Multilingual Information
Chapter
© 2020
Recognizing Textual Entailment in Non-english Text via Automatic Translation into English
Chapter
© 2013
Semi-supervised Textual Entailment on Indonesian Wikipedia Data
Chapter
© 2023
Explore related subjects
Discover the latest articles, news and stories from top researchers in related subjects.
Artificial Intelligence
Use our pre-submission checklist
Avoid common mistakes on your manuscript.
1 IntroductionWith the ever-increasing technological growth, natural language processing has wide applications in different areas, such as health care, cyber security, and IoT systems. It is now a part of many large application-based systems. Semantics, or the meaning conveyed, is an essential element that machines try to understand using artificial intelligence techniques. All these are efforts to make human life more accessible, and more tasks can be delegated to machines when machine intelligence reaches par with human intelligence. The main criteria in natural language processing applications are interpreting information in different forms like text, speech/voice, image, and video.Textual entailment is a small but essential area in Natural Language Processing. Textual entailment defines a semantic relationship between a pair of sentences in terms of entailment, contradiction, and neutral relationships. The reference text with which the meaning is inferred is the text/ premise. The text for which an association is identified is the hypothesis.Textual entailment is focused in two ways: generation and recognition tasks. Natural Language Inference (NLI) is the newer term for Recognizing Textual Entailment (RTE). NLI or RTE is a classification task that identifies the class to which the sentence pairs belong. It is vital in many language processing applications like multi-document summarization, information retrieval, question-answering systems, information extraction, and paraphrasing systems and to the latest ChatGPT and other language models.The generation of textual entailment task deals with an inference of a sentence from the premise sentence. It generates sentences that are entailed, contradictory, or neutral to the given sentence. This task has numerous challenges, such as the generation of invalid sentences for text sentences that are more complex. Proper evaluation metrics to score the generation effectiveness is another bottleneck for this task. [31] proposed that residual LSTM-based encoders are used for sentence generation and a new metric named EBR (Evaluated By Recognizing textual entailment) is used for evaluating the generated sentences. [42] discussed a text generation method for generation of news headlines in Chinese using copy mechanism, which can also be utilized for generation of inference pairs.Recently, NLI has also become an evaluation strategy for various transformer models. NLI is an essential linguistic concept widely addressed in English, German, French, Arabic, Hindi, Chinese, and Japanese. This paper reviews state of the art in NLI, focusing more on low-resource languages.This article reviews the existing literature for natural language inference or textual entailment. A comprehensive study is performed concerning the nature of NLI and the approaches employed for inference. The main contributions of this study are: 
1.
This is a review of NLI that includes low resource languages also.
2.
It showcases state of the art in natural language inference.
3.
List different datasets for NLI and the suitability of these datasets to other models is analyzed.
4.
The trend to this date in the progress of NLI research is analyzed and explained in this article.
The following sections are organized as follows. Section 2 gives an overview of natural language inference, and Sect. 3 reviews state-of-the-art. Section 4 describes the generic datasets, and application-oriented datasets are in Sect. 5. Section 6 talks about the rule-based and machine learning techniques applied to NLI. Deep learning architectures with large datasets are discussed in Sect. 7. Section 8 focuses on sentence representative models used for NLI. The evaluation metrics applied for the NLI research study are defined in Sect. 9. Observations inferred from this evolved research are discussed in Sect. 10. Section 11 concludes the overall study.2 Overview of Natural Language InferenceNatural language inference, as called today, is earlier known by the term textual entailment recognition. The theoretical idea of textual entailment is converted into a research idea by introducing textual entailment benchmark datasets and evaluation forums. The task of identifying textual entailment started in 2005 as a challenge named Recognizing Textual Entailment (RTE). The RTE challenge was introduced for the English language with a small dataset [21]. With the participation of different teams and its popularity, this challenge continued in the subsequent years with improved datasets [22].Semantic variability is an important phenomenon for language-related applications. It also introduces the concepts of ambiguity, paraphrases and entailment. Textual entailment is related to semantic entailment in linguistic level. However, in the applied sense, the focus is on entailment recognition at a shallow linguistic level.In formal semantics concerning semantic entailment, entailment is defined as sentence t entails another sentence h if h remains true in every circumstance in which t is true [17]. It is validated only by human agreements. A more applied definition says that text t entails hypothesis h if human reading h infers that h is most likely true. Later a probabilistic definition was formulated by Glickman, Dagan, and Koppel in 2006, defining hypothesis h as entailed by text t if$$\begin{aligned} P(h\,is\, true \text {|} t) > P(h is true), \end{aligned}$$
                    (1)
                where P(his true | t) gives the mathematical confidence score for entailment.Textual entailment is useful to many systems where semantics has an important role. It is applied in the question answering system where the candidate texts should entail the expected answer. In Information retrieval, the query should be entailed by the retrieved documents. For multi-document summarization, the duplicate sentences can be identified using entailment and can be removed from the summary document. The extracted texts should entail each other in an information extraction system. Machine translation systems should have the machine-translated text entailing the correct translation. Paraphrasing systems are bidirectional textual entailment systems in such a way that the text entails hypotheses and vice versa.An example from the Stanford Natural Language Inference (SNLI) dataset is given below in Table 1.Table 1 An example (from SNLI dataset [16])Full size table3 Challenges in Textual EntailmentThe growth of Textual Entailment (TE) has been limited over these years because of the following challenges: 
1.
The lack of dataset in other languages limited RTE in other languages, which is solved now through the availabilty and feasibility of creating translated datasets in almost every languages.
2.
The initial works in TE were all domain specific TE systems, which are not generic. Creation of domain specific TE systems is therefore challenging, but with the help of sentence embedding models, the systems became more generic and feasible.
3.
The application of NLI as an intermediate step in other NLP systems is limited because of its less feasible domain specific models and datasets. In future, NLI systems can be integrated into other NLP applications as there are generic solutions such as pretrained models, universal sentence encoders that can be integrated into different applications.
4.
The classification task of NLI does exists as a black box classification even today, which lacks proper reasoning for classification. This can be solved by adding explainbility for classification of sentence pairs.
4 State of The Art4.1 Approaches for NLIRecognition of textual entailment has used many data-driven approaches to understand semantic variability. With the availability of datasets and evaluation forums, RTE is more attempted as a classification task visible in almost all challenges. The extracted features and a machine learning classifier classify these sentence pairs. The features included lexical, syntactic, and semantic features based on document co-occurrence counts and first-order syntactic rewrite rules. Approaches based on logical inferences, natural logic, and ontology-based reasoning are applied along with machine learning algorithms.4.1.1 Lexical ApproachesThese methods include a preprocessing stage initially with POS tagging, named entity recognition, phrasal verb identification. Lexical observations include word overlap, longest common sub-sequence matching, and longest substring matching, Skip-grams, Stemming, Lexical Distance which has been carried out in binary and multi class based textual entailment systems for systems in Japanese and Chinese language [50].The general approach for lexical matching includes: 
1.
Preprocess the input string and remove unwanted words: This step includes tokenization, stemming, lemmatization, and stop word removal. It also includes phrasal verb recognition, Idiom processing, named entity recognition, normalization, and Date/Time arguments in several systems.
2.
Re-represent the text with only content words as a bag of words collection or n-gram representation.
3.
Use of linguistic properties such as Hyponymy, Hypernym, Meronymy, Troponym, and Entailment
4.
Compare the rerepresented text and hypothesis using the Local lexical matching algorithm and lexical compare procedure as in Fig. 1 and Fig. 2.
5.
Entailment is decided based on some higher matching score than the threshold value.
4.1.2 Machine Learning ApproachThis approach generates a feature vector based on distance features (number of common words, length of longest common subsequence, longest common syntactic sub tree), entailment triggers (polarity features, Antonymy features, Adjunct Features), pair features using cross pair similarity between the content of text and hypothesis. The feature vector is then used for classification using classifiers namely Support Vector Machine(SVM), Random Forest, Decision Tree, Naive Bayes and many more [28]. Alignment based optimization is used in Arabic textual entailment systems as discussed in [15] and features like length, similarity, named entity are used in Arabic dataset for logistic regression and random forest as discussed in [5].Fig. 1Local lexical matching algorithmFull size imageFig. 2Lexical compare functionFull size image4.1.3 Graph Matching ApproachThe syntactic or semantic dependency of the word is analyzed by converting text and hypothesis into graphs. Dependency nodes representing named entities are removed, and the representation is augmented with semantic roles. The two graphs are then matched along their vertices and paths to arrive at a matching score for classification [28]. Graph based matching algorithms are used in this method. [8] discussed soft dependency tree matching for textual entailment recognition based on a set of rules.4.1.4 Semantic ApproachSemantic approaches consider the meaning of the sentences thus forming semantic representations using knowledge resources like WordNet, VerbOcean, VerbNet, DIRT [28]. Methods based on graph matching and logic reasoning based methods are used in this approach. [40] discussed neural network models based on sentence encoders including continuous bag of words, convolutional neural networks, recurrent neural network and transformer model to identify inferences from natural language text pairs.4.1.5 Similarity based ApproachIt deals with similarities, namely lexical, syntactic and semantic similarity using different parse representations. Tree Edit Distance Algorithm is one such algorithm used in this approach [4, 39]. Lexical similarity measures such as cosine similarity, dice similarity, Levenstein similarity, Jaccard similarity are used. Syntactic similarity is measured in terms of verb-verb similarity, noun phrase- verb phrase similarity, noun-noun similarity [48]. Semantic similarity is measured using word embedding similarity, Term Frequency -Inverse Document Frequency (TF-IDF) and metrics such as BLEU (Bilingual Evaluation Understudy).4.1.6 Deep Learning ApproachWith the development of the SNLI dataset in 2015 [16], it has become feasible to use several deep learning architectures for textual entailments such as Recurrent Neural Network (RNN), Long Short Term Memory (LSTM) networks, and Bidirectional LSTM with inner attention [67]. [34] proposed Convolutional Neural Network based architectures for matching sentence pairs.Several multilingual models exist today which are either classifier models or produce sentence representations that can be used to encode the semantic content of sentence pairs. Universal sentence encoder [55], BERT based language agnostic pretrained models [54] and language agnostic sentence embeddings [6] are examples of sentence representation models. These models are based on attention mechanism to identify significantly important words from a sentence and also use multi-head attention in transformer based encoder decoder architectures. There has been a significant growth in classification approaches from using feature-based machine learning to deep learning based approaches [46].4.2 Tools and ResourcesDifferent linguistic tools are also used for preprocessing the data. The preprocessing techniques applied are tokenization, stemming, lemmatization, and part of speech tagger, parser, and tools for named entity recognition. Machine learning is employed through WEKA and Lucene for indexing and retrieval, and similarity is calculated using WordNet. Entailment is also addressed through anaphora resolution and word sense disambiguation.The resource set used for these challenges are WordNet, EuroNet, eXtended WordNet, and DIRT which has statistically learned inference rules, Verb oriented VerbNet and VerbOcean. Other resources include FrameNet and Wikipedia, Reuters Corpus, English Gigaword and InfoMap, and Dekang Lins thesaurus to infer lexical similarity [22]. ACL has hosted a website for textual entailment named the Textual Entailment Resource Pool, which has the collection of tools and resources for RTEFootnote 1.Recently pretrained language models from AI communities such as Google, Facebook [13], Open AI,Footnote 2 AI4BharatFootnote 3 contributed towards developing very efficient resources for various kinds of language processing. It includes transformer based encoder architectures [36], language agnostic models and generative models.5 Generic DatasetsFraCas dataset is the test suite developed by the FraCaS Consortium in 1996. It consists of complex entailment problems which are difficult to solve by RTE systems. The solutions for these systems are primarily through higher-order logic-based inference rules. It is mainly used to evaluate or showcase the limitations of RTE-based systems [20].RTE 1 dataset introduced in 2005 consists of 1000 text hypothesis pairs extracted from the news domain through different application settings, namely question answering, reading comprehension, information retrieval, machine translation, and similar documents. Cross annotations were performed, and at least two manual annotators annotated each pair. The average agreement is 89.2%, and the kappa level is 0.78 showing substantial agreement. This dataset has a split into a development set containing 567 pairs and a test set containing 800 pairs [21].RTE 2 dataset collection is developed by Bar-Ilan University, CELCT in Italy, and Microsoft Research and MITRE(USA). This dataset is more realistic with different reasoning levels: lexical, syntactic, morphological, and logical. Triple annotation and some preprocessing such as sentence split and dependency parsing are done. This dataset also focussed on applications like Question Answering, information retrieval, information extraction, and multi-document summarization.RTE 3 dataset is similar to the above two datasets with longer texts that facilitate discourse-level inference. The challenge has been part of the ACL 2007 Workshop on Textual Entailment and Paraphrasing [29]. RTE resource pool has been created at this time. Also, the US National Institute of Standards and Technology (NIST) has initiated two extensions to this task: differentiating unknown from contradiction and justifications to answers.RTE 4 dataset was included as a track in Text Analysis Conference, and it was co-organized by the NIST. This challenge was a three-way classification as entailment, contradiction, and unknown. Two-way judgments were also done as contradiction and unknown falling into no entailment class. The best accuracy obtained was 0.685 for three-way judgment and 0.72 for two-way classification [22].RTE 5 dataset was a track at Text Analysis Conference in 2009. This dataset has improvements in terms of higher average text length, raw texts from different sources, the development set’s release, and application setting is limited to question answering, information extraction, and retrieval. This track also introduced a search task to find all the texts that entail a given hypothesis, which is helpful for a multi-document summarizing setting [10].RTE 6 dataset focus on the summarization setting where the task is to find all sentences that entail a given hypothesis. The other scenario focussed through this dataset is Knowledge Base Population Scenario, a slot filling task in which each slot fill would create a hypothesis sentence. The text is the document used for filling the slot.RTE 7 dataset was based on two application settings: Summarization and knowledge base population. There were 21,420 sentence pairs, and these texts are long up to a paragraph-length based on summarization setting [11].PETE dataset Parser Evaluation using Textual Entailments (PETE) dataset is the dataset created for the Parser Evaluation using Textual Entailments shared task in SemEval 2010. This dataset had 367 entailments randomly split into 66 sentences in the development set and 301 in the test set. The entailment field has Yes or No values denoting whether text hypotheses are entailed. Almost half in the test set are positively entailed [72].ArbTE dataset [3] is a Modern Standard Arabic RTE dataset consisting of 600 text hypothesis pairs. It is extracted from an Arabic news website through a semi-automated technique to form a balanced dataset.EVALITA dataset is the dataset created from the periodic evaluation of NLP and speech tools for the Italian language. Textual entailment was one of the tasks in this evaluation conducted in 2009 [14]. The sentence pairs are extracted from the Italian Wikipedia articles and annotated into different classes as below. 
1.
bidirectional: S1 entails S2 and viceversa
2.
left: S1 entails S2, but not viceversa
3.
right: S2 entails S1, but not viceversa
4.
no: neither S1 entails S2, nor vice versa
5.
reject: rejected pairs
SNLI dataset is a collection of text hypothesis pairs collected using Amazon Mechanical Turk. The datasetFootnote 4 consists of 570k sentence pairs, of which 550k is the training set, 10k are development pairs, and 10k are testing pairs. The overall Fleiss kappa score of this dataset is 0.70. Workers writing the text pairs are provided instructions, and if at least three annotators out of five annotators choose the same label from the three lables(entailment, neutral, and contradiction), that label is taken as the gold standard label(majority voting). The average sentence length of premise sentences is 14.1 and for the hypothesis is 8.3 [16].MultiNLI dataset is also a crowd-sourced datasetFootnote 5 containing 433k sentence pairs modeled on the SNLI dataset. The data collected includes different genres of spoken and written texts such as fiction, government, telephone, travel, face-to-face, etc. This corpus has been used in the shared task in RepEval 2017 Workshop [70].XNLI dataset is the cross-lingual datasetFootnote 6 generated from the MultiNLI dataset for 14 languages, namely French, Spanish, German, Greek, Bulgarian, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, Hindi, Swahili, and Urdu. This dataset has 2500 development and 5000 test pairs from the MultiNLI dataset. This corpus helped to evaluate cross-lingual sentence encoding and classification even in low resource languages [18].MaNLI dataset is the Malayalam Natural Language Inference datasetFootnote 7 developed from the SNLI dataset by manual and automatic translations from English to Malayalam by linguistic experts and through Google translator. MaNLI is a small translated subset of the SNLI dataset. It is the first dataset derived in the Malayalam language for the NLI task. The average sentence length for the premise sentences is 9.17 and for the hypothesis is 5.04. Since it is a translated dataset, the BLEU score for translation of entailment sentences ranges from 0.007 to 1.0, for contradiction, 0.002 to 0.96, and for neutral, 0.002 to 0.93. This dataset has 12k sentence pairs, out of which 4026 are entailment pairs, 3963 are contradiction pairs, and 4011 are neutral [56].hindi-nli-data is the recasted NLI datasetFootnote 8 in the Hindi language created by the automatic recasting of three text classification datasets in the Hindi language, which are related to affective content analysis. They are sentiment analysis (Product Review dataset), emotion analysis (BHAAV dataset), discourse analysis (Hindi Discourse Modes Dataset), and topic modeling (Hindi BBC News Dataset). Recasting is done so that the sentence is embedded with the label, and positive and negative hypotheses are generated using the recasting template. Then the dataset is created with sentence pairs (original sentence, recasted positive hypothesis) and (original sentence, recasted negative hypothesis). The team has developed this at Multimodal Digital Media Analysis Lab at IIIT, New Delhi [68].IndicXNLI dataset is the XNLI datasetFootnote 9 for 11 Indic languages from the Indo-Aryan and Dravidian families. The languages are Assamese (as), Gujarat (gu), Kannada (kn), Malayalam (ml), Marathi (mr), Odia (or), Punjabi (pa), Tamil (ta), Telugu (te), Hindi (hi), and Bengali (bn). The English pairs in the XNLI dataset are translated into 11 languages using IndicTrans, resulting in 392K training pairs, 2490 validation pairs, and 5010 test pairs. This dataset is useful for transfer learning from English to Indic languages [2].6 Application Oriented NLI DatasetsThe main repository that has the collection of majority of datasets is the Hugging face repository,Footnote 10 which has almost all of the datasets and pretrained models for majority of tasks.MedNLIFootnote 11and RuMedNLIFootnote 12: Natural language inference dataset for the clinical domain is introduced in 2017 and 2018 by [65] and [60]. Doctors have annotated this dataset with an inference given the patients’ medical history. Two strategies are followed in creating the dataset, using transfer learning to use open domain datasets like SNLI and to include medical domain knowledge from external resources and medical ontology. The Russian version of the MedNLI dataset has been created by Russian translations of MedNLI and published online in 2022. [69] discussed semantic textual similarity of sentence pairs from clinical corpus.SciTail: [38] presented 27K science-related sentence pairs for entailment. The hypothesis sentences are the questions together with the selected answer, and it has to entail or not entail with the relevant reference sentence retrieved from a large corpus. A question combined with an answer should entail the reference answer.SciNLI: SciNLI is the first natural language inference dataset based on scientific texts. Scientific literature and vocabulary vastly differ from everyday language texts. This dataset is helpful in scientific language understanding models. The training data is created from scientific articles in NLP and computational linguistics from the ACL anthology published from 2000 to 2019. The test data consists of sentence pairs extracted from scientific articles in the ACL anthology in 2020. A BiLSTM-based evaluation of the dataset shows that the dataset is challenging compared with the SNLI, MNLI, and SICK datasets [12, 62]. The different classes in this dataset are Entailment, Contrasting, Reasoning, and Neutral. Domain-specific knowledge is one factor in this dataset that limits its classification performance; hence knowledge resources are still significant in some cases when it comes to application-specific NLI.COLIEE dataset: It is the dataset developed as part of the Competition on Legal Information Extraction/Entailment.Footnote 13 The task in this competition is to identify whether a question is entailed by the supporting legal documents. This competition consists of Case retrieval task for which case data is from Federal Court of Canada. The other task is the statute retrieval task, which has data from Japanese Legal Bar exams, and all articles from the Japanese Civil Law [53].7 Rule-Based and Machine Learning ArchitecturesThere has been an enormous amount of work done for textual entailment since its inception in 2005 as a challenging task, but the focus tends to be more on English than other languages. Here we detail the significant contributions that happened in many languages. 
1.
English language: There are numerous works in English using RTE datasets. [48] improved the classification accuracy for RTE datasets using Lexical, WordNet, and syntactic matching. Rule-based methods based on dependency parsers are employed with the PETE dataset, which improves the accuracy of the systems. This system includes dependency tree representation, comparison for entailment scoring, and the decision to Yes or No classes [8]. Anaphora resolution technique is used in systems to recognize entailments from the RTE-6 dataset for three tasks: a summarization scenario, novelty detection task, and Knowledge base population validation. Different similarity metrics are also used for entailment recognition. The features used are cosine similarity, unigram match, Jaccard similarity, Dice similarity, Overlap, harmonic mean, and machine translation evaluation metrics, namely Bilingual Evaluation Understudy (BLEU) and Metric for Evaluation of Translation with Explicit Ordering (METEOR). The feature vector is then classified using LibSVM, SMO, Naive Bayes, AdaboostM1, and J48 [64]. The accuracies of these systems are shown in Table2.
2.
Arabic language: The different systems in Arabic language for RTE are in Table 4. ArbTE uses existing approaches of textual entailment to understand and evaluate its performance in the Arabic language. ARTESys+ includes a preprocessing stage based on analyzing named entities, temporal expressions, number/countable pairs, and ordinary words. BL-SYS is a bag of words based model, SANATE is an improved version of the entailment system, ETED system uses the Tree Edit Distance algorithm, and ETED+ABC uses the ABC algorithm to find the edit operation cost. LR-ALL is a logistic regression classifier using a word embedding and other features [5].
3.
Japanese and Chinese languages: The Recognizing Inference in TExts and Validation (RITE-VAL) at NTCIR-11 had two track, one for Chinese and the other for the Japanese language. Binary and multiclass classification are sub-tasks in this track. [50] preprocessed the sentence pairs by converting them into English. Features such as lexical (Unigram match, bigram match, longest common subsequence, Skip grams, Stemming, Named Entity Match, Lexical distance) and syntactic features (Subject-verb comparison, WordNet based Subject-Verb comparison, Subject-Subject Comparison, Object-Verb Comparison) are extracted to classify using Support Vector Machine. The accuracy of these systems is shown in Table 3. System developed by [43] focussed on forward entailment, reverse entailment, bidirectional entailment, contradiction, and independence. These classes are predicted using SVM with features namely Chinese surface textual features (word overlap, cosine similarity), Chinese lexical semantic features (negatives and antonyms), Chinese syntactic feature (dependency triples) and Chinese linguistic phenomena feature (textual contradiction, temporal, spatial, quantity and modifier exclusions).
4.
Italian language: One of the works [49] in the Italian language using EVALITA dataset included automatic translation of sentence pairs into English using Bing translator. It is preprocessed by stop word removal and co-reference resolution. Features extracted after preprocessing include named entity match, ngram match, chunk similarity, text similarity, and POS match. Feature vectors created based on these features are classified using the Naive Bayes classifier into Yes or No classes. The accuracy for this system is 0.660.
5.
Bengali language: The idea of practical textual entailment was put forth by the work in the Bengali news dataset. There are four categories to decide for partial entailment. They are Type1: both the sentences have the same meaning; hence direct entailment holds, Type 2 - If the second sentence has additional information along with the information from the first sentence, Type 3-first sentence has all information along with additional information, Type 4 - If both sentences have no similar information. Stop words are removed, sentences are tokenized, and POS tagging and stemming are done. Content word extraction and cosine similarity are measured. An accuracy of 0.56 is obtained [24].
6.
Malayalam language: Feature based method for entailment recognition is first attempted in Malayalam language using various features discussed in [58]. The text pairs are initially preprocessed through tokenization and stemming. Different features are used for classification, such as linguistic features, namely unigram match, bigram match, longest common subsequence, skip-gram match, and length features. Semantic features, namely word embedding similarity and TF-IDF similarity, are used. Set/Distance-based measures, namely Dice similarity, Cosine similarity, Levenstein similarity, Needle Wunsch similarity, Smith Watermann similarity, Jaro Wrinkler similarity, and Jaccard similarity, are also used. These features are used in a different set of combinations. Finally, the feature vector formed is classified using machine learning approaches, logistic regression, support vector machines, random forest, decision tree, multinomial naive Bayes, and Adaboost. The results are in Table 5. The authors also understand the impact of the dataset size, and observation has been made that the feature-based method’s performance drops as the dataset’s size increases. In such a scenario, deep learning methods are good alternatives as the model gets tuned with more number of instances.
Table 2 English RTE systemsFull size tableTable 3 Japanese and Chinese RTE systemsFull size tableTable 4 Arabic RTE systemsFull size tableTable 5 Malayalam RTE Multi-classification systemsFull size table8 Deep Learning MethodsDeep learning approaches start with Word embeddings, high-dimensional vectors generated for almost every language. RNN and its variants LSTM and GRU models that use sequential information are used. These models are used mainly because of the availability of larger datasets like SNLI, MNLI, and XNLI. The deep learning models are classified broadly into two categories: sentence encoding based models [16], which derive sentence representation using encoder models like RNN, LSTM, and match encoding based models focus directly on the matching information using attention mechanisms [59]. Tree-based CNN encoders [47] for sentence encoding and classification give an accuracy of 82.1%. BiLSTM-based sentence representation with inner attention is a hybrid model that pays more attention to functional or content words [44].Japanese & Chinese Language: Natural language inference is identified for legal documents from Japanese Civil code using a sentence encoding model and a decomposable attention model. Entailment is identified between a question and its related articles. The text and hypothesis are encoded using the sentence encoder model, and after that, two vectors are concatenated, and three RELU activation followed by logistic regression is done. The accuracy of these systems is 0.512 with 100 epochs [66]. [71] discussed the application of NLI in Commonsense Question Answering system. It is implemented as a Graph Relation Retrieval Reasoning Network which showed 1.74% improvement in precision compared with the existing methods.English language: A variant of tree structured topology is named as Child-Sum Tree-LSTM for obtaining sentence encoding. Feature generation and classification are based on distance-based approximation and perspective matching-based approximation. This method is applied to SNLI, SICK, RTE, and COLIEE dataset [1] and the results are in Table 6. Model named AF-DMN is a fusion of cross attention and self-attention to understand short-term and long-term interaction followed by prediction using softmax [25]. One-dimensional and two-dimensional convolutions are used to capture the semantics at different levels [34]. BiLSTM based dependent reading is employed to understand the dependency between text and hypothesis while encoding and also in the inference stage [27]. The prediction is finally using a multi-layer perceptron based on the aggregated information. An interactive inference network consisting of five components: the embedding layer, encoding layer, interaction layer, feature extraction layer, and output layer is discussed in [30]. The embedding layer converts words or phrases to vectors; the encoding layer uses these vectors in BiLSTM, RNN, or self-attention to model dependencies between sentences. The feature extraction layer used 2-D kernal CNN to extract semantic information, and the final output layer predicts the confidence of each class. In [51], a soft alignment matrix using neural and intra-sentence attention is used for more meaningful classification. This method used the SNLI dataset. Word level BiLSTM and attention applied to the word level representations from BiLSTM replacing the average pooling on the same sentence has improved performance with fewer model parameters [67]. [41] proposed and implemented a Siamese BERT based model helped to recognize similarity between texts using attention mechanism. Sentence representations are obtained using BERT transformer models.Malayalam Language: Siamese architecture is used to model the text and hypothesis separately in [57]. The system consists of an embedding layer that compares different approaches, such as word2vec, LASER, and XLM-R. The obtained embeddings are reduced in dimensions using PCA. It is then fed to a BiLSTM and GRU layer, producing output representations for text and hypothesis. These representations are concatenated and passed through dense layers, followed by sigmoid classification. The results are in Table 7. The application of convolutional neural networks to NLI has been attempted to understand the feasibility and performance of these models. One dimensional CNN, CNN having multiple channels of representation, CNN architecture for matching sentences over interaction space with fastText based embeddings. These approaches analyzed only the feasibility of these architectures, so no improvements from the best results are observed.Table 6 English RTE systemsFull size tableTable 7 Siamese model for RTE in MalayalamFull size table9 Sentence Representation Based MethodsRecently there has been a trend to use NLI as an evaluation strategy for various sentence representation models. The newer sentence embedding models are tested with NLI datasets to see how accurately the semantics of a sentence is transferred to its representative vector.[61] introduced a sequential neural encoder that uses word chunks in a sentence to form chunk-level vectors. A modified LSTM unit converts these chunk-level vectors to sequential encoding outputs. It is then concatenated with word vectors to create the final sentence representation. These methods are evaluated for tasks, namely Natural language inference and Sentiment Analysis.A graph-based semantic representation to extract meaningful relations between Arabic words and then represent the meaning as a rooted acyclic graph is proposed in [26]. The experiments were conducted with the Arabic textual entailment benchmark dataset ArbTED, which improved the accuracy by 8.6%. [45] discussed the usage of a deep LSTM encoder from an attention-based sequence to a sequence model trained for machine translation to be used for several NLP tasks, including textual entailment using the SNLI dataset. The sentence pair is encoded into variable-length representations by a BILSTM, and later, fixed representation is obtained through an aggregation function. Attention mechanism refined these representations, and a single vector is derived, which is used for classification [7].Inference in the Malayalam language has been conducted using different sentence representations for the MaNLI dataset. The various word and sentence embedding models used are paragraph vector (Doc2Vec) [23], fastText word representations, Bidirectional Encoder Representations from Transformers (BERT) [36], and Language Agnostic Sentence Representations (LASER) [6]. The observations using this dataset showed that LASER-based sentence representations capture much of the semantics of the Malayalam sentences in its representative vector, and sentence representation models are efficient for NLI classification for resource-constrained languages [56]. Additive attention is added to the dense layers, resulting in more meaningful representative sentence vectors that helped increase the classification accuracy through sigmoid function.9.1 Multilingual Models for Sentence RepresentationsRecently, models specific to English language are trained and fine tuned with multilingual corpora to make them more generic. As such many multilingual models exists now, which are extensively used in multilingual NLP application systems. NLI has been attempted in Indic languages with the IndicXNLI dataset. Indic-specific models, namely IndicBERT [35] and MuRIL [37], and other models, namely mBERT [54] and XLM-RoBERTa [19], are various multilingual models trained for different languages and used to evaluate the NLI sentence pairs [2].The accuracies for different sentence encoder models are shown in Table 8.Table 8 Sentence encoder models for NLIFull size table10 Evaluation MeasuresThe task of Natural language inference is a classification problem evaluated using the classification metrics such as precision, recall, accuracy, F1-score, support, and the confusion metrics to give a picture of the number of correctly and incorrectly classified instances.Confusion matrix: The confusion matrix is defined in terms of a few terms: 
1.
True Positive (TP), the number of actual positive samples the model classifies as positive.
2.
True Negative (TN) is the number of actual negative samples the model classifies as negative.
3.
False Positives (FP) is the number of actual negative samples incorrectly classified as positive.
4.
False Negative (FN) is the number of actual positive samples incorrectly classified as negative.
Classification Report: It consists of the following evaluation metrics. 
1.
Precision measures the correctness of the system and how a model correctly classifies each class. It is the fraction of true positives to the sum of true and false positives. $$\begin{aligned} Precision = \frac{TP}{TP+FP} \end{aligned}$$
                    (2)
2.
Recall is the model’s capability to find all positive samples. It is the ratio of true positives to the sum of true positives and false negatives. $$\begin{aligned} Recall = \frac{TP}{TP+FN} \end{aligned}$$
                    (3)
3.
Accuracy is the ratio of correctly classified positive and negative instances by the model to the total number of instances. $$\begin{aligned} Accuracy = \frac{TP+TN}{TP+TN+FP+FN} \end{aligned}$$
                    (4)
4.
F1-Score is the weighted harmonic mean of precision and recall. $$\begin{aligned} F1-score = \frac{2* (Recall * Precision)}{(Recall+Precision)} \end{aligned}$$
                    (5)
Few other metrics that can also be used for classification are:ROC-AUC Curve: ROC (Receiver Operating Characteristic is a graphical plot that shows the performance of a classification model. It is plotted against two parameters: True positive rate(Recall) and False Positive rate.$$\begin{aligned} True Positive Rate = \frac{TP}{TP+FN} \end{aligned}$$
                    (6)
                $$\begin{aligned} False Positive Rate = \frac{FP}{FP+TN} \end{aligned}$$
                    (7)
                Log Loss is another graphical plot that indicates the closeness of prediction probability to the corresponding actual value. The lower the log loss value, the more the predicted probability is close to the actual value.11 ObservationsThe growth of the dataset and type of approaches are analyzed over these years, and Table 9 represent the different datasets developed so far. The table below shows the shift in the approaches that deal with the NLI task.Table 9 RTE / NLI datasets in different languages developed till date, size of the dataset is in terms of number of sentence pairsFull size tableTable 10 Progress in approaches to NLIFull size table
1.
A critical revision on NLI over these years is with the development of the dataset. Over the years, RTE datasets are all peculiar concerning the kind of sentences, though the dataset size hasn’t grown much from the first RTE challenge to the latest RTE challenge. SNLI and its subsequent datasets have shown a drastic growth in the size and veracity of its contents. Later SNLI and MNLI datasets have undergone translations to many languages used worldwide. Currently, these are being used as the base data to work on NLI.
2.
Another attribute is the type of approaches used to deal with the NLI task. Recognizing textual entailment has now been renamed to natural language inference. Recognition of textual entailment or now known as natural language inference has been a classification task. It uses logic-based and rule-based approaches for the recognition task, and deep learning based pretrained models, and sentence representations, for the inference task.
3.
There has been a significant importance to application of entailment to other language processing applications like summarization, question answering, information retrieval, paraphrasing, information extraction, fake new detection [63] to the latest. During the RTE challenges there were tasks such as knowledge based population, novelty detection, summarization, which used RTE datasets or included textual entailment as its sub task. Later SNLI has been focussed only as an indiviual task of NLI and only very few works deal with applications of NLI or using NLI as an intermediate step in a bigger NLP applications. The focus has switched from integrating NLI into a bigger NLP application to obtain maximum classification accuracy with the NLI dataset using the latest deep learning and multilingual pretrained models.
4.
The current state we observe is that NLI has been turned into an evaluation strategy for almost all new models that produce sentence representations. The application-oriented theme for NLI, focussed from 2005 has been changed to a metric oriented concept for evaluating the performance of newer models, both monolingual and multilingual as in Table 10.
5.
With NLI being available to almost all languages, or the method to provision dataset to languages for which NLI is not available, will help application level systems like summarization and question answering systems in all languages to incorporate NLI as a necessary intermediate step in the future.
6.
NLI is a task related to semantics and machine understanding of a pair of sentences. From rule, logic, and ontology-based NLI systems that depend on other knowledge-based resources like WordNet, etc., the deep learning based representative sentence vectors produced from various multilingual models have eased the task of NLI giving more flexibility to the type of sentences the systems can handle and more performance accuracies. It is also a positive growth that would help more future research in NLI and open scope for improvement towards its applications.
12 Future OrientationsThe current state of NLI is focussing on the task as a classification as well as an evaluation method. NLI as classification task evaluates text pairs on the degree of similarity in terms of entailment, contradiction and neutral nature. The evaluation strategy is used to find the extent of similarity of the sentence pairs generated by various methods of generative and transformer models. Now NLI solutions stands isolated and is not integrated into many other NLP systems. In future NLI can be used in many of its application areas such as multidocument summarization, information extraction and question answering, to improve the performance of such systems.The focus of textual entailment as classification task has gained more importance compared with generation tasks. This is mainly due to lack of generative methods or challenges in text generation since 2005. Recent developments of generative AI models such as Generative Adversarial Networks can be applied to generate entailed sentences for a text. The sentence generation accuracy can be increased with improved generative models, which would help derive entailed sentences that can be used for summarization or semantic textual similarity tasks.The use of generic models that are applicable to low resource and languages other than English would help to develop NLI and NLP systems in almost all languages. The scope of explainable AI (XAI) in understanding NLI classification is also useful, when NLI becomes integral part of other systems. With XAI, the similarity of sentence pairs can be understood as a white box method from which inferences can be made that are really useful to subsequent NLP systems using NLI.13 ConclusionThroughout this study on the progressions happened to the field of NLI, the various datasets, both generic and application specific are discussed. Machine learning and deep learning approaches for textual entailment are also analyzed. Today, NLI is considered as an evaluation strategy for sentence similarity or sentence representation models that differ from the traditional classification task for recognizing entailments.There is a shift in NLI approach from the classification task to an evaluation strategy as seen today. NLI has also expanded to many languages other than English and this will go on. Also application oriented NLI is observed for few languages which will also expand to many languages. In the near future, NLI can also become an integral part of many multilingual language processing application. This survey observes the approaches and progressions, in terms of datasets and learning techniques in the field of NLI.
Noteshttp://aclweb.org/aclwiki/index.php?title=Textual Entailment Resource Pool.https://platform.openai.com/docs/models.https://ai4bharat.iitm.ac.in/areas/language-understanding/https://nlp.stanford.edu/projects/snli/https://cims.nyu.edu/ sbowman/multinli/https://github.com/facebookresearch/XNLI.https://github.com/SaraRenG/MaNLI-data.https://github.com/midas-research/hindi-nli-data/blob/master/README.md.https://indicxnli.github.io/https://huggingface.co/datasets/https://jgc128.github.io/mednli/https://physionet.org/content/rumednli-russian-inference/1.0.0/https://sites.ualberta.ca/ rabelo/COLIEE2022/ReferencesAdebayo KJ, Di Caro L, Robaldo L, Boella G (2016) Textual inference with tree-structured lstm. In: BNCAI, pp. 17–31Aggarwal D, Gupta V, Kunchukuttan A (2022) Indicxnli: evaluating multilingual inference for indian languages. arXiv preprint arXiv:2204.08776Alabbas M (2013) A dataset for Arabic textual entailment. Proceedings of the student research workshop associated with RANLP 2013:7–13
                    Google Scholar 
                Alabbas M, Ramsay A (2013) Optimising tree edit distance with subtrees for textual entailment. Proceedings of the international conference recent advances in natural language processing RANLP 2013:9–17
                    Google Scholar 
                Almarwani N, Diab M (2017) Arabic textual entailment with word embeddings. In: Proceedings of the third Arabic natural language processing workshop, pp 185–190Artetxe M, Schwenk H (2019) Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Trans Assoc Comput Linguist 7:597–610Article 
                    Google Scholar 
                Balazs J, Marrese-Taylor E, Loyola P, Matsuo Y (2017) Refining raw sentence representations for textual entailment recognition via attention. In: Proceedings of the 2nd workshop on evaluating vector space representations for NLP, pp 51–55Basak R, Naskar SK, Pakray P, Gelbukh A (2015) Recognizing textual entailment by soft dependency tree matching. Computación y Sistemas 19(4):685–700Article 
    MATH 
                    Google Scholar 
                Bensley J, Hickl A (2008) Workshop: Application of lcc’s groundhog system for rte-4. In TAC, CiteseerMATH 
                    Google Scholar 
                Bentivogli L, Clark P, Dagan I, Giampiccolo D (2009) The fifth pascal recognizing textual entailment challenge. In: TACBentivogli L, Clark P, Dagan I, Giampiccolo D (2011) The seventh pascal recognizing textual entailment challenge. In TAC, Citeseer
                    Google Scholar 
                Bentivogli L, Bernardi R, Marelli M, Menini S, Baroni M, Zamparelli R (2016) Sick through the semeval glasses. Lesson learned from the evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. Lang Resources Eval 50:95–124Article 
                    Google Scholar 
                Bojanowski P, Grave E, Joulin A, Mikolov T (2017) Enriching word vectors with subword information. Trans Assoc Comput Linguistics 5:135–146Article 
                    Google Scholar 
                Bos J, Zanzotto FM, Pennacchiotti M (2009) Textual entailment at evalita 2009. Proceedings of EVALITA, 2009(6.4):2Boudaa T, El Marouani M, Enneya N (2019) Alignment based approach for Arabic textual entailment. Proc Comput Sci 148:246–255Article 
    MATH 
                    Google Scholar 
                Bowman SR, Angeli G, Potts C, Manning CD (2015) A large annotated corpus for learning natural language inference. In: EMNLPChierchia G, McConnell-Ginet S (2000) Meaning and grammar: an introduction to semantics, 2nd edn. MIT Press, Cambridge, MAMATH 
                    Google Scholar 
                Conneau A, Rinott R, Lample G, Williams A, Bowman S R, Schwenk H, Stoyanov V (2018) Xnli: Evaluating cross-lingual sentence representations. In: Proceedings of the 2018 conference on empirical methods in natural language processing. Association for Computational LinguisticsConneau A, Khandelwal K, Goyal N, Chaudhary V, Wenzek G, Guzmán F, GraveÉ, Ott M, Zettlemoyer L, V Stoyanov (2020) Unsupervised cross-lingual representation learning at scale. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp 8440–8451Cooper R, Crouch R, van Eijck J, Fox C, van Genabith J, Jaspars J, Kamp H Pinkal M, Milward D, Poesio M et al (1996) Using the framework. Technical Report LRE 62-051 D-16Dagan I, Glickman O, Magnini B (2005) The pascal recognising textual entailment challenge. In: Machine learning challenges workshop, pp 177–190. SpringerDagan I, Dolan B, Magnini B, Roth D (2010) Recognizing textual entailment: rational, evaluation and approaches-erratum. Nat Lang Eng 16(1):105–105Article 
                    Google Scholar 
                Dai AM, Olah C, Le QV (2015) Document embedding with paragraph vectors. arXiv preprint arXiv:1507.07998Das A, Pal DR (2014) Exploring the partial textual entailment problem for bengali news texts. Res Comput Sci 86:43–52Article 
    MATH 
                    Google Scholar 
                Duan C, Cui L Chen X, Wei F, Zhu C, Zhao T (2018) Attention-fused deep matching network for natural language inference. In: IJCAI, pp 4033–4040Etaiwi W, Awajan A (2020) Graph-based Arabic text semantic representation. Inf Process Manage 57(3):102183Article 
    MATH 
                    Google Scholar 
                Ghaeini R, Hasan SA, Datla V, Liu J, Lee K, Qadir A, Ling Y, Prakash A, Fern X, Farri O (2018) Dr-bilstm: Dependent reading bidirectional lstm for natural language inference. In: Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (Long Papers), pp 1460–1469Ghuge S, Bhattacharya A (2014) Survey in textual entailment. Center for Indian Language Technology, retrieved on AprilGiampiccolo D, Magnini B, Dagan I, Dolan WB (2007) The third pascal recognizing textual entailment challenge. In: Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp 1–9Gong Y, Luo H, Zhang J (2018) Natural language inference over interaction space. In: International conference on learning representationsGuo M, Zhang Y, Zhao D, Liu T (2017) Generating textual entailment using residual lstms. In: Chinese computational linguistics and natural language processing based on naturally annotated big data, pp 263–272. SpringerHickl A, Bensley J (2007) A discourse commitment-based framework for recognizing textual entailment. In: Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp 171–176Hickl A, Williams J, Bensley J, Roberts K, Rink B, Shi Y (2006) Recognizing textual entailment with lcc’s groundhog system. In: Proceedings of the second PASCAL challenges workshop, vol 18Hu B, Lu Z, Li H, Chen Q (2014) Convolutional neural network architectures for matching natural language sentences. Adv Neural Inf Process Syst, 27Kakwani D, Kunchukuttan A, Golla S, NC G, Bhattacharyya A, Khapra MM, Kumar P (2020) IndicNLPSuite: monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for indian languages. In: Findings of EMNLPKenton JDM-WC, Toutanova LK (2019) Bert: pre-training of deep bidirectional transformers for language understanding. In: Proceedings of NAACL-HLT, pp 4171–4186Khanuja S, Bansal D, Mehtani S, Khosla S, Dey A, Gopalan B, Margam DK, Aggarwal P, Teja Nagipogu R, Dave S et al (2021) Muril: multilingual representations for Indian languages. arXiv e-prints: arXiv–2103DKhot T, Sabharwal A, Clark P (2018) Scitail: a textual entailment dataset from science question answering. In: Proceedings of the thirty-second AAAI conference on artificial intelligence and thirtieth innovative applications of artificial intelligence conference and eighth AAAI symposium on educational advances in artificial intelligence, pp. 5189–5197Kouylekov M, Magnini B (2005) Recognizing textual entailment with tree edit distance algorithms. In: Proceedings of the first challenge workshop recognising textual entailment, pp 17–20Le-Hong P, Cambria E (2023) A semantics-aware approach for multilingual natural language inference. Lang Resources Eval, pp 1–29Li R, Cheng L, Wang D, Tan J (2023a) Siamese bert architecture model with attention mechanism for textual semantic similarity. Multimedia Tools Appl, pp 1–22Li Z, Wu J, Miao J, Yu X, Li S (2023) A topic inference Chinese news headline generation method integrating copy mechanism. Neural Process Lett 55(2):1337–1353Article 
    MATH 
                    Google Scholar 
                Liu M, Guo Y, Nie L (2015) Recognizing entailment in chinese texts with feature combination. In: 2015 International conference on Asian language processing (IALP), pp 82–85. IEEELiu Y, Sun C, Lin L, Wang X (2016) Learning natural language inference using bidirectional lstm model and inner-attention. arXiv preprint: arXiv:1605.09090McCann B, Bradbury J, Xiong C, Socher R (2017) Learned in translation: contextualized word vectors. Adv Neural Inf Process Syst, 30Mishra A, Bhattacharyya P (2013) Deep learning techniques in textual entailment. Surv Pap Center Indian Lang Technol 273–282:2018MATH 
                    Google Scholar 
                Mou L, Men R, Li G, Xu Y, Zhang L, Yan R, Jin Z (2016) Natural language inference by tree-based convolution and heuristic matching. In: Proceedings of the 54th annual meeting of the association for computational linguistics (volume 2: Short Papers), pp 130–136Pakray P, Bandyopadhyay S, Gelbukh A (2011) Textual entailment using lexical and syntactic similarity. Int J Artif Intell Appl 2(1):43–58MATH 
                    Google Scholar 
                Pakray P, Neogi S, Bandyopadhyay S, Gelbukh A (2021) Recognizing textual entailment in non-english text via automatic translation into english. In: Mexican international conference on artificial intelligence, pp 26–35. SpringerPakray P, Bandyopadhyay S, Gelbukh AF (2013) Binary-class and multi-class based textual entailment system. In NTCIR, CiteseerMATH 
                    Google Scholar 
                Parikh A, Täckström O, Das D, Uszkoreit J (2016) A decomposable attention model for natural language inference. In: Proceedings of the 2016 conference on empirical methods in natural language processing, pp 2249–2255Pérez D, Alfonseca E (2005) Application of the bleu algorithm for recognising textual entailments. In: Proceedings of the first challenge workshop recognising textual entailment, pp 9–12. CiteseerRabelo J, Goebel R, Kim M-Y, Kano Y, Yoshioka M, Satoh K (2022) Overview and discussion of the competition on legal information extraction/entailment (coliee) 2021. Rev Socionetwork Strategies 16(1):111–133Article 
                    Google Scholar 
                Reimers N, Gurevych I (2020a) Making monolingual sentence embeddings multilingual using knowledge distillation. arXiv preprint: arXiv:2004.09813, 04Reimers N, Gurevych I (2020b) Making monolingual sentence embeddings multilingual using knowledge distillation. In: Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pp 4512–4525Renjit S, Idicula S (2021) Natural language inference for malayalam language using language agnostic sentence representation. PeerJ Comput Sci 7:e508Article 
                    Google Scholar 
                Renjit S, Idicula SM (2021b) Siamese networks for inference in malayalam language texts. In: Proceedings of the international conference on recent advances in natural language processing (RANLP 2021), pp 1167–1173Renjit S, Sumam MI (2022) Feature based entailment recognition for malayalam language texts. Int J Adv Comput Sci Appl, 13(2)Rocktäschel T, Grefenstette E, Hermann K M, Kočiskỳ T, Blunsom P (2015) Reasoning about entailment with neural attention. arXiv preprint arXiv:1509.06664Romanov A, Shivade C (2018) Lessons from natural language inference in the clinical domain. In: Proceedings of the 2018 conference on empirical methods in natural language processing, pp 1586–1596Ruan Y-P, Chen Q, Ling Z-H (2018) A sequential neural encoder with latent structured description for modeling sentences. IEEE/ACM Trans Audio Speech Lang Process (TASLP) 26(2):231–242Sadat M, Caragea C (2022) Scinli: A corpus for natural language inference on scientific text. In: Proceedings of the 60th annual meeting of the association for computational linguistics (volume 1: Long Papers), pp 7399–7409Sadeghi F, Bidgoly AJ, Amirkhani H (2022) Fake news detection on social media using a natural language inference approach. Multimedia Tools Appl 81(23):33801–33821Article 
    MATH 
                    Google Scholar 
                Saikh T, Naskar SK, Giri C, Bandyopadhyay S (2015) Textual entailment using different similarity metrics. In: International conference on intelligent text processing and computational linguistics, pp 491–501. SpringerC Shivade (2017) Mednli-a natural language inference dataset for the clinical domain. Published onlineSon NT, Phan V-A, Nguyen LM (2017) Recognizing entailments in legal texts using sentence encoding-based and decomposable attention models. In: COLIEE@ ICAIL, pp 31–42Sun C, Liu Y, Jia C, Liu B, Lin L (2017) Recognizing text entailment via bidirectional lstm model with inner-attention. In: International conference on intelligent computing, pp 448–457. SpringerUppal S, Gupta V, Swaminathan A, Zhang H, Mahata D, Gosangi R, Shah RR, Stent A (2020) Two-step classification using recasted data for low resource settings. In: Proceedings of the 1st conference of the Asia-Pacific chapter of the association for computational linguistics and the 10th international joint conference on natural language processing, pp 706–719, Suzhou, China, Association for Computational Linguistics. https://www.aclweb.org/anthology/2020.aacl-main.71Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, Liu H (2020) Medsts: a resource for clinical semantic textual similarity. Lang Resour Eval 54:57–72Article 
                    Google Scholar 
                Williams A, Nangia N, Bowman S (2018) A broad-coverage challenge corpus for sentence understanding through inference. In: Proceedings of the 2018 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (Long Papers), pp 1112–1122. Association for Computational Linguistics. http://aclweb.org/anthology/N18-1101Yang P, Liu Z, Li B, Zhang P (2022) Implicit relation inference with deep path extraction for commonsense question answering. Neural Process Lett 54(6):4751–4768Article 
    MATH 
                    Google Scholar 
                Yuret D, Rimell L, Han A (2013) Parser evaluation using textual entailments. Lang Resour Eval 47(3):639–659Article 
                    Google Scholar 
                Download referencesAuthor informationAuthors and AffiliationsDepartment of Computer Science, Cochin University of Science and Technology, Kalamassery, Kochi, Kerala, 682022, IndiaSara RenjitDepartment of Computer Science, Muthoot Institute of Technology and Science, Varikoli, Kochi, Kerala, 682308, IndiaSumam Mary IdiculaAuthorsSara RenjitView author publicationsYou can also search for this author in
                        PubMed Google ScholarSumam Mary IdiculaView author publicationsYou can also search for this author in
                        PubMed Google ScholarCorresponding authorCorrespondence to
                Sara Renjit.Additional informationPublisher's NoteSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Rights and permissions
Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.
Reprints and permissionsAbout this articleCite this articleRenjit, S., Idicula, S.M. A Study of the State of the Art Approaches and Datasets for Multilingual Natural Language Inference.
                    Neural Process Lett 56, 243 (2024). https://doi.org/10.1007/s11063-024-11673-2Download citationAccepted: 02 July 2024Published: 25 October 2024DOI: https://doi.org/10.1007/s11063-024-11673-2Share this articleAnyone you share the following link with will be able to read this content:Get shareable linkSorry, a shareable link is not currently available for this article.Copy to clipboard
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        KeywordsNatural language inferenceText entailmentDeep learningMachine learningRTENLI datasetsSentence representationsEmbeddingsEncodersSNLI
Use our pre-submission checklist
Avoid common mistakes on your manuscript.
Advertisement
Search
Search by keyword or author
Search
Navigation
                        Find a journal
                        Publish with us
                        Track your research
Discover content
Journals A-Z
Books A-Z
Publish with us
Journal finder
Publish your research
Open access publishing
Products and services
Our products
Librarians
Societies
Partners and advertisers
Our imprints
Springer
Nature Portfolio
BMC
Palgrave Macmillan
Apress
Your privacy choices/Manage cookies
Your US state privacy rights
Accessibility statement
Terms and conditions
Privacy policy
Help and support
Cancel contracts here
153.33.229.24
Not affiliated
© 2025 Springer Nature