Research on Inference and Training Acceleration of Large Language Model | Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial Intelligence
skip to main content
Advanced Search
Browse
About
                Sign in
                        Register
Advanced SearchJournalsMagazinesProceedingsBooksSIGsConferencesPeopleMore
Search ACM Digital LibrarySearchSearch
Advanced Search
10.1145/3703187.3703238acmotherconferencesArticle/Chapter ViewFull TextPublication PagescisaiConference Proceedingsconference-collectionscisaiConferenceProceedingsUpcoming EventsAuthorsAffiliationsAward WinnersMore
HomeConferencesCISAIProceedingsCISAI '24Research on Inference and Training Acceleration of Large Language Model
research-articleOpen access
Share on
Research on Inference and Training Acceleration of Large Language ModelAuthor: Qianyu ChenAuthors Info & ClaimsCISAI '24: Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial IntelligencePages 303 - 307https://doi.org/10.1145/3703187.3703238Published: 27 December 2024 Publication History
0citation43DownloadsMetricsTotal Citations0Total Downloads43Last 12 Months43Last 6 weeks43
Get Citation AlertsNew Citation Alert added!This alert has been successfully added and will be sent to:You will be notified whenever a record that you have chosen has been cited.To manage your alert preferences, click on the button below.Manage my AlertsNew Citation Alert!Please log in to your account
PDFeReaderContentsCISAI '24: Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial IntelligenceResearch on Inference and Training Acceleration of Large Language ModelPages 303 - 307PREVIOUS CHAPTERDouMH: Improving DouZero with Multiple HeadsPreviousNEXT CHAPTERImproved remote sensing image building extraction network based on U-net networkNextAbstract1 Introduction2 Related work3 Methodologies3.1 Model architecture optimization3.2 Distributed training strategy4 Experiments4.1 Experimental setups4.2 Experimental analysis5 ConclusionReferences
Information & ContributorsBibliometrics & CitationsView OptionsReferencesMediaTablesShareAbstractLarge language models have become an important research direction in the field of deep learning, and have received extensive attention from academia and industry. These models excel in natural language processing tasks, significantly improving the performance of downstream tasks. However, due to the large scale of the model, high-performance computing resources are required for deployment, and the latency problem in the inference process also limits its practical use in some industrial applications. Therefore, how to optimize these models to improve their practical application effect is still an urgent problem to be solved. In this work, by optimising the model architecture, introducing sparsity techniques, using quantisation methods and adopting distributed training strategies, we have achieved a substantial reduction in the computational overhead and memory requirements of large-scale language models, while simultaneously improving the inference speed and training efficiency. Firstly, the model architecture was optimised in order to reduce redundant calculations and enhance the parameter efficiency of the model, particularly in the design of the self-attention mechanism and the feedforward network layer. Secondly, the incorporation of sparsity technology has the potential to reduce the number of parameters and the amount of computation without a significant impact on the model's performance. This is achieved through the utilisation of sparse matrix multiplication and pruning techniques, which serve to minimise unnecessary computation. Furthermore, the quantization method markedly diminishes the memory footprint and bandwidth requirements by transforming the model weights and activation functions from high-precision floating-point numbers to low-precision representations, thereby enhancing the computational efficiency of the model. In the training process, a distributed training strategy was employed, utilising a combination of data parallelism and model parallelism to optimise the use of computing resources across multiple machines, thereby significantly reducing the training time. The results of the experimental analysis demonstrate that these methods can markedly enhance the speed of both inference and training, while concurrently reducing the consumption of resources, whilst maintaining the performance of the model.1 IntroductionSince the initial introduction of the pre-trained model BERT, there has been a notable increase in the research and application of pre-trained models. Many scholars have commenced in-depth exploration and practice within this field, resulting in the attainment of noteworthy research outcomes. The advent of pre-trained models has not only facilitated breakthroughs in the domain of natural language processing (NLP), but has also propelled the advancement of the entire field of deep learning. Pretrained language models are typically composed of multi-layer Transformer networks, and the training process encompasses two principal stages: pre-training and fine-tuning. In the pre-training phase, the model captures common natural language features through self-supervised learning on a vast corpus of unsupervised text [1]. This process allows the model to gain extensive linguistic knowledge without relying on data that is specific to a given task. Subsequently, the model proceeds to the fine-tuning stage, utilising task-specific labelled data for supervised learning. This enables the model to develop specific capabilities and achieve optimised performance through task-specific training. The two-stage training paradigm allows pre-trained models to excel in a range of NLP tasks, including text classification, named entity recognition.A substantial body of research has demonstrated that pre-trained models can efficiently learn common language representations on large-scale corpora. This not only enhances the performance of subsequent tasks but also circumvents the expenditure of time and resources typically associated with training the model from scratch. The considerable capabilities of pre-trained models have led to a notable advancement in the performance of numerous natural language processing tasks, thereby accelerating the development of related industrial applications [2]. These include intelligent customer service, machine translation, text analytics, and other fields, all of which have witnessed a considerable enhancement in performance and user experience due to the support of pre-trained models. Furthermore, as the scale and intricacy of pre-trained models continue to expand, researchers are actively investigating methods to enhance the training and inference efficiency of these models. Academia and industry are working in collaboration to advance the efficiency and cost-effectiveness of pre-trained models across a broader range of application scenarios [3]. This is being achieved through the introduction of advanced architectural designs, sparsity techniques, quantisation methods and distributed training strategies. These developments not only offer new avenues for future research but also hold significant promise for the real-world applications of natural language processing.However, with the advancement of computing capabilities, the advent of deep neural networks, and the sustained emergence of diverse training methodologies, the architectural framework of pre-trained models is undergoing a gradual transition from shallow to deep. Consequently, the performance of the model and the accuracy of task execution have been markedly enhanced [4]. Nevertheless, the exponential increase in model complexity has resulted in significant production costs and substantial computational challenges. The substantial computational and memory resources required by these large models, coupled with the high latency of the inference process, present significant challenges for their deployment in real-time devices. Pretrained language models, such as BERT, XLNet, and RoBERTa, are composed of multi-layered networks and millions or even billions of parameters, which renders them computationally expensive and inefficient in terms of memory consumption and inference latency [5]. The high computational demand and inefficiency of these models restrict their deployment in scenarios with stringent computing resource and latency requirements. Despite the considerable success of large-scale deep learning models in a range of tasks, their inherent complexity and substantial storage requirements present significant challenges to deployment in real-time applications, particularly in resource-constrained environments such as video surveillance systems and autonomous vehicles [6].Furthermore, the restricted computing capacity of the device presents a significant obstacle to the implementation of a comprehensive deep learning model in pioneering fields such as mobile technology or the Internet of Things. This limitation has significantly impeded the widespread deployment of pretrained language models in these domains. In accordance with the law of diminishing returns, an expansion in model size and computational power may result in incremental gains in performance. However, these gains are frequently marginal and insufficient to offset the high computational cost. In practice, the inference of large models often necessitates significant computational resources, which underscores the necessity to identify more efficient model architectures and optimisation techniques in numerous scenarios [7]. In order to address these challenges, researchers are exploring a variety of ways to reduce the computational overhead and memory requirements of pre-trained models. This includes the use of techniques such as model pruning, weighting, knowledge distillation, and other methods. The objective of these methods is to enhance the efficiency of the inference process and reduce the associated computational costs. This is achieved by reducing the number of model parameters or simplifying the model structure, thereby facilitating the deployment of pretrained language models in resource-constrained devices and real-time systems. These endeavours are propelling further advancement in the field of pretrained model technology, thereby conferring greater importance to this area of research.2 Related workIn a pioneering study, Han et al. [8] proposed a method to sparsify deep neural networks by removing non-significant synapses and subsequently training them to restore model performance. This optimisation method markedly reduces the number of redundant parameters in the network, resulting in a model that is more than 30 times smaller while maintaining high accuracy. The rationale behind this technique is that not all connections in a neural network are critical to the final output, and that many connection weights contribute less to the overall performance of the model. The pruning of these unimportant weights not only renders the model more compact and efficient, but also significantly reduces the computational and storage requirements.Fan et al. [9] put forth a strategy, designated as LayerDrop, which is a structured “dropout” approach. During the training phase, the LayerDrop technique introduces a regularisation effect by randomly discarding the entire layer, thereby enhancing the robustness and generalisation ability of the model. Concurrently, this approach enables the reasonable pruning of the model during the inference stage, thereby reducing the number of superfluous computational layers and optimising the inference speed. The fundamental concept of LayerDrop is to compel the model to learn more condensed and resilient representations by enabling the selective elimination of an entire network layer during the training phase. This approach has the additional benefit of preventing overfitting while also providing the model with greater flexibility during the inference phase. In the context of inference, depending on the specific requirements of the task or the constraints of the computational resources available, certain network layers can be selectively removed in order to reduce the computational overhead and accelerate the inference process without significantly affecting the performance of the model.Shen et al. [10] put forth an innovative method, designated as Q-BERT, which is capable of quantifying the BERT model to an ultra-low level of precision. This approach has the effect of significantly reducing the overall parameters and embedded tables of the model. In particular, the method reduces the size of the model parameters by up to 13 times, while simultaneously decreasing the storage requirements for embedded tables by up to 4 times. This novel quantisation technique effectively reduces the consumption of computing resources and memory usage by reducing the accuracy of the numerical representation without significantly affecting the performance of the model. The fundamental aspect of Q-BERT is its capacity to transform the weights and activation functions within the model into a low-precision format. This may entail a conversion from a 32-bit floating-point number to a numeric representation of 8 bits or less. This quantization has the additional benefit of reducing the storage requirements of the model, as well as alleviating pressure on memory bandwidth. As a result, it becomes possible to run deep learning models on resource-constrained hardware, such as mobile devices or edge computing devices. In comparison to conventional quantization techniques, Q-BERT is capable of maintaining the accuracy of the model while reducing its size, thereby ensuring efficient performance in a range of natural language processing tasks.In their seminal work, Kaya et al. [11] put forth the Shallow-Deep Networks model, which incorporates an internal classifier and mitigates the computational wastage associated with “overthinking” through an early exit mechanism based on confidence. This approach enables the model to terminate the calculation at an appropriate point, allowing for the performance of inference. This results in a reduction of the average inference cost by over 50%, while maintaining the accuracy of the model. Subsequently, Schwartz et al. [12] adopted a similar early exit mechanism and made improvements to it. The optimised model demonstrated an exceptional capacity to balance inference speed and accuracy, particularly in the domain of natural language inference, where it was five times faster than the prevailing state-of-the-art technology while maintaining a high level of accuracy. The innovative mechanism of Shallow-Deep Networks and subsequent improved models effectively addresses the issue of computational resource wastage during model inference. This not only enhances efficiency but also broadens the scope for practical applications of these models.3 MethodologiesIn this section, we aim to accelerate the inference and training process of large-scale language models by optimizing the model architecture, introducing sparsity techniques, applying quantization methods, and adopting distributed training strategies.3.1 Model architecture optimizationThe self-attention mechanism represents the fundamental component of the Transformer model, exhibiting a computational complexity of \(O( {{n}^2d} )\), where \(n\) denotes the length of the input sequence and \(d\) represents the dimension of the hidden layer. In the traditional self-attention mechanism, the attention matrix \(A\) is obtained by calculating the similarity between each pair of elements in the input sequence. This is achieved through the following formula: \begin{equation} A = softmax\left( {\frac{{Q{K}^T}}{{\sqrt {{d}_k} }}} \right) \end{equation} (1)where \(Q = X{W}_Q\), \(K = X{W}_K\), \(V = X{W}_V\), \(X\epsilon {R}^{d \times {d}_k}\), and \(X\epsilon {R}^{n \times d}\) represents the input matrix, while \({W}_Q\), \({W}_K\), \({W}_V\epsilon {R}^{d \times {d}_k}\) denotes the linear transformation matrix. In order to reduce the computational complexity, we introduce low-rank decomposition and kernel methods. Low-rank decomposition assumes that the attention matrix \(A\) can be approximated by two smaller matrices, \(U \in {R}^{n \times k}\) and \(V \in {R}^{k \times n}\), which is expressed as Equation 2. \begin{equation} A \approx U{V}^T \end{equation} (2)Note that the \(k \ll n\). In this way, the computational complexity is reduced from \(O( {{n}^2d} )\) is reduced to \(O( {nkd + {k}^2d} )\). Further optimization can be achieved by the nucleated attention method, where the attention score calculation is reconstructed into the dot product form of a set of kernel functions, so that the calculation can be performed in a lower dimension and expressed as Equation 3. \begin{equation} {A}_{ij} \approx \emptyset {\left( {{Q}_i} \right)}^T\emptyset \left( {{K}_j} \right) \end{equation} (3)where \(\emptyset ( \cdot )\) is a kernel function used for projection by applying a Gaussian kernel or trigonal kernel. This conversion reduces the complexity of attention computation to linear \(O( {n{k}^2d} )\), further reducing the computational requirements. Feedforward networks typically include two linear transformations and a nonlinear activation function. The traditional feedforward network takes the following Equation 4. \begin{equation} FFN\left( X \right) = \max \left( {0,X{W}_1 + {b}_1} \right){W}_2 + {b}_2 \end{equation} (4)Note that \({W}_1 \in {R}^{d \times {d}_{ff}}\), \({W}_2 \in {R}^{{d}_{ff} \times d}\). In large-scale models, the middle dimension \({d}_{ff}\) of the feedforward layer is usually much larger than the input-output dimension \(d\), which makes the parameter size very large. Therefore, we employ a strategy of factoring and sharing parameters. First, matrix factorization decomposes each large weight matrix into two smaller matrices, which is expressed as Equation 5. \begin{equation} {W}_1 = {U}_1V_1^T,\ {W}_2 = {U}_2V_2^T \end{equation} (5)where \({U}_1,\ {U}_2 \in {R}^{d \times r}\), \({V}_1,\ {V}_2 \in {R}^{r \times {d}_{ff}}\) and \(r \ll d\). This factorization reduces the number of parameters from \(2d{d}_{ff}\) to \(2dr + 2r{d}_{ff}\), significantly reducing the amount of computation and storage required. In addition, we have introduced a layer-by-layer parameter sharing strategy, which shares the same weight matrix across multi-layer networks to reduce redundancy. Assuming that the number of layers of the feedforward network is \(L\), and the total number of parameters of the original model is \(L \times ( {2d{d}_{ff}} )\), after using parameter sharing, the total number of parameters is reduced to \(2d{d}_{ff}\), which greatly reduces the computational overhead.Further, the core idea of sparsity techniques is to improve efficiency by reducing the number of parameters and the amount of computation in the model, which is especially important in large-scale language models. In deep learning, sparse matrix multiplication can significantly reduce computational requirements. For a given sparse matrix \(W\), its sparsity is defined as the proportion \(p\) of the non-zero element. If the weight matrix \(W\) of a fully connected layer is sparsified, the computational complexity is reduced from \(O( {{d}^2} )\) to \(O( {p{d}^2} )\), and the computational resource and storage cost savings are significant when \(p \ll 1\). To further improve the sparsity effect, we used a pruning technique based on the L1 norm. During training, we force unimportant weights to gradually approach zero by introducing L1 regularization terms into the objective function is expressed as Equation 6. \begin{equation} L = {L}_{original} + \lambda \mathop \sum \nolimits_{i,j} \left| {{W}_{ij}} \right| \end{equation} (6)3.2 Distributed training strategyIn order to train large-scale language models efficiently, especially in scenarios with tens of billions of parameters, distributed training strategies are indispensable. We adopted a strategy that combines data parallelism and model parallelism to optimize the use of computing resources across multiple machines, thereby significantly reducing training time. In the data parallelism policy, the dataset \(D\) is split into multiple subsets \({D}_1\), \({D}_2\), …, \({D}_N\), and distributed to different computing devices (such as GPUs). Each device independently calculates the loss function gradient on its subset. Each device independently calculates the loss function gradient \(\nabla {\mathcal{L}}_i\) on its subset, and then aggregates these gradients to update the model's global parameter is expressed as Equation 7. \begin{equation} W \leftarrow W - \eta \frac{1}{N}\mathop \sum \limits_{i = 1}^N \nabla {\mathcal{L}}_i \end{equation} (7)This strategy allows all compute nodes to process data in parallel, greatly improving the training speed. However, when the model is very large, the main problem with data parallelism is memory bottlenecks, as each device has to store a copy of the complete model. To combat memory bottlenecks, model parallelism strategies distribute different parts of the model across multiple devices. Suppose the model consists of \(M\) parts, i.e., \(f( x ) = {f}_M^\circ {f}_{M - 1}^\circ \cdots ^\circ {f}_1( x )\), and each device handles only a portion of forward and backward propagation. Specifically, for part \({f}_i\) on device \(i\), the forward propagation is expressed as Equation 8. \begin{equation} {x}_{i + i} = {f}_i\left( {{x}_i} \right) \end{equation} (8)In backward propagation, each device only needs to calculate the local gradient and communicate with the other devices to ensure that the gradient is updated through Equation 9. \begin{equation} \frac{{\partial \mathcal{L}}}{{\partial {x}_i}} = \frac{{\partial \mathcal{L}}}{{\partial {x}_{i + 1}}}\frac{{\partial {x}_{i + 1}}}{{\partial {x}_i}} \end{equation} (9)The benefit of this is that each device only needs to store the part of the model for which it is responsible, significantly reducing memory requirements and allowing larger models to be trained on multiple devices. On the basis of model parallelism, we also adopted the Pipeline Parallelism strategy. Divide the model into contiguous segments, each assigned to a different device. Each device receives the output of the previous device and immediately begins processing, thus forming a pipeline. Let the model be divided into \(S\) segments, each segment has a processing time of \(t\), and the throughput of the pipeline in parallel is expressed as Equation 10. \begin{equation} Throughput = \frac{1}{{t + \frac{{S - 1}}{S} \cdot t}} \end{equation} (10)Pipeline parallelism can greatly improve the utilization of hardware resources, especially in multi-GPU systems, which can reduce device idle time and improve training efficiency. To make the most of our hardware resources, we adopted a hybrid parallelism strategy that combines data parallelism, model parallelism, and pipeline parallelism. Data parallelism is used for parallel processing of different data samples, model parallelism is used to allocate different parts of a large model, and pipeline parallelism is used to efficiently perform forward and backward propagation of a model on each device. Through the combination of these distributed training strategies, we can efficiently train large-scale language models on multiple devices, significantly reducing training time, improving training efficiency, and expanding the trainable scale of the model.4 Experiments4.1 Experimental setupsInference and training acceleration experiments were conducted on large-scale language models, with an optimised Transformer architecture being adopted and validated on the MNLI and QNLI datasets. The Multi-Genre Natural Language Inference Corpus (MNLI) is a large-scale, multi-type natural language inference dataset designed to assess the model's capacity for reasoning in diverse contexts. The dataset comprises pairs of sentences drawn from a variety of textual domains. The objective is to ascertain whether the relationship between each pair of sentences can be characterised as entailed, contradictory, or neutral. The QNLI (Question-answering Natural Language Inference) dataset has been derived from the SQuAD dataset with the objective of evaluating the performance of the model in the question-answering inference task. The QNLI task requires the model to determine whether a given paragraph contains information that can be used to answer a particular question. This is a binary classification task. The introduction of INT8 quantization and 50% sparsity, in conjunction with a distributed training strategy that incorporates data parallelism and model parallelism, has led to a notable enhancement in the inference speed and training efficiency of the model, accompanied by a reduction in memory usage by 50%.4.2 Experimental analysisInference time refers to the amount of time a model spends making predictions. This metric is important for evaluating the efficiency of the model in real-world applications, especially in real-time application scenarios. The shorter the inference time, the more suitable the model will be in resource-constrained environments. Figure 1 is a comparison of the inference time of different methods on the two datasets. In each graph, the sparsity factor grows from 0.1 to 0.5, with the ordinate representing the inference time and the shaded portion representing the error range of the inference time. It can be seen that with the increase of the sparsity coefficient, the inference time of all methods decreases, and our method shows a lower inference time at all sparsity coefficients.Figure 1.Figure 1. Inference Time Comparison Results.Convergence velocity is defined as the number of iterations or training time required for a model to achieve the expected performance during training. The faster a model converges, the more rapidly it can be trained, while consuming fewer computational resources and maintaining high performance. Figure 2 shows comparison of convergence speeds of different methods. Figure 2 shows the convergence speed of four methods at different sample sizes. We can observe our method converges the fastest across all sample sizes.Figure 2.Figure 2. Convergence Speed Comparison Results.5 ConclusionIn conclusion, by optimizing the model architecture, introducing sparsity techniques, applying quantization methods, and adopting distributed training strategies, we have significantly accelerated the inference and training efficiency of large-scale language models. Experimental results show that compared with methods such as LayerDrop, Q-BERT and Shallow-Deep Networks, our method has superior performance in convergence speed, inference time, and computational efficiency, and can significantly reduce the requirement of computing resources while maintaining high performance. These optimization methods provide an effective solution for the deployment of large-scale language models in resource-constrained environments. In the future, we will further optimize these technologies and explore their combination with new model architectures to continue to promote the efficient development of deep learning.References[1]Wang, Yiding, et al. "Tabi: An efficient multi-level inference system for large language models." Proceedings of the Eighteenth European Conference on Computer Systems. 2023: 233-248.Google Scholar[2]Aminabadi, Reza Yazdani, et al. "Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale." SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE. 2022: 1-15.Google Scholar[3]Zhang, Zhenyu, et al. "H2o: Heavy-hitter oracle for efficient generative inference of large language models." Advances in Neural Information Processing Systems 36. 2024: 36.Google Scholar[4]Frantar, Elias, and Dan Alistarh. "Sparsegpt: Massive language models can be accurately pruned in one-shot." International Conference on Machine Learning. PMLR. 2023: 10323-10337.Google Scholar[5]Wortsman, Mitchell, et al. "Stable and low-precision training for large-scale vision-language models." Advances in Neural Information Processing Systems 36. 2023: 10271-10298.Google Scholar[6]Rasch, Malte J., et al. "Hardware-aware training for large-scale and diverse deep learning inference workloads using in-memory computing-based accelerators." Nature communications 14.1. 2023: 5282.Google Scholar[7]Lin, Ji, et al. "AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration." Proceedings of Machine Learning and Systems 6. 2024: 87-100.Google Scholar[8]Han, Song, Huizi Mao, and William J. Dally. "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding." arXiv preprint arXiv:1510.00149. 2015.Google Scholar[9]Fan, Angela, Edouard Grave, and Armand Joulin. "Reducing transformer depth on demand with structured dropout." arXiv preprint arXiv:1909.11556. 2019.Google Scholar[10]Shen, Sheng, et al. "Q-bert: Hessian based ultra low precision quantization of bert." Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 05. 2020.Google Scholar[11]Kaya, Yigitcan, Sanghyun Hong, and Tudor Dumitras. "Shallow-deep networks: Understanding and mitigating network overthinking." International conference on machine learning. (PMLR, 2019): 3301-3310.Google Scholar[12]Schwartz, Roy, et al. "The right tool for the job: Matching model and instance complexities." arXiv preprint arXiv:2004.07453. 2020.Google Scholar
Index Terms
Research on Inference and Training Acceleration of Large Language ModelTheory of computationComputational complexity and cryptographyQuantum complexity theory
Recommendations
Distributed Training of Large Language Models on AWS TrainiumSoCC '24: Proceedings of the 2024 ACM Symposium on Cloud Computing   Large language models (LLMs) are ubiquitously powerful but prohibitively expensive to train, often requiring thousands of compute devices, typically GPUs. To reduce the cost of training LLMs for customers, Amazon Web Services (AWS) launched the Amazon ...Read MoreHierarchical Training: Scaling Deep Recommendation Models on Large CPU ClustersKDD '21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining  
Neural network based recommendation models are widely used to power many internet-scale applications including product recommendation and feed ranking. As the models become more complex and more training data is required during training, improving the ...Read MoreEnabling Parallelism Hot Switching for Efficient Training of Large Language ModelsSOSP '24: Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles   Training of large-scale deep learning models necessitates parallelizing the model and data across numerous devices, and the choice of parallelism strategy substantially depends on the training workloads such as memory consumption, computation cost, and ...Read More
Comments
Please enable JavaScript to view thecomments powered by Disqus.
Information & ContributorsInformationPublished In
CISAI '24: Proceedings of the 2024 7th International Conference on Computer Information Science and Artificial IntelligenceSeptember 2024764  pagesISBN:9798400707254DOI:10.1145/3703187
Copyright © 2024 Copyright held by the owner/author(s).This work is licensed under a Creative Commons Attribution International 4.0 License.PublisherAssociation for Computing MachineryNew York, NY, United StatesPublication HistoryPublished: 27 December 2024Check for updatesAuthor TagsDistributed trainingInference accelerationLarge language modelSparse matrix multiplicationQualifiersResearch-articleConferenceCISAI 2024CISAI 2024: 2024 7th International Conference on Computer Information Science and Artificial IntelligenceSeptember 13 - 15, 2024Shaoxing, China
Contributors
Other MetricsView Article MetricsBibliometrics & CitationsBibliometrics
                Article Metrics
0Total Citations43Total DownloadsDownloads (Last 12 months)43Downloads (Last 6 weeks)43Reflects downloads up to 30 Dec 2024
Other MetricsView Author MetricsCitationsView OptionsView options PDFView or Download as a PDF file.PDF eReaderView online with eReader.eReader
Login optionsCheck if you have access through your login credentials or your institution to get full access on this article.Sign inFull AccessGet this Publication
MediaFiguresOtherTablesShareShareShare this Publication linkCopy LinkCopied!Copying failed.Share on social mediaXLinkedInRedditFacebookemailAffiliationsQianyu ChenSchool of Information Science and Technology ShanghaiTech University, Shanghai, China [email protected]https://orcid.org/0009-0005-2371-115XSearch about this authorView full text|Download PDF
View Table of Contents
Export CitationsSelect Citation formatBibTeXEndNoteACM RefPlease download or close your previous search result export first before starting a new bulk export.Preview is not available.By clicking download,a status dialog will open to start the export process. The process may takea few minutes but once it finishes a file will be downloadable from your browser. You may continue to browse the DL while the export process is in progress.DownloadDownload citationCopy citation
Footer
Categories
Journals
Magazines
Books
Proceedings
SIGs
Conferences
Collections
People
About
About ACM Digital Library
ACM Digital Library Board
Subscription Information
Author Guidelines
Using ACM Digital Library
All Holdings within the ACM Digital Library
ACM Computing Classification System
Accessibility Statement
Join
Join ACM
Join SIGs
Subscribe to Publications
Institutions and Libraries
Connect
Contact us via email
ACM on Facebook
ACM DL on X
ACM on Linkedin
Send Feedback
Submit a Bug Report
The ACM Digital Library is published by the Association for Computing Machinery. Copyright © 2025 ACM, Inc.
Terms of Usage
Privacy Policy
Code of Ethics
Your Search Results Download Request We are preparing your search results for download ...We will inform you here when the file is ready.Download now!Your Search Results Download RequestYour file of search results citations is now ready.Download now!Your Search Results Download RequestYour search export query has expired. Please try again.