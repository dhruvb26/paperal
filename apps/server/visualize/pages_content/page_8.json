["BookC2\nBooks3\nCC\nEuroParl\nFreeLaw\nGithub\nGutenberg\nHackerNews\nMath\nOWT2\nOpenSubs\nPhilPapers\nPubMed Abs.\nPubMed Cen.\nStack\nUSPTO\nUbuntu\nWiki\nYTSubs\narXiv\nDataset\nTrue positive\nFalse positive\n<1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 0.01 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.00\n0.58\n0\n1\nFigure 4: p-values of dataset inference By applying dataset inference to Pythia-12b models with 1000\ndata points, we observe that we can correctly distinguish train and validation splits of the PILE with very\nlow p-values (always below 0.1). Also, when considering false positives for comparing two validation subsets,\nwe observe a p-value higher than 0.1 in all cases, indicating no false positives.\nScore Aggregation\nTo aggregate scores from different MIAs, we (i) normalize feature values to\nensure that all features aggregated across various membership inference attacks are on a comparable scale.\nThen, we (ii) adjust values of outliers before learning correlations with the classifier by replacing the\ntop and bottom 2.5% of outlier values with the mean of that (normalized) feature. Finally, we (iii) remove\noutliers before performing t-test in Stage 3 once we have a single membership value from the regressor\noutputs for each sample in the B splits of the suspect and validation sets. Once again we remove the top\nand bottom 2.5% of outlier.\n5.2\nAssumptions for Dataset Inference\nIn order to operationalize dataset inference, we must obey certain assumptions on both the datasets (points\n1 and 2 below), and the suspect language model (point 3 below).\n1. The suspect train set and the unseen validation sets should be IID. This prevents the results from being\nconfounded due to distribution shifts (such as temporal shifts in the case of WikiMIA).\n2. We must ensure no leakage between the (suspected) train and (unseen) validation sets. The validation\nset should be strictly private, and only accessible to the victim.\n3. We need access to the output loss of the suspect LLM in order to perform various MIAs.\n5.3\nExperimental Details\nDatasets and Architectures\nWe perform dataset inference experiments on all 20 subsets of the PILE.\nFor experiments with false positives, we split the validation sets into two subsets of 500 examples each. In\nall other experiments, we compare 1000 examples of train and validation sets of the PILE [Gao et al., 2020].\nWe perform dataset inference on models from the Pythia [Biderman et al., 2023] family at 410M, 1.4B,\n6.9B, and 12B scales. These open-source models allow us to know exactly which examples trained on.\nMIAs used\nIn our experiments, we aggregate 52 different Membership Inference Attacks (MIAs) in Stage\n1 (many of which are overlapping and only differ in whether they capture the perplexity or the log-likelihood,\nor contrast the ratios or differences of model predictions). For the linear regression model trained in Stage 2,\nwe train for 1000 updates over the data using simple weights over the 52 features. A total of 1000 examples\nare saved for training the regressor to learn correlations for stage 2, except in the false positive experiments\nwhere we use half the data. A complete list of all the MIAs used in our work is present in Appendix C.\n5.4\nAnalysis and Results with Dataset Inference\nWe analyze the performance of LLM dataset inference on the Pythia suite of models [Biderman et al.,\n2023] trained on the Pile dataset [Gao et al., 2020]. We separately perform dataset inference on each and\nevery subset of the PILE using the provided train and validation sets, and report the p-values for correctly\n9\n"]