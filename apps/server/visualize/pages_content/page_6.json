["20\n40\n60\nK\n0.4\n0.5\n0.6\nAUC\nModel Parameters\n410m\n1.4b\n6.9b\n(a) Performance for different model sizes.\n5.0\n10.0\n20.0\n30.0\n40.0\n50.0\n60.0\nK\n0.00\n0.25\n0.50\n0.75\n1.00\nAUC\nMin-K Prob\nReversed Train/Val\n(b) False Positives when reversing train/val sets.\nFigure 2: Comparative analysis of the Min-k% Prob [Shi et al., 2024].\nWe measure the\nperformance (a) across different model sizes and (b) the observed reversal effect. The method performs\nclose to a random guess on non-members from the Pile validation sets.\nBookC2\nBooks3\nCC\nEuroParl\nFreeLaw\nGithub\nGutenberg\nHackerNews\nMath\nOWT2\nOpenSubs\nPhilPapers\nPubMed Abs.\nPubMed Cen.\nStack\nUSPTO\nUbuntu\nWiki\nYTSubs\narXiv\nDataset\nMax-10% Prob\nMin-10% Prob\nPerplexity\nPerturbation-based\nReference-based\nZlib Ratio\nMetric\n0.49 0.43 0.45 0.48 0.49 0.47 0.44 0.48 0.49 0.47 0.45 0.38 0.57 0.46 0.35 0.44 0.65 0.46 0.62 0.47\n0.48 0.46 0.51 0.49 0.50 0.50 0.51 0.53 0.49 0.52 0.50 0.52 0.51 0.49 0.49 0.48 0.42 0.59 0.55 0.48\n0.54 0.57 0.53 0.51 0.50 0.50 0.54 0.48 0.52 0.50 0.54 0.59 0.48 0.53 0.63 0.55 0.30 0.47 0.40 0.53\n0.53 0.55 0.52 0.45 0.53 0.48 0.59 0.54 0.48 0.50 0.56 0.70 0.32 0.52 0.42 0.48 0.47 0.48 0.55 0.51\n0.57 0.52 0.49 0.50 0.49 0.49 0.47 0.42 0.50 0.48 0.52 0.42 0.53 0.50 0.50 0.53 0.45 0.45 0.45 0.50\n0.48 0.56 0.52 0.47 0.51 0.50 0.51 0.49 0.51 0.48 0.57 0.69 0.36 0.54 0.60 0.50 0.43 0.50 0.45 0.55\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 3: Performance of various MIAs on different subsets of the Pile dataset. We report\n6 different MIAs based on the best performing ones across various categories like reference based, and\nperturbation based methods (Section 2.1). An effective MIA must have an AUC much greater than 0.5.\nFew methods meet this criterion for specific datasets, but the success is not consistent across datasets.\ndatasets, (3) experiments must be performed over multiple data distributions (4) careful experimentation\nmust be done on both false positives and false negatives to ensure MIAs do not wrongly label non-members\nas members.\n5\nLLM Dataset Inference\nDataset inference builds on the idea of membership inference by leveraging distributional properties to\ndetermine if a model was trained on a particular dataset. While MIAs operate at the instance level\u2014aiming\nto identify whether each example was part of the training data. In the previous sections, we have shown\nthat MIAs often yield signals that is close to random in determining example membership. However, if\nwe achieve even slightly better than random accuracy in inferring membership, we can aggregate these\nattacks across multiple examples to perform a statistical test. This test can then distinguish between the\ndistributions of the model\u2019s training and validation sets. In the context of LLM dataset inference, we\ncombine all the MIA methods discussed in Section 2.1.\n7\n"]