["Preprint", "P (p < \u03b1) \u2192 \u03b1 uniformly as m \u2192 \u221e (Lehmann & Romano, 2005, Theorem 11.4.5).", "This result ensures that the sharded rank comparison test also provides (asymptotic) guarantees on false positive rates, much like the permutation test. The test we propose here has two small dif- ferences relative to the permutation test \u2013 it provides asymptotic, rather than finite-sample valid p-values and assumes i.i.d X for the proof. These conditions could be relaxed by the use of Berry- Esseen bounds to obtain finite-sample convergence rates for the CLT as well as replacing our use of a standard central limit theorem with one applicable to the sums of exchangable random vari- ables. However, we opted to present the simpler asymptotic test given the frequent use of i.i.d data generation assumption in the literature as well as the fast convergence of the CLT in practice.", "4 EXPERIMENTS", "We now demonstrate that our test is effective for detecting many common forms of test set contam- ination. We begin by training a 1.4 billion parameter language model, consisting of both Wikipedia and a known collection of exchangeable test sets. These canaries serve as positive controls for our test, and our goal will be to flag as many of these as possible. Having validated the test in a setting with known contamination, we then explore its use with existing open models.", "4.1 PRETRAINING WITH INTENTIONAL CONTAMINATION", "Datasets and training To validate our test statistic, we train a 1.4 billion parameter GPT-2 model from scratch with a combination of standard pretraining data (Wikitext, taken from the RedPajama corpus (Together Computer, 2023)) and known test sets. We derive 10 test sets from numerous standard datasets (BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), OpenbookQA (Mi- haylov et al., 2018b), MNLI (Williams et al., 2018), Natural Questions (Kwiatkowski et al., 2019a), TruthfulQA (Lin et al., 2022), PIQA (Bisk et al., 2019), MMLU (Hendrycks et al., 2021)), and sub- sample the datasets to around 1000 examples to ensure that the test sets remain a small part of the overall pretraining dataset (See Table 1 for exact sizes). While we do not know if these datasets are exchangable when they were constructed, we can make them exchangable simply by applying a random shuffle to the dataset, which would make all orderings of the examples equally likely.", "To test our ability to detect benchmarks at various duplication rates, we duplicate each of the datasets a different number of times - ranging from 1 to 100 (See Table 1). The overall pretraining dataset has 20.2B tokens, with 20M tokens associated with some benchmark dataset.", "Test parameters The sharded rank comparison test requires two additional parameters: the shard count m and the permutation count r. Thoughout these experiments we use m = 50 shards and r = 51 permutations. In our ablations below, we found that the tests are not particularly sensitive to these parameters, and we fix these parameters to avoid the possibility of p-hacking.", "Canary Results In Table 1, we find that our test is highly sensitive, and provides near-zero p- values at duplication rates of 10 or above. These detections hold for relatively small datasets (\u2264 1000 examples) and for a modestly sized language model with 1.4 billion parameters. Given that many test sets are much larger in practice, and many language models of interest are much larger and memorize more aggressively (Carlini et al., 2019), these findings suggest that our test is likely to detect contamination in practice.", "While the permutation test attains significance (at a typical \u03b1 = 0.05, say) for all benchmarks duplicated at least 10 times, the p-values are bounded below by 1/(1 + r), where the number of permutations r used here is 100. Results for our sharded test use r = 50; even with half the compute, the sharded test attains comparable performance for benchmarks with small duplication rate. However, the p-values attained by the sharded test for moderate to high duplication rates are vanishingly small.", "Attaining comparably low p-values using the permutation test is computationally infeasible. For example, to allow for the possibility of a p-value as low as 1.96e-11 (matching the MNLI result) would require permuting the dataset 1011 times, and as many forward passes of the model.", "6"]