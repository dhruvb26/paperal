Preprint
Figure 10: The “Drawing App” generated by MetaGPT.
C
EXPERIMENTS
C.1
DETAILS OF THE SOFTWAREDEV DATASET
The SoftwareDev dataset includes 70 diverse software development tasks. Table 8 displays the
names and detailed prompts of 11 tasks within the dataset. Note that the first seven tasks listed are
used in the main experiments of this paper.
C.2
ADDITIONAL RESULTS
Quantitative results of MetaGPT
As shown in Table 4, MetaGPT achieves an average score
of 3.9, surpassing ChatDev’s score of 2.1 Zhao et al. (2023), which is based on the Chat chain.
Compare the scores of general intelligent algorithms, including AutoGPT Torantulino et al. (2023),
which all score 1.0, failing to generate executable code. We observe that the generated code is often
short, lacks comprehensive logic, and tends to fail to handle cross-file dependencies correctly.
While models such as AutoGPT (Torantulino et al., 2023), Langchain (Chase, 2022), and Agent-
Verse (Chen et al., 2023) display robust general problem-solving capabilities, they lack an essential
element for developing complex systems: systematically deconstructing requirements. Conversely,
MetaGPT simplifies the process of transforming abstract requirements into detailed class and func-
tion designs through a specialized division of labor and SOPs workflow. When compared to Chat-
Dev (Zhao et al., 2023), MetaGPT’s structured messaging and feedback mechanisms not only reduce
loss of communication information but also improve the execution of code.
Quantitative results of MetaGPT w/o executable feedback
Table 9 presents the performance of
MetaGPT with GPT-4 32K on 11 tasks within the SoftwareDev dataset. It also shows the average
performance across all 70 tasks (in the last line). Note that the version of MetaGPT used here is the
basic version without the executable feedback mechanism.
Quantitative results of MetaGPT with different LLMs
To verify the performance of MetaGPT
on different LLMs, we randomly selected 5 SoftwareDev tasks and conducted experiments using
GPT-3.5 and Deepseek Coder 33B5 as backends. As shown in Table 5, the results indicate that
although MetaGPT can complete tasks with these LLMs, using GPT-4 as the backend yields superior
performance.
5https://deepseekcoder.github.io
23
