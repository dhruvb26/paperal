["Preprint", "Pre-training Data Contamination Test Canonical Order Shuffled Order The music was composed by Hitoshi Sakinoto, who had also worked on the previous Valkyria Chronicles ganes. BELO \u00a9 biel) HE=D Cle Gir ei CLO Does a frog jump out of boiling water? it possible to create mass fron energy? 2 Does @ frog jump out of boiling water? | 18 at possible to create mass fron energy? @ | Is it possible to create mass from energy? @ Test Set | Is it possible to create mass from enersy?| 15 there a movie with 0 on rotten tomatoes? @ | Is the jaguar S type rear wheel drive? Contamination] rs there a movie with 0 on rotten tonatoes? i Is the jaguar S type rear wheel drive? | 1 the jaguar $ type rear wheel drive? \u00a9 | rs there a movie with @ on rotten tomatoes? highmaye9 was crested out of highway rerouting in @ high model log-probability low model log-probability she dat slants elo Differences in log-probability between orderings reveal contamination.", "Figure 1: Given a pre-training dataset contaminated with the BoolQ(Clark et al., 2019) test set (left), we detect such contamination by testing for exchangability of the dataset (right). If a model has seen a benchmark dataset, it will have a preference for the canonical order (i.e. the order that examples are given in public repositories) over randomly shuffled examples orderings. We test for these differences in log probabilities, and aggregate them across the dataset to provide false positive rate guarantees.", "2019; Mattern et al., 2023) as well as provide some evidence for test set contamination (Sainz et al., 2023; Golchin & Surdeanu, 2023). However, the heuristic nature of these methods limits their usefulness, as these methods cannot elevate speculation about a suspected instance of test set con- tamination into an irrefutable proof of contamination.", "In this work, we show it is possible to go beyond heuristics and provide provable guarantees of test set contamination in black box language models. More specifically, we provide a statistical test that can identify the presence of a benchmark in the pre-training dataset of a language model with provable false positive rate guarantees and without access to the model\u2019s training data or weights.", "To achieve these guarantees, we exploit the fact that many datasets have a property known as ex- changeability, where the order of examples in the dataset can be shuffled without affecting its joint distribution. Our key insight is that if a language model shows a preference for any particular or- dering of the dataset \u2013 such as a canonical ordering that appears in publicly available repositories \u2013 this violates exchangeability and can only occur by observing the dataset during training (Figure 1). We leverage this insight to propose a set of tests that compares the language model\u2019s log probabil- ity on the \u2018canonical\u2019 ordering (taken from public repositories) to the log probability on a dataset with shuffled examples, and flag a dataset if the two log probabilities have statistically significant differences.", "Using these ideas, we propose a computationally efficient and statistically powerful test for con- tamination which shards the dataset into smaller segments and performs a series of log probability comparisons within each shard. We prove that this sharded test provides control over the false pos- itive rate, enables computationally efficient parallel tests, and substantially improves the power of the test for small p-values.", "We evaluate our statistical test on a 1.4 billion parameter language model trained on a combination of Wikipedia and a curated set of canary test sets. Our test is sensitive enough to identify test sets with as few as 1000 examples, and sometimes even appearing only twice in the pretraining corpus. In the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain vanishingly small p-values on our test. Finally, we run our test on four commonly used, public language models to study the behavior of our test on language models in the wild and find little evidence of pervasive and strong test set contamination.", "We summarize our contributions below.", "\u2022 Demonstrating the use of exchangability as a way to provably identify test set contamina- tion using only log probability queries.", "\u2022 Construction of an efficient and powerful sharded hypothesis test for test set contamination.", "\u2022 Empirical demonstration of black-box detection of contamination for small datasets that appear few times during pretraining.", "2"]