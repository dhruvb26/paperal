["6\nDiscussions\nMembership Inference for LLMs.\nIn this work, we question the central foundations of research on\nmembership inference in the context of LLMs trained on trillions of tokens of web data. Our findings indicate\nthat current membership inference attacks for LLMs are as good as random guessing. We demonstrate that\npast successes in MIAs are often due to specific experimental confounders rather than inherent vulnerabilities.\nWe provide guidelines for future researchers to conduct robust experiments, emphasizing the use of IID\nsplits, considering various data distributions, assessing false positives, and using multiple random seeds to\navoid confounders.\nShift to LLM Dataset Inference.\nHistorically, membership inference focused on whether an individual\ndata point was part of a training dataset. Instead, we aggregate multiple data points from individual\nentities, forming what we now consider a dataset. In our work, we have not only put thought towards the\nscientific framework of dataset inference but also the ways it will operationalize in real-world settings, for\ninstance, through our running example of a writer who suspects that their books were trained on. Our\nresearch demonstrates that LLM dataset inference is effective in minimizing false positives and detecting\neven minute differences between training and test splits of IID samples.\nLimitations\nA central limitation to dataset inference is the assumptions under which it can be performed.\nMore specifically, we require that the training and validation sets must be IID, and the validation set must\nbe completely private to the victim. While this may appear elusive a priori, we outline concrete scenarios to\nshow how these sets naturally occur. For instance, through multiple drafts of a book, until one gets finalized.\nThe same applies to many artistic and creative uses of LLMs across language and vision today. In terms of\ndata and model access, we assume that the victim or a trusted third party, such as law enforcement, is\nresponsible for running the dataset inference so that there are no privacy-related concerns. This will require\nthe necessary legal framework to be brought in place, or otherwise suspect adversaries may deny querying\ntheir model altogether.\n7\nAcknowledgements\nWe would like to acknowledge our sponsors, who support our research with financial and in-kind contributions:\nAmazon, Apple, CIFAR through the Canada CIFAR AI Chair, Meta, NSERC through the Discovery Grant\nand an Alliance Grant with ServiceNow and DRDC, the Ontario Early Researcher Award, the Schmidt\nSciences foundation through the AI2050 Early Career Fellow program, and the Sloan Foundation. Resources\nused in preparing this research were provided, in part, by the Province of Ontario, the Government of\nCanada through CIFAR, and companies sponsoring the Vector Institute. Pratyush Maini was supported by\nDARPA GARD Contract HR00112020006.\n"]