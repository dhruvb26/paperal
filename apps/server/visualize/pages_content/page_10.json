["Preprint", "using Fisher\u2019s method (Fisher (1934)). Although the omnibus p-values resulting from this proce- dure can no longer provide a proof of contamination (due to non-independence and heuristic nature of the fitering step), their magnitude serves as heuristic evidence for contamination. The resulting p-values and empirical CDFs (Figure 4) of the 43 test sets are indicate mild deviation from the null hypothesis, consistent with the findings of mild test set contamination in Touvron et al. (2023).", "5 RELATED WORK", "Our work relates to a large literature on data memorization, privacy, and membership inference attacks for large langauge models. We discuss some of the most relevant works to ours below.", "There is a substantial literature studying memorization of data in large language models, often from the privacy perspective (Carlini et al., 2021; 2019; Kandpal et al., 2022; Mattern et al., 2023; Carlini et al., 2023). Most of these works have focused on analyses of what is memorized and whether pri- vate information can be extracted from a large langauge model, but do not build tests to specifically identify test set contamination. Our work has a narrower focus on test set contamination, but this also allows us to build tests that provide more precise guarantees of contamination.", "Data contamination has been studied in many contexts, including in the study of pretraining corpora ((Dodge et al., 2021)) as well as in the analysis section of many language model papers (Hoffmann et al., 2022; Brown et al., 2020a; Gao et al., 2020). The n-gram based analyses in these papers can shed light on contamination, but they can have high false positives (e.g. SQuAD (Rajpurkar et al., 2016) containing Wikipedia) and are limited to those datasets chosen for analysis. Our approach enables third party tests of dataset contamination with only access to log probabilities, enabling broader testing, without having to trust the model provider.", "For third-party tests of contamination, there have been a few recently proposed heuristics. Sainz et al. (2023) propose to identify test set contamination in GPT-3 and GPT-4 by prompting the models to generate verbatim examples from a test set. The contemporaneous work of Golchin & Surdeanu (2023) similarly proposes to identify contamination in black-box models by prompting a model to generate completions of random example prefixes and using GPT-4 to judge the closeness between the completion and the ground truth. While these approaches are efficient and do not require access to pre-training data, they do not enjoy the same provable false-positive guarantees of our work and require strong memorization that is detectable in generated outputs.", "Closest to our work is the exposure statistic in Carlini et al. (2019) and other subsequent variations (Mattern et al. (2023)), which tests the perplexity differences between a target sequence and random sequences. The idea of comparing the rank of the target log probability to some baseline distribution is similar to our work. However, our work is distinct in using the exchangability of datasets to obtain an exact null distribution (giving us provable guarantees when identifying contamination) and in developing a sensitive and efficient shard-based test.", "Beyond language modeling, identifying the presence of a particular set of examples in the training data of a machine learning model is related to the security and privacy topic of membership inference (Shokri et al. (2017); Mattern et al. (2023)). Our work contributes to this literature by developing a new form of membership inference attack that leverages the exchangability of benchmark datasets.", "6 LIMITATIONS", "We highlight a few limitations of our approach for detecting test set contamination. First, the p- values presented in this paper do not have multiple test corrections applied, as it is difficult to define the \u2018total number of hypotheses\u2019 tested throughout development.", "Second, any application of this test in practice will likely involve taking an off-the-shelf benchmark dataset X, for which it will be difficult to know if the dataset is truly exchangable. Heuristic negative controls such as our BioMedLM experiments can be helpful, but we cannot ever prove that a dataset is exchangable without knowing its data generating process. We strongly encourage future dataset creators to apply a random shuffle to their datasets (and to publicize this fact), which would allow our tests to be applied.", "10"]