["Preprint\nTable 4: Executability comparison. The executability scores are on a grading system ranging from\n\u20191\u2019 to \u20194\u2019. A score of \u20191\u2019 signifies complete failure, \u20192\u2019 denotes executable code, \u20193\u2019 represents\nlargely satisfying expected workflow, and \u20194\u2019 indicates a perfect match with expectations.\nTask\nAutoGPT\nLangChain\nAgentVerse\nChatDev\nMetaGPT\nFlappy bird\n1\n1\n1\n2\n3\nTank battle game\n1\n1\n1\n2\n4\n2048 game\n1\n1\n1\n1\n4\nSnake game\n1\n1\n1\n3\n4\nBrick breaker game\n1\n1\n1\n1\n4\nExcel data process\n1\n1\n1\n4\n4\nCRUD manage\n1\n1\n1\n2\n4\nAverage score\n1.0\n1.0\n1.0\n2.1\n3.9\nTable 5: Performance of MetaGPT on SoftwareDev using different LLMs as agent backends.\nModel\nOpen source Time(/s) # Lines Executability Revisions\nMetaGPT (w/ GPT-3.5)\n%\n75.18\n161.6\n2.8\n2.4\nMetaGPT (w/ GPT-4)\n%\n552.94\n178.2\n3.8\n1.2\nMetaGPT (w/ Deepseek Coder 33B)\n\"\n1186.20\n120.2\n1.4\n2.6\nImpact of Instruction Levels (High-level v.s. Detailed Instructions)\nDoes the variation in the\nlevel of initial input from humans significantly influence performance outcomes? For examples:\n1. High-level prompt: Create a brick breaker game.\n2. Detailed prompt: Creating a brick breaker game. In a brick breaker game, the player\ntypically controls a paddle at the bottom of the screen to bounce a ball towards a wall of\nbricks. The goal is to break all the bricks by hitting them with the ball.\nAdditional experiments were conducted to investigate this aspect: we selected 5 tasks from Soft-\nwareDev, and constructed detailed prompts for them. Here are the experimental results:\nTable 6: Impact of Instruction Levels. The executability is scored on a grading system ranging\nfrom \u20181\u2019 to \u20184\u2019. A score of \u20181\u2019 signifies complete failure, \u20182\u2019 denotes runnable code, \u20183\u2019 represents\nlargely expected workflow, and \u20184\u2019 indicates a perfect match to expectations.\nModel\n# Word Time(/s) Token usage # Lines Executability Productivity Reversions\nHigh-level\n13.2\n552.9\n28384.2\n178.2\n3.8\n163.8\n1.2\nDetailed\n42.2\n567.8\n29657.0\n257.0\n4.0\n118.0\n1.6\nWe observe that: detailed prompts lead to better software projects with lower productivity ratios\nbecause of clearer requirements and functions, while simple inputs can still generate good enough\nsoftware using MetaGPT with an executability rating of 3.8, which is comparable to the detailed\nprompt scenario. (Note that, Productivity = Token usage / Total Code Lines. The lower this ratio,\nthe better.)\nThe performance of GPT variants in HumanEval benchmark\nWe use the GPT-4\u2019s 67% Hu-\nmanEval score (OpenAI, 2023) as our baseline, acknowledging its acceptance in the HumanEval\nbenchmark. We further extend to experiments(five times) with GPT-4 (gpt-4-0613) and GPT-3.5-\nTurbo (gpt-3.5-turbo-0613) under various conditions to assess performance. (A) We directly called\nthe OpenAI API with the prompt in HumanEval. (B) We called the OpenAI API and parsed the\ncode with regex in the response. (C) We added an additional system prompt, then called the OpenAI\nAPI. The prompt is \u201dYou are an AI that only responds with Python code, NOT ENGLISH. You will\n24\n"]