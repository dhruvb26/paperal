["Preprint\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\nCamel: Communicative agents for\u201d mind\u201d exploration of large scale language model society.\narXiv preprint, 2023.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00b4emi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\nwith alphacode. Science, 2022.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\nagent debate. arXiv preprint, 2023.\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\nChandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and\nslow thinking for complex interactive tasks. arXiv preprint, 2023.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and\nSoroush Vosoughi. Training socially aligned language models in simulated human society. arXiv\npreprint, 2023.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct. arXiv preprint, 2023.\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language models. arXiv preprint, 2023.\nAgile Manifesto. Manifesto for agile software development. Snowbird, UT, 2001.\nJohn McCarthy. History of lisp. In History of programming languages. 1978.\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\nLin. Lever: Learning to verify language-to-code generation with execution. In ICML, 2023.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\nsynthesis, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint,\n2023.\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and\nMaosong Sun. Communicative agents for software development, 2023.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint, 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint, 2023.\nJ. Schmidhuber. A self-referential weight matrix. In Proceedings of the International Conference\non Artificial Neural Networks, Amsterdam, pp. 446\u2013451. Springer, 1993a.\nJ. Schmidhuber.\nG\u00a8odel machines: self-referential universal problem solvers making provably\noptimal self-improvements. Technical Report IDSIA-19-03, arXiv:cs.LO/0309048 v3, IDSIA,\nManno-Lugano, Switzerland, December 2003.\nJ. Schmidhuber. G\u00a8odel machines: Fully self-referential optimal universal self-improvers. In B. Go-\nertzel and C. Pennachin (eds.), Artificial General Intelligence, pp. 199\u2013226. Springer Verlag,\n2006. Variant available as arXiv:cs.LO/0309048.\n12\n"]