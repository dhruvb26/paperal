["however, no additional information such as model weights or gradients. In the white-box access, we assume\nfull access to the model, where we can inspect model weights.\nOperationalizing Dataset Inference.\nDataset inference for LLMs serves as a detection method for\ndata used to train an LLM. We consider the following three actors during a dispute:\n1. Victim (V). We consider a victim creator whose private or copyrighted content was used to train an LLM\nwithout explicit consent. The actor is presumed to have only black-box access to the suspect model,\nwhich limits their ability to evaluate if their dataset was used in the LLM\u2019s training process.\n2. Suspect (A). The suspect (or potential adversary in this case) is an LLM provider who may have\npotentially trained their model on the victim\u2019s proprietary, or private data.\n3. Arbiter. We assume the presence of an arbiter, i.e., a third-trusted party, such as law enforcement, that\nexecutes the dataset inference procedure. The arbiter can obtain gray-box access to the suspect LLM.\nFor instance, in scenarios when API providers only give black-box access to users, legal arbiters may\nhave access to model loss to perform MIAs.\nScenario.\nConsider a scenario where a book writer discovers that their publicly available but copyrighted\nmanuscripts have been used without their consent to train an LLM. The writer, the victim V\nin this\ncase, gathers a small set of text sequences (say 100) from their manuscripts that they believe the model\nwas trained on. The suspect A in this scenario is the LLM provider, who may have included the writer\u2019s\npublished work in their training data without obtaining explicit permission. The provider is under suspicion\nof potentially infringing on the writer\u2019s manuscripts. An arbiter, such as a law enforcement agency, steps in\nto resolve the dispute. The arbiter obtains gray-box access to the suspect LLM, allowing them to execute\nour dataset inference procedure and resolve the dispute. By performing dataset inference (as depicted in\nFigure 1), the arbiter determines whether the writer\u2019s published manuscripts were used in the training of the\nLLM. This process highlights the practical application and significance of dataset inference in safeguarding\nthe rights of artists.\nNotation.\nWe consider x to be an input sentence with N tokens x = x1, x2, ..., xN and f\u03b8 is a Language\nModel (LM) with parameters \u03b8. We can compute the probability of an arbitrary sequence f\u03b8(x1, ..., xn),\nand obtain next-token xn+1 predictions. For simplicity, assume that the next token is sampled under greedy\ndecoding, as the next token with the highest probability given the first n tokens.\n4\nFailure of Membership Inference\nWe demonstrate that the challenge of successfully performing membership inference for large language\nmodels (LLMs) remains unresolved. This problem is inherently difficult because LLMs are typically trained\nfor a single epoch on trillions of tokens of web data. In their work, Maini et al. [2021] demonstrated a near\nimpossibility result (Theorem 2), suggesting that as the size of the training set increases, the success rate of\nany MIA approaches 0.5 (as good as a coin flip). While this was shown in a simplified theoretical model, we\nassess how this holds up for contemporary LLMs with billions of parameters. As a demonstrative example,\nwe consider the most recent (and supposedly best performing) work that proposed the Min-k% Prob [Shi\net al., 2024] membership inference attack, alongside a dedicated dataset to facilitate future evaluations.\nIn their work, they show that this method performs notably better than other MIAs such as perplexity\nthresholding and DetectGPT that they benchmark their work against.\nTemporal Shift and the Need for IID Analysis.\nThe evaluation dataset used to showcase the success\nof Min-k% Prob was the WikiMIA dataset, a dataset constructed using spans of Wikipedia articles written\nbefore (train set) and after the cut-off year 2023 (validation set). This was chosen considering the training of\nthe Pythia models [Biderman et al., 2023], which was based on scrapes of Wikipedia before 2023. Note that\nsuch an evaluation setup naturally has the potential confounder of a temporal shift in the concepts in data\n5\n"]