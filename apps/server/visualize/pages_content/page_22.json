["Preprint\nFigure 10: The \u201cDrawing App\u201d generated by MetaGPT.\nC\nEXPERIMENTS\nC.1\nDETAILS OF THE SOFTWAREDEV DATASET\nThe SoftwareDev dataset includes 70 diverse software development tasks. Table 8 displays the\nnames and detailed prompts of 11 tasks within the dataset. Note that the first seven tasks listed are\nused in the main experiments of this paper.\nC.2\nADDITIONAL RESULTS\nQuantitative results of MetaGPT\nAs shown in Table 4, MetaGPT achieves an average score\nof 3.9, surpassing ChatDev\u2019s score of 2.1 Zhao et al. (2023), which is based on the Chat chain.\nCompare the scores of general intelligent algorithms, including AutoGPT Torantulino et al. (2023),\nwhich all score 1.0, failing to generate executable code. We observe that the generated code is often\nshort, lacks comprehensive logic, and tends to fail to handle cross-file dependencies correctly.\nWhile models such as AutoGPT (Torantulino et al., 2023), Langchain (Chase, 2022), and Agent-\nVerse (Chen et al., 2023) display robust general problem-solving capabilities, they lack an essential\nelement for developing complex systems: systematically deconstructing requirements. Conversely,\nMetaGPT simplifies the process of transforming abstract requirements into detailed class and func-\ntion designs through a specialized division of labor and SOPs workflow. When compared to Chat-\nDev (Zhao et al., 2023), MetaGPT\u2019s structured messaging and feedback mechanisms not only reduce\nloss of communication information but also improve the execution of code.\nQuantitative results of MetaGPT w/o executable feedback\nTable 9 presents the performance of\nMetaGPT with GPT-4 32K on 11 tasks within the SoftwareDev dataset. It also shows the average\nperformance across all 70 tasks (in the last line). Note that the version of MetaGPT used here is the\nbasic version without the executable feedback mechanism.\nQuantitative results of MetaGPT with different LLMs\nTo verify the performance of MetaGPT\non different LLMs, we randomly selected 5 SoftwareDev tasks and conducted experiments using\nGPT-3.5 and Deepseek Coder 33B5 as backends. As shown in Table 5, the results indicate that\nalthough MetaGPT can complete tasks with these LLMs, using GPT-4 as the backend yields superior\nperformance.\n5https://deepseekcoder.github.io\n23\n"]