["Abstract\n\nThe proliferation of large language models (LLMs) in the real world has come with a rise in copyright\ncases against companies for training their models on unlicensed data from the internet. Recent works\nhave presented methods to identify if individual text sequences were members of the model\u2019s training\ndata, known as membership inference attacks (MIAs). We demonstrate that the apparent success of\nthese MIAs is confounded by selecting non-members (text sequences not used for training) belonging to\na different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared\nwith ones used to train the model). This distribution shift makes membership inference appear successful.\nHowever, most MIA methods perform no better than random guessing when discriminating between\nmembers and non-members from the same distribution (e.g., in this case, the same period of time).\nEven when MIAs work, we find that different MIAs succeed at inferring membership of samples from\ndifferent distributions. Instead, we propose a new dataset inference method to accurately identify\nthe datasets used to train large language models. This paradigm sits realistically in the modern-day\ncopyright landscape, where authors claim that an LLM is trained over multiple documents (such as a\nbook) written by them, rather than one particular paragraph. While dataset inference shares many\nof the challenges of membership inference, we solve it by selectively combining the MIAs that provide\npositive signal for a given distribution, and aggregating them to perform a statistical test on a given\ndataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile\nwith statistically significant p-values < 0.1, without any false positives.\n1\nIntroduction\nTraining of large language models (LLMs) on large scrapes of the web [Gem, Ope] has recently raised\nsignificant privacy concerns [Rahman and Santacana, 2023, Wu et al., 2023]. The inclusion of personally\nidentifiable information (PII) and copyrighted material in the training corpora has led to legal challenges,\nnotably the lawsuit between The New York Times and OpenAI [Gry, 2023], among others [Bak, 2023,\nSil, 2023]. Such cases highlight the issue of using copyrighted content without attribution and/or license.\nPotentially, they undermine the rights of creators and disincentivize future artistic endeavors due to the lack\nof monetary compensation for works freely accessible online. This backdrop sets the stage for the technical\nchallenge of identifying training data within machine learning models [Maini et al., 2021, Shokri et al.,\n2017]. Despite legal ambiguities, the task holds critical importance for understanding LLMs\u2019 operations and\nensuring data accountability.\nMembership inference [Shokri et al., 2017] is a long-studied privacy problem, intending to infer if a given\ndata point was included in the training data of a model. However, identifying example membership is a\nchallenging task even for models trained on small datasets Carlini et al. [2022], Duan et al. [2023], and\n\u2217Equal contribution. Code is available at https://github.com/pratyushmaini/llm_dataset_inference/.\n1\narXiv:2406.06443v1  [cs.LG]  10 Jun 2024\n"]