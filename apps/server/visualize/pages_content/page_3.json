["Min-k% Prob.\nAs a remedy to the problem of predictability, Shi et al. [2024] proposed the Min-k%\nProb metric which evaluates the likelihood of the K% of tokens in x that have the lowest probability,\nconditioned on the preceding tokens. Hence, this MIA ignores highly predictable tokens in the suspect\nsequence. The membership prediction is made by thresholding the average negative log-likelihood of these\ntokens. The input sentence x is marked as included in pretraining data simply by thresholding the Min-k%\nProb result: Af\u03b8(x) = 1[Min-k% Prob(x) < \u03b3].\nPerturbation Based.\nThe central hypothesis behind Perturbation-based MIAs is that a sample that an\nLLM saw during training should have a lower perplexity on its original version (x), as opposed to a perturbed\nversion of the same (\u02dcx). Formally, the membership attack is defined as Af\u03b8(x) = 1 [Pf\u03b8(x)/Pf\u03b8(\u02dcx) < \u03b3], for a\nthreshold \u03b3. In our work, we investigate various forms of perturbations such as (1) white-space perturbation,\n(2) synonym substitution [Mattern et al., 2023], (3) character-level typos, (4) random deletion, and (5)\nchanging character case.\nDetectGPT.\nThis is a special case of perturbation-based MIAs, originally designed to detect machine-\ngenerated text [Mitchell et al., 2023]. The key difference is that perturbations to the input are made using\nan external language model that infills randomly masked-out spans of the original input. It then compares\nthe log-probability of x with expected value of the same from multiple infilled neighbors \u02dcxi.\nReference Model Based.\nThese methods compare the perplexity ratio between a suspect model and\na reference model on a given string. The suspect model may have seen the string during training, while\nthe reference model has not. The corresponding MIA is: Af\u03b8(x) = 1[L(f\u03b8, x) < L(f \u2032\n\u03b8, x)], where f \u2032\n\u03b8 is the\nreference model. In our work, we use the SILO [Min et al., 2023], Tinystories-33M [Eldan and Li, 2023],\nTinystories-1M [Eldan and Li, 2023], and Phi-1.5 [Li et al., 2023] models as reference models. Notably,\nthese models were not trained on general web data. In particular, the Phi-1.5 and Tinystories models were\ntrained on synthetic data generated by GPT models, and the SILO model was trained on data that is freely\nlicensed for training.\nzlib Ratio.\nAnother simple MI baseline uses the zlib library [Gailly and Adler, 2004], where a potential\nmember has a low ratio of the model\u2019s perplexity to the entropy of the text, which is computed as the\nnumber of bits for the sequence when compressed with the zlib library: Af\u03b8(x) = 1[Pf\u03b8 (x)/zlib(x) < \u03b3] [Carlini\net al., 2021]. The idea is that a model trained on a dataset will have low perplexity for its members because\nit was optimized for them, unlike the zLib algorithm, which was not tailored to the training data.\n3\nProblem Setup\nLLMs train on trillions of tokens, and the sizes of the training sets are only likely to increase [Met, dbr].\nTo increase training efficiency (in terms of time, financial costs, and environmental impact), improve\nperformance, and decrease the risk of privacy leakage, many LLM practitioners deduplicate their pre-training\ndata [Biderman et al., 2023, Carlini et al., 2021, Lee et al., 2022]. In our work, we ask this question: How\nto detect if a given dataset was used to train an LLM? and propose the idea of dataset inference for LLMs.\nAccess Levels.\nIn the black-box setting, we assume an input-output access to an LLM along with access\nto model loss, hence we are not allowed to inspect individual weights or hidden states (e.g., attention layer\nparameters) of the language model. This threat model is realistic in the case of LLM\u2019s users since many\nlanguage models can be accessed through APIs that provide limited visibility into their inner workings. For\ninstance, OpenAI [Ope] offers API access to GPT-3 and GPT-4, while Google [Gem] offers Gemini, without\nrevealing the full architecture of the models or training methodology. The gray-box access, commonly\nassumed for MIAs, additionally assumes that we can obtain the perplexity or the loss values from an LLM,\n4\n"]