["5.1\nProcedure for the LLM Dataset Inference\nWe describe the procedure for LLM dataset inference in four stages (see also visualization in Figure 1).\nRecall the initial example of a book writer who suspects that a portion of their books was trained on. We\nuse this as a running example to describe the four stages of LLM dataset inference.\nStage 0: Victim approaches an LLM provider.\nA victim (author) V approaches an arbiter with a\nclaim of ownership over data (book) that they suspect a model trainer or adversary S utilized. This stage\ninvolves the arbiter validating if the claim by V satisfies the assumptions under which dataset inference\noperates, that is, they provide an IID set of data that they suspect was trained on, and an equivalent\ndataset that S could not have seen, denoted as the validation set. This can, for instance, happen when\nauthors have multiple drafts of a book chapter, and only one of the drafts makes it to the actual print. In\nsuch a case, V\nclaims that S\ntrained on the published version of their book, because f\u03b8 (S) responds\ndifferently to the final versus rejected drafts of the book. Both suspect and validation sets are divided into\nnon-overlapping splits (partitions) A and B. We will use Aval, Asus in Stages 1, 2 and Bval, Bsus in Stage 3\nto actually perform the ownership assessment.\nStage 1: Aggregate Features with MIAs.\nThis step involves aggregating scores from various MIA\nmethods described in the previous Section 2.1. Aval, Asus sets are passed through the LLM under question\nto obtain their features derived from MIAs. Note that we use an aggregation of all the MIA methods\ndiscussed in the previous sections to create a single feature vector. In the next step, we will determine\nwhich MIAs are useful for identifying dataset membership for the given distribution.\nStage 2: Learn MIA correlations.\nIn this stage, we train a linear regressor to learn the importance of\nweights for different MIA attacks to use for the final dataset inference procedure. Across each MIA feature\nvalue, we first modify the top 5% outliers by changing their values to the mean of the distribution. This\nstep is crucial to prevent issues in Step 3, where the model might learn skewed correlations due to a few\noutlier samples. We then pass the data through a linear regression model to learn weights for each feature.\nAll \u2018suspect\u2019 samples in Asus are labeled as 0, and all validation samples in Aval are labeled as 1. A regressor\nis trained to predict the label given the samples, effectively learning the correlation between the features\nand their membership status.\nStage 3: Perform Dataset Inference.\nWe use B splits of the suspect and validation sets, holding\nout up to 1000 samples in these splits for ownership assessment. Each sample is assigned a membership\nvalue using a trained linear classifier. These values are used to perform a statistical t-test to determine\nif the suspect set was used in training the model. We then report whether the model was trained on the\nsuspect dataset based on the t-test results. For members of the suspect set, their confidence scores are\nsignificantly closer to 0 compared to non-members. The null hypothesis (H0) is that the suspect dataset\nwas not used for training. Assume that \u00b5M(Bsus) and \u00b5M(Bval) are the mean membership values of the\nsuspect and validation sets, respectively. Then, H0 and H1 (alternate hypothesis) are:\nH0 : \u00b5M(Bsus) \u2265\u00b5M(Bval);\nH1 : \u00b5M(Bsus) \u2264\u00b5M(Bval).\n(1)\nCombining p-values for Dependent Tests.\nIn order to assess the significance of the results, we\nperformed multiple t-tests using 10 different random seeds to obtain various splits of examples between\nA and B sets. Since the subsets had overlapping examples, the statistical tests are dependent [Vovk and\nWang, 2020], and p-values must be aggregated accordingly [Brown, 1975, Kost and McDermott, 2002, Meng,\n1994, R\u00fcschendorf, 1982] . Let p1, p2, . . . , pn denote the p-values obtained from the n t-tests performed with\ndifferent random seeds. Under the null hypothesis, each p-value is uniformly distributed on the interval\n[0, 1]. We approximate the combined p-value by:\npcombined = 1 \u2212exp\n n\nX\ni=1\nlog(1 \u2212pi)\n!\n(2)\n8\n"]