["Preprint", "Table 1: We report the results of training a 1.4B language model from scratch on Wikitext with intentional contamination. For each injected dataset, we report the number of examples used (size), how often the test set was injected into the pre-training data (dup count), and the p-value from the permutation test and sharded likelihood comparison test. The bolded p-values are below 0.05 and demonstrate in the case of higher duplication counts, such as datasets appearing 10 or more times, we obtain vanishingly small p-values on our test. Finally, rows marked 1e-38 were returned as numerically zero due to the precision of our floating point computation.", "Name Size Dup Count Permutation p Sharded p BoolQ 1000 1 0.099 0.156 HellaSwag 1000 1 0.485 0.478 OpenbookQA 500 1 0.544 0.462 MNLI 1000 10 0.009 1.96e-11 TruthfulQA 1000 10 0.009 3.43e-13 Natural Questions 1000 10 0.009 1e-38 PIQA 1000 50 0.009 1e-38 MMLU Pro. Psychology 611 50 0.009 1e-38 MMLU Pro. Law 1533 50 0.009 1e-38 MMLU H.S. Psychology 544 100 0.009 1e-38", "Although our test is unable to detect contamination at a duplication rate of 1, other existing literature on memorization has suggested that detection at this duplication level is extremely difficult. Prior work has found that existing tests of memorization begin to work with 10-30 duplicates (Carlini et al., 2021), that deduplicated text is hard to extract (Kandpal et al., 2022), and that dataset con- tamination with a duplication rate of 1 barely affects downstream benchmark performance (Magar & Schwartz, 2022).", "Power as a function of duplication rate. We carefully study the lowest duplication rate for which our test can reliably detect contamination. To do this, we perform the above canary study but with duplication rates ranging from 1 to 7, and we show the aggregate log p-values for each duplication rate in Figure 2. We find that we cannot reliably detect duplication rates of 1, but that at counts of 2 and 4 we begin to detect some test sets (gray points below the dashed line) and that the detection threshold is around a duplication rate of 4. This suggests that even small amounts of dataset du- plication would be sufficient for detection, and future improvements to the power of this test could enable reliable detection at much lower duplication rates.", "Log(p value) vs Dataset Duplication Count logip value) dataset's log(p value) mean log(p value) per dup count \u2014207 ---- log(p=0.05) 1 2 4 7 Dataset Duplication Count", "Figure 2: For a model pre-trained with canary datasets injected at a duplication count of 1, 2, 4, and 7, we plot the log p-value against dataset duplication count to quantify how the test\u2019s power depends on dataset duplication count.", "A public benchmark of provable test set contamination Our work demonstrates that exploiting exchangability allows us to provably detect test set contamination even for small, low-duplication count datasets. However, it is an open question whether there are tests that can reliably detect contamination at a duplication rate of 1. To support future work on this open problem, we release", "7"]