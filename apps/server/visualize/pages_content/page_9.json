["Preprint", "(2021)), LAMBADA (Paperno et al. (2016)), NaturalQA (Kwiatkowski et al. (2019b)), Open- BookQA (Mihaylov et al. (2018a)), PIQA (Bisk et al. (2019)), and MMLU (Hendrycks et al. (2021)). Computationally, we find that our test runs reasonably quickly for a 7 billion parameter model, al- lowing for the testing of 65 files for contamination in under 24 hours using 50 shards with 250 permutations per shard, and we find that the test outcomes are in general agreement with the con- tamination study results of Brown et al. (2020c) and Touvron et al. (2023): we do not find evidence of pervasive verbatim test set contamination across the models and benchmarks we tested.", "Table 2: P-values for contamination tests on open models and benchmarks. With the exception of ARC for Mistral, none of the tests give evidence for contamination. The MMLU results are marked with a \u2020 to indicate that the p-values are the result of p-value aggregating the constituent datasets of MMLU after heuristic filtering for non-exchangable datasets (see main text). The resulting LLaMA2 and Mistral p-values are small, consistent with the contamination studies in Touvron et al. (2023) identifying mild MMLU contamination.", "Dataset Size Arc-Easy 2376 0.318 0.001 0.686 0.929 0.795 BoolQ 3270 0.421 0.543 0.861 0.903 0.946 GSM8K 1319 0.594 0.507 0.619 0.770 0.975 LAMBADA 5000 0.284 0.944 0.969 0.084 0.427 NaturalQA 1769 0.912 0.700 0.948 0.463 0.595 OpenBookQA 500 0.513 0.638 0.364 0.902 0.236 PIQA 3084 0.877 0.966 0.956 0.959 0.619 MMLU\u2020 \u2013 0.014 0.011 0.362 \u2013 \u2013", "LLaMA2-7B Mistral-7B Pythia-1.4B GPT-2 XL BioMedLM", "We tested five models for contamination by eight publicly available benchmarks and list the results in Table 2. We use 50 shards and 250 permutations per shard throughout the experiments. For test sets containing more than 5,000 examples, we truncate and test only the first 5,000. Benchmark datasets in the wild may contain non-exchangable elements such as sequential indexes or duplicate examples which would break the false positive guarantees of our test. To check for these possibilities we manually inspected each dataset for non-exchangable elements, and also run our tests on a \u2018negative control\u2019 of BioMedLM (Bolton et al., 2022), a language model trained exclusively on PubMed data and known not to contain the benchmarks used here. The p-values computed for BioMedLM are not significant across the benchmarks shown here, suggesting that any significant results for the other models tested are not simply due to non-exchangeability.", "Our results in Table 2 show non-significant results across most models and datasets. While failure to reject the null hypothesis is not direct evidence in favor of the null, our synthetic canary evaluations from section 4.1 suggest that it is unlikely for these models to have seen most of these test sets more than 2 to 10 times. One notable exception is AI2-ARC on Mistral, which does return a low p-value of 0.001 and could suggest some contamination. While this p-value appears small, we caution the reader that applying a multiple hypothesis test corection would imply that 0.001 is right at the boundary of statistical significance, and due to challenges with garden-of-forking-paths type analysis, significance tests that are at the boundary of the rejection cutoff should be interpreted cautiously. We present these results as showing promising first steps towards third-party audits of test set contamination, rather than a direct proof of contamination of specific models.", "We additionally discuss contamination tests on MMLU, which was identified as a potential con- taminant in recent studies (Touvron et al. (2023)), and involves important details. MMLU is not a single test set, but rather a collection of test sets. We speculate that at least 14 of the test sets are non-exchangeable, and applying our test directly would break the false positive guarantees. To understand if our tests can still provide some insights into contamination, we run our MMLU test with a non-exchangability filtering criterion.", "To evaluate models for contamination by MMLU, we first exclude those 14 test files from con- sideration for which our test flags either BioMedLM or GPT-2 as contaminated (both are negative controls as the GPT-2 family of models predates MMLU). We run our test on each of the 43 remain- ing test files (with 1000 permutations, due to the small size of each file) and aggregate the p-values", "9"]