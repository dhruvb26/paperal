["Preprint\nJ. Schmidhuber. Ultimate cognition `a la G\u00a8odel. Cognitive Computation, 1(2):177\u2013193, 2009.\nJ\u00a8urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\nlearn: the meta-meta-... hook. PhD thesis, 1987.\nJ\u00a8urgen Schmidhuber. A \u2018self-referential\u2019weight matrix. In ICANN\u201993: Proceedings of the Interna-\ntional Conference on Artificial Neural Networks Amsterdam, The Netherlands 13\u201316 September\n1993 3, 1993b.\nJ\u00a8urgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations\nof reinforcement learning controllers and recurrent neural world models. arXiv preprint, 2015.\nJ\u00a8urgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with self-\nmodifying policies. In Learning to learn. 1998.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint, 2023.\nMarta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bj\u00f8rn Kristensen,\nKourosh Darvish, Al\u00b4an Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful\nprompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv\npreprint, 2023.\nElliot Soloway. Learning to program = learning to construct mechanisms and explanations. Com-\nmunications of the ACM, 1986.\nYashar Talebirad and Amirhossein Nadiri.\nMulti-agent collaboration: Harnessing the power of\nintelligent llm agents, 2023.\nTorantulino et al.\nAuto-gpt.\nhttps://github.com/Significant-Gravitas/\nAuto-GPT, 2023.\nR. J. Waldinger and R. C. T. Lee. PROW: a step toward automatic program writing. In D. E. Walker\nand L. M. Norton (eds.), Proceedings of the 1st International Joint Conference on Artificial Intel-\nligence (IJCAI), 1969.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\nand Anima Anandkumar. Voyager: An open-ended embodied agent with large language models.\narXiv preprint, 2023a.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\narXiv preprint, 2023b.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint, 2022.\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\ncognitive synergy in large language models: A task-solving agent through multi-persona self-\ncollaboration. arXiv preprint, 2023c.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS,\n2022.\nMichael Wooldridge and Nicholas R. Jennings. Pitfalls of agent-oriented development. In Pro-\nceedings of the Second International Conference on Autonomous Agents, 1998. URL https:\n//doi.org/10.1145/280765.280867.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint, 2022.\nEric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop):\nRecursively self-improving code generation. arXiv preprint, 2023.\n13\n"]