["\nGermini, https://gemini.google.com/. URL https://gemini.google.com/.\nIntroducing meta llama 3: The most capable openly available llm to date, https://ai.meta.com/blog/meta-\nllama-3/. URL https://ai.meta.com/blog/meta-llama-3/.\nOpenai, https://openai.com. URL https://openai.com/.\nIntroducing dbrx:\nA new state-of-the-art open llm, https://www.databricks.com/blog/introducing-\ndbrx-new-state-art-open-llm.\nURL\nhttps://www.databricks.com/blog/\nintroducing-dbrx-new-state-art-open-llm.\n11\nLLM Dataset Inference\nDid you train on my dataset?\nPratyush Maini\u2217\u22171,2\nHengrui Jia\u22173,4\nNicolas Papernot3,4\nAdam Dziedzic5\n1Carnegie Mellon University\n2DatologyAI\n3University of Toronto\n4Vector Institute\n5CISPA Helmholtz Center for Information Security\nAbstract\nThe proliferation of large language models (LLMs) in the real world has come with a rise in copyright\ncases against companies for training their models on unlicensed data from the internet. Recent works\nhave presented methods to identify if individual text sequences were members of the model\u2019s training\ndata, known as membership inference attacks (MIAs). We demonstrate that the apparent success of\nthese MIAs is confounded by selecting non-members (text sequences not used for training) belonging to\na different distribution from the members (e.g., temporally shifted recent Wikipedia articles compared\nwith ones used to train the model). This distribution shift makes membership inference appear successful.\nHowever, most MIA methods perform no better than random guessing when discriminating between\nmembers and non-members from the same distribution (e.g., in this case, the same period of time).\nEven when MIAs work, we find that different MIAs succeed at inferring membership of samples from\ndifferent distributions. Instead, we propose a new dataset inference method to accurately identify\nthe datasets used to train large language models. This paradigm sits realistically in the modern-day\ncopyright landscape, where authors claim that an LLM is trained over multiple documents (such as a\nbook) written by them, rather than one particular paragraph. While dataset inference shares many\nof the challenges of membership inference, we solve it by selectively combining the MIAs that provide\npositive signal for a given distribution, and aggregating them to perform a statistical test on a given\ndataset. Our approach successfully distinguishes the train and test sets of different subsets of the Pile\nwith statistically significant p-values < 0.1, without any false positives.\n1\nIntroduction\nTraining of large language models (LLMs) on large scrapes of the web [Gem, Ope] has recently raised\nsignificant privacy concerns [Rahman and Santacana, 2023, Wu et al., 2023]. The inclusion of personally\nidentifiable information (PII) and copyrighted material in the training corpora has led to legal challenges,\nnotably the lawsuit between The New York Times and OpenAI [Gry, 2023], among others [Bak, 2023,\nSil, 2023]. Such cases highlight the issue of using copyrighted content without attribution and/or license.\nPotentially, they undermine the rights of creators and disincentivize future artistic endeavors due to the lack\nof monetary compensation for works freely accessible online. This backdrop sets the stage for the technical\nchallenge of identifying training data within machine learning models [Maini et al., 2021, Shokri et al.,\n2017]. Despite legal ambiguities, the task holds critical importance for understanding LLMs\u2019 operations and\nensuring data accountability.\nMembership inference [Shokri et al., 2017] is a long-studied privacy problem, intending to infer if a given\ndata point was included in the training data of a model. However, identifying example membership is a\nchallenging task even for models trained on small datasets Carlini et al. [2022], Duan et al. [2023], and\n\u2217Equal contribution. Code is available at https://github.com/pratyushmaini/llm_dataset_inference/.\n1\narXiv:2406.06443v1  [cs.LG]  10 Jun 2024\nSplit A\nSuspect\nFeatures\n(A)\nVal\nFeatures\n(A)\nSplit A\n,\u00a0 0\n,\u00a0 1\nSuspect\nFeatures\n(A)\nVal\nFeatures\n(A)\n, Attack\nStage 1: Aggregate Features with MIAs\n+\nStage 2: Learn Correlations\nStage 3: Perform Dataset Inference\nSplit B\nSplit B\n+Attack\n{0.3, 0.1, ..., 0.2}\n{0.9, 0.2, ..., 0.7}\nT-Test\nTrain Linear Model\nStage 0: Victim approaches with a Suspect set\nClaim:\nLLM trained on Suspect set\nAssumption:\nSuspect set and Val set are IID\nVal set is private to the victim\u00a0\nSplit A\nSplit B\nSuspect Set\n...\nSplit A\nSplit B\n...\nVal Set\nFigure 1: LLM Dataset Inference. Stage 0: Victim approaches an LLM provider. The victim\u2019s data\nconsists of the suspect and validation (Val) sets. A victim claims that the suspect set of data points was\npotentially used to train the LLM. The validation set is private to the victim, such as unpublished data\n(e.g., drafts of articles, blog posts, or books) from the same distribution as the suspect set. Both sets are\ndivided into non-overlapping splits (partitions) A and B. Stage 1: Aggregate Features with MIAs. The\nA splits from suspect and validation sets are passed through the LLM to obtain their features, which\nare scores generated from various MIAs for LLMs. Stage 2: Learn Correlations (between features and\ntheir membership status). We train a linear model using the extracted features to assign label 0 (denoting\npotential members of the LLM) to the suspect and label 1 (representing non-members) to the validation\nfeatures. The goal is to identify useful MIAs. Stage 3: Perform Dataset Inference. We use the B splits of\nthe suspect and validation sets, (i) perform MIAs on them for the suspect LLM to obtain features, (ii) then\nobtain an aggregated confidence score using the previously trained linear model, and (iii) apply a statistical\nT-Test on the obtained scores. For the suspect data points that are members, their confidence scores are\nsignificantly closer to 0 than for the non-members.\nMaini et al. [2021] presented an impossibility result suggesting that as the size of the training set increases,\nthe success of membership inference degrades to random chance. Is testing the membership of individual\nsentences for LLMs trained for a single epoch on trillions of tokens of text data feasible? In our work, we\nfirst demonstrate that previous claims of successful membership inference for individual text sequences in\nLLMs [Mattern et al., 2023, Shi et al., 2024] are overly optimistic (Section 4). Our evaluation of the MIA\nmethods for LLMs reveals a crucial confounder: they detect (temporal) distribution shifts rather than the\nmembership of data points (as also concurrently observed by [Duan et al., 2024]). Specifically, we find that\nthese MIAs infer whether an LLM was trained on a concept rather than an individual sentence. Even when\nthe outputs of such MIAs (weakly) correlate with actual sentence membership, we find that they remain\nvery brittle across sentences from different data distributions, and no single MIA succeeds across all. Based\non our experiments, we conclude the discussion of MIAs with guidelines for future researchers to conduct\nrobust experiments, highlighting the importance of using IID splits (between members and non-members),\nconsidering various data distributions, and evaluating false positives to mitigate confounding factors.\nIf membership inference attacks are so brittle, do content writers and private individuals have no recourse\nto claim that LLM providers unfairly trained on their data? As an alternative to membership inference, we\nadvocate for a shift in focus towards dataset inference [Maini et al., 2021], which is a statistically grounded\nmethod to detect if a given dataset was in the training set of a model. We propose a new dataset inference\nmethod for LLMs that aims at detecting sets of text sequences by specific authors, thereby offering a more\nviable approach to dataset attribution than membership inference. Our method is presented in Figure 1.\nThe motivation behind dataset inference stems from the observation that in the rapidly evolving discourse\non copyright, individual data points have much less agency than sets of data points attributed to a particular\ncreator; and the fact that more often than not, cases of unfair use emerge in scenarios when multiple such\n2\nsequences or their clusters naturally occur. For instance, consider the Harry Potter series written by J.K.\nRowling. Dataset inference tests whether a \u2018dataset\u2019 or a collection of paragraphs from her books was\nused for training a language model, rather than testing the membership of individual sentences alone. We\nalso outline the specific framework required to operationalize dataset inference, including the necessary\nassumptions for the same.\nWe carry out our analysis of dataset inference using LLMs with known training and validation data.\nSpecifically, we leverage the Pythia suite of models Biderman et al. [2023] trained on the Pile dataset Gao\net al. [2020] (Section 5). This controlled experimental setup allows us to precisely analyze the model\nbehavior on members and non-members when they occur IID (without any temporal shift) as the training\nand validation splits of PILE are publicly accessible. Across all subsets, dataset inference achieves p-values\nless than 0.1 in distinguishing between training and validation splits. At the same time, our method shows\nno false positives, with our statistical test producing p-values larger than 0.5 in all cases when comparing\ntwo subsets of validation data. To its practical merit, dataset inference requires only 1000 text sequences to\ndetect whether a given suspect dataset was used to train an LLM.\n2\nBackground and Baselines\nMembership Inference\n(MI) [Shokri et al., 2017]. The central question is: Given a trained model and a\nparticular data point, can we determine if the data point was in the model\u2019s training set? Applications of\nMI methods span across detecting contamination in benchmark datasets [Oren et al., 2024, Shi et al., 2024],\nauditing privacy [Steinke et al., 2023], and identifying copyrighted texts within pre-training data [Shafran\net al., 2021]. The field has been studied extensively in the realm of ML models trained via supervised\nlearning on small datasets. The ability of membership inference in the context of large-scale language\nmodels (LLMs) remains an open problem. Recently, new methods [Mattern et al., 2023, Shi et al., 2024]\nhave been proposed to close the gap and we present them in \u00a7 2.1.\nDataset Inference\n[Maini et al., 2021] provides a strong statistical claim that a given model is a\nderivative of its own private training data. The key intuition behind the original method proposed for\nsupervised learning is that classifiers maximize the distance of training examples from the model\u2019s decision\nboundaries, while the test examples are closer to the decision boundaries since they have no impact on the\nmodel weights. Subsequently, dataset inference was extended from supervised learning to the self-supervised\nlearning (SSL) models [Dziedzic et al., 2022] based on the observation that representations of the training\ndata points induce a significantly different distribution than the representation of the test data points. We\nintroduce dataset inference for large language models to detect datasets used for training.\n2.1\nMetrics for LLM Membership Inference\nThis section explores various metrics used to assess Membership Inference Attacks (MIAs) against LLMs.\nWe study MIAs under gray-box access (which assumes access to the model loss, but not to parameters or\ngradients). The adversary aims to learn an attack function Af\u03b8 : X \u2192{0, 1} that takes an input x from\ndistribution X and determines whether x was in the training set Dtrain of the LM f\u03b8 or not. Let us now\ndescribe the MIAs we use in our work.\nThresholding Based.\nThese MIAs leverage loss [Yeom et al., 2018] or perplexity [Carlini et al., 2021] as\nscores and then threshold them to classify samples as members or non-members. Specifically, the decision\nrule for membership is: Af\u03b8(x) = 1[L(f\u03b8, x) < \u03b3], where \u03b3 is a selected pre-defined threshold. However,\nMIAs based solely on perplexity suffer from many false positives, where simple and predictable sequences\nthat never occur in the training set can be labeled as members.\n3\nMin-k% Prob.\nAs a remedy to the problem of predictability, Shi et al. [2024] proposed the Min-k%\nProb metric which evaluates the likelihood of the K% of tokens in x that have the lowest probability,\nconditioned on the preceding tokens. Hence, this MIA ignores highly predictable tokens in the suspect\nsequence. The membership prediction is made by thresholding the average negative log-likelihood of these\ntokens. The input sentence x is marked as included in pretraining data simply by thresholding the Min-k%\nProb result: Af\u03b8(x) = 1[Min-k% Prob(x) < \u03b3].\nPerturbation Based.\nThe central hypothesis behind Perturbation-based MIAs is that a sample that an\nLLM saw during training should have a lower perplexity on its original version (x), as opposed to a perturbed\nversion of the same (\u02dcx). Formally, the membership attack is defined as Af\u03b8(x) = 1 [Pf\u03b8(x)/Pf\u03b8(\u02dcx) < \u03b3], for a\nthreshold \u03b3. In our work, we investigate various forms of perturbations such as (1) white-space perturbation,\n(2) synonym substitution [Mattern et al., 2023], (3) character-level typos, (4) random deletion, and (5)\nchanging character case.\nDetectGPT.\nThis is a special case of perturbation-based MIAs, originally designed to detect machine-\ngenerated text [Mitchell et al., 2023]. The key difference is that perturbations to the input are made using\nan external language model that infills randomly masked-out spans of the original input. It then compares\nthe log-probability of x with expected value of the same from multiple infilled neighbors \u02dcxi.\nReference Model Based.\nThese methods compare the perplexity ratio between a suspect model and\na reference model on a given string. The suspect model may have seen the string during training, while\nthe reference model has not. The corresponding MIA is: Af\u03b8(x) = 1[L(f\u03b8, x) < L(f \u2032\n\u03b8, x)], where f \u2032\n\u03b8 is the\nreference model. In our work, we use the SILO [Min et al., 2023], Tinystories-33M [Eldan and Li, 2023],\nTinystories-1M [Eldan and Li, 2023], and Phi-1.5 [Li et al., 2023] models as reference models. Notably,\nthese models were not trained on general web data. In particular, the Phi-1.5 and Tinystories models were\ntrained on synthetic data generated by GPT models, and the SILO model was trained on data that is freely\nlicensed for training.\nzlib Ratio.\nAnother simple MI baseline uses the zlib library [Gailly and Adler, 2004], where a potential\nmember has a low ratio of the model\u2019s perplexity to the entropy of the text, which is computed as the\nnumber of bits for the sequence when compressed with the zlib library: Af\u03b8(x) = 1[Pf\u03b8 (x)/zlib(x) < \u03b3] [Carlini\net al., 2021]. The idea is that a model trained on a dataset will have low perplexity for its members because\nit was optimized for them, unlike the zLib algorithm, which was not tailored to the training data.\n3\nProblem Setup\nLLMs train on trillions of tokens, and the sizes of the training sets are only likely to increase [Met, dbr].\nTo increase training efficiency (in terms of time, financial costs, and environmental impact), improve\nperformance, and decrease the risk of privacy leakage, many LLM practitioners deduplicate their pre-training\ndata [Biderman et al., 2023, Carlini et al., 2021, Lee et al., 2022]. In our work, we ask this question: How\nto detect if a given dataset was used to train an LLM? and propose the idea of dataset inference for LLMs.\nAccess Levels.\nIn the black-box setting, we assume an input-output access to an LLM along with access\nto model loss, hence we are not allowed to inspect individual weights or hidden states (e.g., attention layer\nparameters) of the language model. This threat model is realistic in the case of LLM\u2019s users since many\nlanguage models can be accessed through APIs that provide limited visibility into their inner workings. For\ninstance, OpenAI [Ope] offers API access to GPT-3 and GPT-4, while Google [Gem] offers Gemini, without\nrevealing the full architecture of the models or training methodology. The gray-box access, commonly\nassumed for MIAs, additionally assumes that we can obtain the perplexity or the loss values from an LLM,\n4\nhowever, no additional information such as model weights or gradients. In the white-box access, we assume\nfull access to the model, where we can inspect model weights.\nOperationalizing Dataset Inference.\nDataset inference for LLMs serves as a detection method for\ndata used to train an LLM. We consider the following three actors during a dispute:\n1. Victim (V). We consider a victim creator whose private or copyrighted content was used to train an LLM\nwithout explicit consent. The actor is presumed to have only black-box access to the suspect model,\nwhich limits their ability to evaluate if their dataset was used in the LLM\u2019s training process.\n2. Suspect (A). The suspect (or potential adversary in this case) is an LLM provider who may have\npotentially trained their model on the victim\u2019s proprietary, or private data.\n3. Arbiter. We assume the presence of an arbiter, i.e., a third-trusted party, such as law enforcement, that\nexecutes the dataset inference procedure. The arbiter can obtain gray-box access to the suspect LLM.\nFor instance, in scenarios when API providers only give black-box access to users, legal arbiters may\nhave access to model loss to perform MIAs.\nScenario.\nConsider a scenario where a book writer discovers that their publicly available but copyrighted\nmanuscripts have been used without their consent to train an LLM. The writer, the victim V\nin this\ncase, gathers a small set of text sequences (say 100) from their manuscripts that they believe the model\nwas trained on. The suspect A in this scenario is the LLM provider, who may have included the writer\u2019s\npublished work in their training data without obtaining explicit permission. The provider is under suspicion\nof potentially infringing on the writer\u2019s manuscripts. An arbiter, such as a law enforcement agency, steps in\nto resolve the dispute. The arbiter obtains gray-box access to the suspect LLM, allowing them to execute\nour dataset inference procedure and resolve the dispute. By performing dataset inference (as depicted in\nFigure 1), the arbiter determines whether the writer\u2019s published manuscripts were used in the training of the\nLLM. This process highlights the practical application and significance of dataset inference in safeguarding\nthe rights of artists.\nNotation.\nWe consider x to be an input sentence with N tokens x = x1, x2, ..., xN and f\u03b8 is a Language\nModel (LM) with parameters \u03b8. We can compute the probability of an arbitrary sequence f\u03b8(x1, ..., xn),\nand obtain next-token xn+1 predictions. For simplicity, assume that the next token is sampled under greedy\ndecoding, as the next token with the highest probability given the first n tokens.\n4\nFailure of Membership Inference\nWe demonstrate that the challenge of successfully performing membership inference for large language\nmodels (LLMs) remains unresolved. This problem is inherently difficult because LLMs are typically trained\nfor a single epoch on trillions of tokens of web data. In their work, Maini et al. [2021] demonstrated a near\nimpossibility result (Theorem 2), suggesting that as the size of the training set increases, the success rate of\nany MIA approaches 0.5 (as good as a coin flip). While this was shown in a simplified theoretical model, we\nassess how this holds up for contemporary LLMs with billions of parameters. As a demonstrative example,\nwe consider the most recent (and supposedly best performing) work that proposed the Min-k% Prob [Shi\net al., 2024] membership inference attack, alongside a dedicated dataset to facilitate future evaluations.\nIn their work, they show that this method performs notably better than other MIAs such as perplexity\nthresholding and DetectGPT that they benchmark their work against.\nTemporal Shift and the Need for IID Analysis.\nThe evaluation dataset used to showcase the success\nof Min-k% Prob was the WikiMIA dataset, a dataset constructed using spans of Wikipedia articles written\nbefore (train set) and after the cut-off year 2023 (validation set). This was chosen considering the training of\nthe Pythia models [Biderman et al., 2023], which was based on scrapes of Wikipedia before 2023. Note that\nsuch an evaluation setup naturally has the potential confounder of a temporal shift in the concepts in data\n5\nbefore and after 2023. Any article written after 2023 was naturally a non-member of the Pythia models,\nand those written before 2023 were considered members. However, with changing times, we also encounter\ntemporal shifts in writing styles and concepts in the Wikipedia dataset. This raises concerns if membership\ntests using WikiMIA actually assess membership of a particular data point, or of that concept/style. A\nsimilar question was concurrently asked by Duan et al. [2024], who independently showed that MIAs are\nonly successful because of the temporal shift in such datasets.\nTo critically assess the robustness of the Min-k% Prob method, we conducted an exploration using the\nPythia models and their (original) train and validation splits that come from the PILE [Gao et al., 2020]\ndataset, as provided by the authors during pre-training. This facilitates a confounder-free evaluation of the\ncapabilities of membership inference attacks. In particular, the PILE dataset has more than 20 different\ndomain-specific subsets with their own training and validation splits, such as Arxiv, Wikipedia, OpenWebtext\nto name a few. Some of the key observations from our experiments on the PILE were (Figure 2a):\n1. Contrary to performance on the WikiMIA dataset where Min-k% Prob metric achieved an AUC close\nto 0.7, the method got an AUC close to 0.5 when tested on IID train and validation splits of Wikipedia\nfrom the PILE dataset, hinting at a performance akin to random guessing.\n2. We found that the method shows very high variance in AUC between different random subsets of the\ntraining and validation sets of the PILE dataset, oscillating between 0.4 and 0.7.\n3. Results on Arxiv and OpenWebText2 subsets of the PILE show AUC values near 0.4, suggesting that\nMin-k% Prob suffers from false positives, labeling validation set examples as members.\nFalse Positive Assessment by Reversing Train and Val Sets.\nDo membership inference attacks\nactually test membership? To answer this question, we do the following modification to the WikiMIA setup:\nFor every sentence in the pre-2023 subset of Wikipedia, we replace it with a sentence from the validation set\nfor Wikipedia as given in the PILE dataset. We keep the post-2023 Wikipedia subset as it is. On the one\nhand, since Pythia models were trained before 2023, it is clear that they never trained on data on Wikipedia\nfrom pages written after 2023. On the other hand, we also know that the validation set of the PILE dataset\nwas not trained on and was also deduplicated from the train set. We now perform the same membership\ninference test on these two data splits Wikipedia Val (as the now designated \u2018suspect\u2019 set) versus Wikipedia\npost-2023 (as the supposed unseen set). Remarkably, the method demonstrates an extremely high AUC\nof 0.7 in labeling examples from the suspect (validation set) as members of the training set (Figure 2b).\nThis confirms that these membership inference attacks (such as Min-k% Prob) only distinguish between\nconcepts across different temporal phases rather than verifying specific data membership, which they were\noriginally designed for.\nNo single MIA works across distributions.\nNow, we further expand our experimentation across\nmultiple different membership inference attacks outlined in Section 2.1, across 20 different subsets of the\nPILE dataset. The goal is to analyze if there is any MIA that consistently performs well across all such\ndistributions. In Figure 3, we show a heatmap of the performance of various (selected) MIA methods across\ndifferent distributions of the PILE (refer to Appendix C for full results). While some MIAs perform well\nand achieve high AUC (such as synonym substitution on PhilPapers), the same methods have an AUC of\nless than 0.5 on the next dataset of Pubmed Abstracts. These results consolidate the finding that no single\nMIA for LLMs works across all datasets, and we need to potentially find methods that adapt the choice\nof metric to the distribution. In Section 5, we will leverage a (selective) combination of different MIAs to\nimprove over the performance of any single MIA in order to perform successful LLM Dataset Inference.\nGuidelines for Future Research.\nBased on our observations in this section, we outline four important\npractices for future research in membership inference to enable sound experimentation and inferences. In\nparticular, (1) assessment for membership inference must be done in an IID setup where train and validation\nsplits are from the same distribution, (2) experiments must be repeated over multiple random splits of the\n6\n20\n40\n60\nK\n0.4\n0.5\n0.6\nAUC\nModel Parameters\n410m\n1.4b\n6.9b\n(a) Performance for different model sizes.\n5.0\n10.0\n20.0\n30.0\n40.0\n50.0\n60.0\nK\n0.00\n0.25\n0.50\n0.75\n1.00\nAUC\nMin-K Prob\nReversed Train/Val\n(b) False Positives when reversing train/val sets.\nFigure 2: Comparative analysis of the Min-k% Prob [Shi et al., 2024].\nWe measure the\nperformance (a) across different model sizes and (b) the observed reversal effect. The method performs\nclose to a random guess on non-members from the Pile validation sets.\nBookC2\nBooks3\nCC\nEuroParl\nFreeLaw\nGithub\nGutenberg\nHackerNews\nMath\nOWT2\nOpenSubs\nPhilPapers\nPubMed Abs.\nPubMed Cen.\nStack\nUSPTO\nUbuntu\nWiki\nYTSubs\narXiv\nDataset\nMax-10% Prob\nMin-10% Prob\nPerplexity\nPerturbation-based\nReference-based\nZlib Ratio\nMetric\n0.49 0.43 0.45 0.48 0.49 0.47 0.44 0.48 0.49 0.47 0.45 0.38 0.57 0.46 0.35 0.44 0.65 0.46 0.62 0.47\n0.48 0.46 0.51 0.49 0.50 0.50 0.51 0.53 0.49 0.52 0.50 0.52 0.51 0.49 0.49 0.48 0.42 0.59 0.55 0.48\n0.54 0.57 0.53 0.51 0.50 0.50 0.54 0.48 0.52 0.50 0.54 0.59 0.48 0.53 0.63 0.55 0.30 0.47 0.40 0.53\n0.53 0.55 0.52 0.45 0.53 0.48 0.59 0.54 0.48 0.50 0.56 0.70 0.32 0.52 0.42 0.48 0.47 0.48 0.55 0.51\n0.57 0.52 0.49 0.50 0.49 0.49 0.47 0.42 0.50 0.48 0.52 0.42 0.53 0.50 0.50 0.53 0.45 0.45 0.45 0.50\n0.48 0.56 0.52 0.47 0.51 0.50 0.51 0.49 0.51 0.48 0.57 0.69 0.36 0.54 0.60 0.50 0.43 0.50 0.45 0.55\n0.00\n0.25\n0.50\n0.75\n1.00\nFigure 3: Performance of various MIAs on different subsets of the Pile dataset. We report\n6 different MIAs based on the best performing ones across various categories like reference based, and\nperturbation based methods (Section 2.1). An effective MIA must have an AUC much greater than 0.5.\nFew methods meet this criterion for specific datasets, but the success is not consistent across datasets.\ndatasets, (3) experiments must be performed over multiple data distributions (4) careful experimentation\nmust be done on both false positives and false negatives to ensure MIAs do not wrongly label non-members\nas members.\n5\nLLM Dataset Inference\nDataset inference builds on the idea of membership inference by leveraging distributional properties to\ndetermine if a model was trained on a particular dataset. While MIAs operate at the instance level\u2014aiming\nto identify whether each example was part of the training data. In the previous sections, we have shown\nthat MIAs often yield signals that is close to random in determining example membership. However, if\nwe achieve even slightly better than random accuracy in inferring membership, we can aggregate these\nattacks across multiple examples to perform a statistical test. This test can then distinguish between the\ndistributions of the model\u2019s training and validation sets. In the context of LLM dataset inference, we\ncombine all the MIA methods discussed in Section 2.1.\n7\n5.1\nProcedure for the LLM Dataset Inference\nWe describe the procedure for LLM dataset inference in four stages (see also visualization in Figure 1).\nRecall the initial example of a book writer who suspects that a portion of their books was trained on. We\nuse this as a running example to describe the four stages of LLM dataset inference.\nStage 0: Victim approaches an LLM provider.\nA victim (author) V approaches an arbiter with a\nclaim of ownership over data (book) that they suspect a model trainer or adversary S utilized. This stage\ninvolves the arbiter validating if the claim by V satisfies the assumptions under which dataset inference\noperates, that is, they provide an IID set of data that they suspect was trained on, and an equivalent\ndataset that S could not have seen, denoted as the validation set. This can, for instance, happen when\nauthors have multiple drafts of a book chapter, and only one of the drafts makes it to the actual print. In\nsuch a case, V\nclaims that S\ntrained on the published version of their book, because f\u03b8 (S) responds\ndifferently to the final versus rejected drafts of the book. Both suspect and validation sets are divided into\nnon-overlapping splits (partitions) A and B. We will use Aval, Asus in Stages 1, 2 and Bval, Bsus in Stage 3\nto actually perform the ownership assessment.\nStage 1: Aggregate Features with MIAs.\nThis step involves aggregating scores from various MIA\nmethods described in the previous Section 2.1. Aval, Asus sets are passed through the LLM under question\nto obtain their features derived from MIAs. Note that we use an aggregation of all the MIA methods\ndiscussed in the previous sections to create a single feature vector. In the next step, we will determine\nwhich MIAs are useful for identifying dataset membership for the given distribution.\nStage 2: Learn MIA correlations.\nIn this stage, we train a linear regressor to learn the importance of\nweights for different MIA attacks to use for the final dataset inference procedure. Across each MIA feature\nvalue, we first modify the top 5% outliers by changing their values to the mean of the distribution. This\nstep is crucial to prevent issues in Step 3, where the model might learn skewed correlations due to a few\noutlier samples. We then pass the data through a linear regression model to learn weights for each feature.\nAll \u2018suspect\u2019 samples in Asus are labeled as 0, and all validation samples in Aval are labeled as 1. A regressor\nis trained to predict the label given the samples, effectively learning the correlation between the features\nand their membership status.\nStage 3: Perform Dataset Inference.\nWe use B splits of the suspect and validation sets, holding\nout up to 1000 samples in these splits for ownership assessment. Each sample is assigned a membership\nvalue using a trained linear classifier. These values are used to perform a statistical t-test to determine\nif the suspect set was used in training the model. We then report whether the model was trained on the\nsuspect dataset based on the t-test results. For members of the suspect set, their confidence scores are\nsignificantly closer to 0 compared to non-members. The null hypothesis (H0) is that the suspect dataset\nwas not used for training. Assume that \u00b5M(Bsus) and \u00b5M(Bval) are the mean membership values of the\nsuspect and validation sets, respectively. Then, H0 and H1 (alternate hypothesis) are:\nH0 : \u00b5M(Bsus) \u2265\u00b5M(Bval);\nH1 : \u00b5M(Bsus) \u2264\u00b5M(Bval).\n(1)\nCombining p-values for Dependent Tests.\nIn order to assess the significance of the results, we\nperformed multiple t-tests using 10 different random seeds to obtain various splits of examples between\nA and B sets. Since the subsets had overlapping examples, the statistical tests are dependent [Vovk and\nWang, 2020], and p-values must be aggregated accordingly [Brown, 1975, Kost and McDermott, 2002, Meng,\n1994, R\u00fcschendorf, 1982] . Let p1, p2, . . . , pn denote the p-values obtained from the n t-tests performed with\ndifferent random seeds. Under the null hypothesis, each p-value is uniformly distributed on the interval\n[0, 1]. We approximate the combined p-value by:\npcombined = 1 \u2212exp\n n\nX\ni=1\nlog(1 \u2212pi)\n!\n(2)\n8\nBookC2\nBooks3\nCC\nEuroParl\nFreeLaw\nGithub\nGutenberg\nHackerNews\nMath\nOWT2\nOpenSubs\nPhilPapers\nPubMed Abs.\nPubMed Cen.\nStack\nUSPTO\nUbuntu\nWiki\nYTSubs\narXiv\nDataset\nTrue positive\nFalse positive\n<1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 0.01 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3 <1e-3\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.00\n1.00\n1.00\n0.98\n1.00\n1.00\n1.00\n0.58\n0\n1\nFigure 4: p-values of dataset inference By applying dataset inference to Pythia-12b models with 1000\ndata points, we observe that we can correctly distinguish train and validation splits of the PILE with very\nlow p-values (always below 0.1). Also, when considering false positives for comparing two validation subsets,\nwe observe a p-value higher than 0.1 in all cases, indicating no false positives.\nScore Aggregation\nTo aggregate scores from different MIAs, we (i) normalize feature values to\nensure that all features aggregated across various membership inference attacks are on a comparable scale.\nThen, we (ii) adjust values of outliers before learning correlations with the classifier by replacing the\ntop and bottom 2.5% of outlier values with the mean of that (normalized) feature. Finally, we (iii) remove\noutliers before performing t-test in Stage 3 once we have a single membership value from the regressor\noutputs for each sample in the B splits of the suspect and validation sets. Once again we remove the top\nand bottom 2.5% of outlier.\n5.2\nAssumptions for Dataset Inference\nIn order to operationalize dataset inference, we must obey certain assumptions on both the datasets (points\n1 and 2 below), and the suspect language model (point 3 below).\n1. The suspect train set and the unseen validation sets should be IID. This prevents the results from being\nconfounded due to distribution shifts (such as temporal shifts in the case of WikiMIA).\n2. We must ensure no leakage between the (suspected) train and (unseen) validation sets. The validation\nset should be strictly private, and only accessible to the victim.\n3. We need access to the output loss of the suspect LLM in order to perform various MIAs.\n5.3\nExperimental Details\nDatasets and Architectures\nWe perform dataset inference experiments on all 20 subsets of the PILE.\nFor experiments with false positives, we split the validation sets into two subsets of 500 examples each. In\nall other experiments, we compare 1000 examples of train and validation sets of the PILE [Gao et al., 2020].\nWe perform dataset inference on models from the Pythia [Biderman et al., 2023] family at 410M, 1.4B,\n6.9B, and 12B scales. These open-source models allow us to know exactly which examples trained on.\nMIAs used\nIn our experiments, we aggregate 52 different Membership Inference Attacks (MIAs) in Stage\n1 (many of which are overlapping and only differ in whether they capture the perplexity or the log-likelihood,\nor contrast the ratios or differences of model predictions). For the linear regression model trained in Stage 2,\nwe train for 1000 updates over the data using simple weights over the 52 features. A total of 1000 examples\nare saved for training the regressor to learn correlations for stage 2, except in the false positive experiments\nwhere we use half the data. A complete list of all the MIAs used in our work is present in Appendix C.\n5.4\nAnalysis and Results with Dataset Inference\nWe analyze the performance of LLM dataset inference on the Pythia suite of models [Biderman et al.,\n2023] trained on the Pile dataset [Gao et al., 2020]. We separately perform dataset inference on each and\nevery subset of the PILE using the provided train and validation sets, and report the p-values for correctly\n9\nidentifying the training dataset. Before diving into various design choices, the key result is that dataset\ninference reliably finds the training distribution in all subsets of the PILE. (Figure 4). For the analysis\nof false positives, we carry out dataset inference on two splits of the validation set for each subset of the\nPILE. Since neither of the validation subsets was used to train the model f\u03b8, the returned p-values should\nbe (and are) significantly above the selected threshold of 0.1 for any useful attribution framework. It is\nworth noting that the p-values for these tests are often remarkably low in the order of 1e \u221230 and lower,\nsuggesting high confidence in attributing dataset ownership. When contrasted with the lack of reliability\nof membership inference, dataset inference indeed shows great promise for future discourse on inspecting\ntraining datasets. We will now dive deeper into various ablations and results around dataset inference such as\nthe features (membership inference attacks) chosen by the regressor, choice of pre-processing function, change\nin performance of dataset inference with model size, data duplication and number of permitted queries.\nFeature Selection.\nFor each domain, we find that the most prioritized metrics are different. This is the\nreason that the linear classifier is essential to appropriately determine the importance of each feature for\ndetermining per example membership, based on the dataset statistics. We present the results in Figure 5a.\nFor example, while the Perturbation-based metric is necessary to be present for the CC dataset, it is not\nuseful for the OWT2 dataset, which instead requires the Perplexity metric. Dataset inference automatically\nlearns which MIAs are positively correlated with a given distribution. The linear regressor can be trained\nquickly on a CPU since it is only learning a weight assignment for each feature. Now, we investigate which\nMIAs get selected by dataset inference by analyzing the importance weights given by our linear regression\nto various MIAs.\nFeature Pre-Processing.\nConsidering the chosen features, we explore various pre-processing techniques to\napply before training the linear regressor. The selected approach, referred to as Removal (Norm.) in Figure 5b,\ninvolves eliminating outliers and normalizing the feature values. We tried other approaches such as mean\ncorrection, and outlier clipping, but we found that these approaches make the dataset inference procedure\nless reliable by artificially modifying the score distributions and modifying the feature correlations learned\nby the linear model. Its effect can be seen through the occurrence of false positives for some of the datasets.\nNumber of Queries.\nWe analyze the number of queries that have to be executed against the tested\nmodel f\u03b8 to determine if a given dataset was used for training. We present the results in Figure 6a. It can\nbe seen more than half of datasets only require about 100 points, while 1000 points are sufficient to obtain\np-values smaller than the significance threshold of 0.1 for all datasets.\nSize of LLMs and Training Set Deduplication.\nBy studying the Pythia suite of models [Biderman\net al., 2023] which are trained on the same dataset, we observe the success of dataset inference is positively\ncorrelated with the number of parameters in the LLMs. We present this result in Figure 6b as a violin\nplot to allow for visualizing distributions of the datasets\u2019 p-values. It can be seen as the size of the model\nincreases, the p-value distribution concentrates below the threshold of 0.1. This correlation can be explained\nby the phenomenon that memorization by LLMs increases as their parameter size increases [Carlini et al.,\n2021], which provides a stronger signal for the intermediate MIAs responsible for dataset inference to\nsucceed. We also contrast the models trained on deduplicated or non-deduplicated training sets when we\nare only allowed 500 query points. Observe that while the aforementioned trend holds for both kinds of\nmodels, the p-value distribution is more concentrated below 0.1 for the non-deduplicated models. Following\nthe same explanation as above, this also indicates that memorization is more severe when some training\ndata is duplicated, allowing various membership inference attacks to have a stronger signal.\n10\n6\nDiscussions\nMembership Inference for LLMs.\nIn this work, we question the central foundations of research on\nmembership inference in the context of LLMs trained on trillions of tokens of web data. Our findings indicate\nthat current membership inference attacks for LLMs are as good as random guessing. We demonstrate that\npast successes in MIAs are often due to specific experimental confounders rather than inherent vulnerabilities.\nWe provide guidelines for future researchers to conduct robust experiments, emphasizing the use of IID\nsplits, considering various data distributions, assessing false positives, and using multiple random seeds to\navoid confounders.\nShift to LLM Dataset Inference.\nHistorically, membership inference focused on whether an individual\ndata point was part of a training dataset. Instead, we aggregate multiple data points from individual\nentities, forming what we now consider a dataset. In our work, we have not only put thought towards the\nscientific framework of dataset inference but also the ways it will operationalize in real-world settings, for\ninstance, through our running example of a writer who suspects that their books were trained on. Our\nresearch demonstrates that LLM dataset inference is effective in minimizing false positives and detecting\neven minute differences between training and test splits of IID samples.\nLimitations\nA central limitation to dataset inference is the assumptions under which it can be performed.\nMore specifically, we require that the training and validation sets must be IID, and the validation set must\nbe completely private to the victim. While this may appear elusive a priori, we outline concrete scenarios to\nshow how these sets naturally occur. For instance, through multiple drafts of a book, until one gets finalized.\nThe same applies to many artistic and creative uses of LLMs across language and vision today. In terms of\ndata and model access, we assume that the victim or a trusted third party, such as law enforcement, is\nresponsible for running the dataset inference so that there are no privacy-related concerns. This will require\nthe necessary legal framework to be brought in place, or otherwise suspect adversaries may deny querying\ntheir model altogether.\n7\nAcknowledgements\nWe would like to acknowledge our sponsors, who support our research with financial and in-kind contributions:\nAmazon, Apple, CIFAR through the Canada CIFAR AI Chair, Meta, NSERC through the Discovery Grant\nand an Alliance Grant with ServiceNow and DRDC, the Ontario Early Researcher Award, the Schmidt\nSciences foundation through the AI2050 Early Career Fellow program, and the Sloan Foundation. Resources\nused in preparing this research were provided, in part, by the Province of Ontario, the Government of\nCanada through CIFAR, and companies sponsoring the Vector Institute. Pratyush Maini was supported by\nDARPA GARD Contract HR00112020006.\n"]