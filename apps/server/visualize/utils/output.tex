\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage[hidelinks]{hyperref}
\usepackage{natbib}
\usepackage{geometry}
\usepackage{bm}
\usepackage{textbf}
\geometry{margin=1in}

\title{Converted PDF Document}
\author{}
\date{}

\begin{document}
\maketitle

4
2
0
2


t
c
O
8
1


]
I


1
v
8
6
3
4
1
.
0
1
4
2
:
v
i
X
r
a


CoMAL: Collaborative Multi-Agent Large Language Models for
Mixed-Autonomy Traffic


Huaiyuan Yao1, Longchao Da1, Vishnu Nandam1, Justin Turnau1,
Zhiwei Liu2, Linsey Pang2, Hua Wei1
1Ar


\section{Abstract}


The integration of autonomous vehicles into urban traffic has
great potential to improve efficiency 


1


\section{Introduction}


Recently, there has been significant growth in end-to-
end autonomous driving systems [1]. The integ


Traffic dynamics are extremely complex and chaotic
dynamical systems [8]. Pioneering researchers [9,


Rethinking human behavioral patterns, adolescents
can learn to drive in just 20 hours and handle unf


Figure 1: The overall framework of CoMAL. (a) Single-Agent Pipeline: The prompt generator integrates


knowledge-driven, relying on common sense, verbal
communication, and reasoning [22]. This is in con-


Building upon these insights and recognizing the
limitations of RL in generalizing to new traffic si


the traffic system. Leveraging the stored experiences
within the Memory Module, the agents participa


Our primary contributions are as follows:


1) To the best of our knowledge, we are the first to in-
tegrate the collaborative capability of mul


2) We propose CoMAL, a multi-agent framework de-
signed to address mixed-autonomy traffic chal-
leng


3) We evaluate CoMAL on Flow benchmark in three
classical traffic scenarios (Figure 1 (c)) and com-


Collaboration ModuleShared Message PoolReason EngineMultiple Agents on CAVs(a) Single-Agent Pipeline


4) We conduct experiments using the latest LLM
and Qwen-
including GPT-4o-mini
models,
72B/32B/7B,
s


superior performance


its


\section{2 Related Work}


\subsection{2.1 Mixed-Autonomy Traffic}

Mixed-autonomy traffic, where connected autonomous
vehicles (CAVs) along
Reinforcement learning (RL) offers a more dynamic
and adaptable solution. Benchmarks in RL [25] like


\subsection{2.2 Large Language Model-based Multi-Agents}

Large Language models (LLMs) have become integral
to mul
The communication structure of LLM-based multi-
agent systems varies across different studies to ad-


In traffic control, LLMs facili-
multi-agent systems.
tate human-machine interaction and improve dec


\section{3 Methodology}

We introduce CoMAL, a framework designed for LLM
agents integrated into connected auto
\subsection{3.1 Single-Agent Pipeline}

The quality of prompts significantly influences the out-
put quality of LL
Figure 2: (a) Left: A detailed prompt example for CoMAL, consisting of a system prompt that specifie


Memory Module; (3) receive shared messages from other
agents through the Collaboration Module; (4) g


3.1.1 Environment Perception Module To effi-
ciently extract prompts from complex environmental
data


The static map information represents the scenario
type, providing semantic priors for vehicle motio


hicle and surrounding agents, which directly influences
the planning of vehicles’ movement.


3.1.2 Memory Module Similar to human drivers,
the agent must make decisions based on reasoning
proce


\subsection{3.2 Multi-Agent Workflow In a mixed-autonomy}

traffic setting, where CAVs operate alongside human
dri
Collaboration ModuleThank you all for your cooperation. Since everyone has confirmed their positions


determines its driving plan based on its assigned role
and generates a driving planner. Finally, the


3.2.1 Collaboration Module Collaborative agents
work together towards a shared objective, typically 


Communication Structure Here, we introduce a
shared message pool to boost communication efficiency,



3.2.2 Reason Engine During team brainstorming,
each agent determines its role and formulates a strat


System Prompt The system prompt defines the
planning task and associated driving knowledge. Its pri-


3.2.3 Execution Module We utilize the rule-based
IDM model as a planner to execute driving strategie


Good day, fellow vehicles! This is Vehicle 01. In order to form an efficient queue that allows us al


\begin{equation}
ak =
\end{equation}


dvk
dt


\begin{equation}
= amax[1 − (
\end{equation}


vk
v0


)δ − (


s∗(vk, ∆vk)
sk


)2]


where s∗ is the desired headway of the vehicle,


denoted by:


(3.2)


\begin{equation}
s∗(vk, ∆vk) = s0 + max(0, vkT +
\end{equation}


vk∆vk
√
2


amaxb


)


where s0, v0, T, δ, amax, b are given parameters. We
set the desired time headway T , the comfortabl


\section{4 Experiments}


In a mixed-autonomy setting, a subset of vehicles are
tasked with the objective of improving overall


• How can CAVs enhance traffic flow and eliminate


stop-and-go shockwaves?


• How do multiple LLM-based agents collaborate to


achieve this goal?


• Do different LLM models influence the results?


Implementation Details The experiments are
4.1
conducted in Flow [15] with SUMO [37], a microscopic



\subsection{4.2 Scenarios We evaluate our model on the Figure}

Eight (FE), Ring, and Merge scenarios from the Flo
Ring The ring road network consists of a circu-
lar lane where vehicles continuously travel in a loo


Figure Eight (FE) The FE network builds on
the ring road by connecting two circular loops via an
int


Merge The merged network simulates highway
disturbances caused by vehicles entering from an on-
ramp


We investigate different levels of difficulty for each
proposed benchmark by adjusting their scenari


Table 1: Configurations of Benchmarks


Scenario Name Time(s)


Vehicles Distribution


150
150
150
150
150
150
75
75
75
75
75


13 humans, 1 CAV
7 humans, 7 CAVs
0 humans, 14 CAVs
21 humans, 1 CAV
19 humans, 3 CAVs
11 humans, 11


\subsection{4.3 Metrics To provide a comprehensive assessment}

of traffic flow and mitigate the occurrence of sho
• Average vehicle speed in the network (m/s). Higher


values indicate better overall traffic flow.


• Standard deviation of vehicle speed (m/s). The
smaller is more stable. Lower values reflect greate


\subsection{4.4 Specification on Communication In this sec-}

tion, we focus on the interactive process among agen
Metric


Model


FE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4


Table 2: Quantitative Evaluation of CoMAL on Flow Benchmarks


Avg


Std


Human Driver 5.61 5.61 5.61
6.40 6.47 6.29


CoMAL


2.88
2.86


Human Driver 4.55 4.55 4.55


0.79
1.74 1.77 2.24 0.29


CoMAL


2.88
2.85


0.79
0.26


2.88
2.87


0.79
0.31


6.40
6.59


3.12
2.88


6.40
7.40


3.12
2.91


6.40
7.42


3.12
2.61


6.40
7.86


3.12
2.47


6.40
8.83


3.12
2.70


Figure 4: Visualization of vehicle trajectories in Ring 0 setting. The ring road has a total length 


Table 3: Ablation Study
No. Perception Memory Collaboration FE 1 Merge 1


1
2
3
4
5


×
×
✓
✓
✓


×
✓
×
✓
✓


×
✓
✓
×
✓


5.61 6.40
5.81 6.51
5.17 6.72
5.18 6.88
6.47 7.40


\subsection{4.5 Quantitative Results We evaluated our model}

on the aforementioned benchmarks, varying the number
As shown in Table 2, we compared the performance
of CoMAL with that of human drivers. The results in


\subsection{4.6 Ablation Studies We conducted a detailed}

analysis of the effectiveness of each component of Co-
Ablation on Perception The comparisons in the
second and fifth rows of Table 3 demonstrate the
effec


Ablation on Memory The comparisons in the
third and fifth rows of Table 3 illustrate the impact
of t


Ablation on Collaboration The comparison pre-
sented in the fourth and fifth rows of Table 3, as wel


Table 4: Quantitative Experiment of Different LLMs: Average Velocity and Standard Deviation Analysis


Model


Average Velocity (m/s)


FE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4


Human Driver
GPT-4o-mini
Qwen-72B
Qwen-32B
Qwen-7B


5.61
5.61
5.61
6.40 6.47 6.29
6.07
5.96
6.37
6.11
5.73
6.39
4.80
5.17
5.60


2.88
2.86
2.86
2.84
2.82


2.88
2.85
2.83
2.86
2.84


2.88
2.87
2.85
2.81
2.83


6.40
6.59
6.66
6.58
6.55


6.40
7.40
7.38
7.12
6.87


6.40
7.42
7.46
7.39
7.05


6.40
7.86
8.07
7.73
7.47


6.40
8.83
8.75
8.54
8.53


Model


Standard Deviation (m/s)


FE 0 FE 1 FE 2 Ring 0 Ring 1 Ring 2 Merge 0 Merge 1 Merge 2 Merge 3 Merge 4


Human Driver
GPT-4o-mini
Qwen-72B
Qwen-32B
Qwen-7B


4.55
1.74
1.73
1.74
4.51


4.55
1.77
2.76
2.37
1.89


4.55
2.24
1.92
2.64
1.76


0.79
0.29
0.32
0.33
0.49


0.79
0.26
0.32
0.37
0.61


0.79
0.31
0.30
0.33
0.48


3.12
2.88
2.82
2.81
3.02


3.12
2.91
2.71
2.99
2.88


3.12
2.61
2.54
2.62
2.74


3.12
2.47
2.61
2.57
2.71


3.12
2.70
2.78
2.50
2.64


Table 5: Comparison to RL Benchmark


Model


FE 0 FE 1 FE 2 Merge 0 Merge 1 Merge 2


6.40
11.30
13.31
14.95
13.66
6.59


6.40
11.06
17.29
13.74
14.61
7.40


6.40
11.50
17.36
14.14
14.54
7.42


ule causes all agents to adopt nearly identical strategies,
which in turn leads to conflicts and dup


\subsection{4.7 Discussion}


Compared to RL method We conducted exper-
iments on FE and the Merge scenarios by comparing
them to 


Comparison of various LLM models We eval-
uate performance across LLM models of varying sizes
(see T


\section{5 Conclusion}


In this paper, we present CoMAL, an effective
LLM-based multi-agent framework to address mixed-
auto


We also acknowledge the limitations of our current
work and would like to point out several importan


tion of RL with LLM might be helpful in improving
the performance of LLMs. In the future, more sophi


Acknowledgements


We thank OpenAI for providing us with API credits
under the Researcher Access program.


\section{References}


[1] P. S. Chib and P. Singh, ”Recent Advancements in
End-to-End Autonomous Driving Using Deep Learn-


[2] Hallgarten, M., Zapata, J., Stoll, M., Renz, K., Zell,
A. (2024). Can Vehicle Motion Planning Ge


[3] Da, L., Gao, M., Mei, H., Wei, H. (2024). Prompt
to Transfer: Sim-to-Real Transfer for Traffic S


[4] Y. Hu et al., ”Planning-oriented Autonomous Driv-
ing,” 2023 IEEE/CVF Conference on Computer
Vis


[5] Dauner, D., Hallgarten, M., Geiger, A., Chitta, K.
(2023, December). Parting with misconceptions


[6] Mao, J., Qian, Y., Ye, J., Zhao, H., Wang, Y. (2023).
Gpt-driver: Learning to drive with gpt. ar


[7] Vinitsky, E., Kreidieh, A., Flem, L.L., Kheterpal,
N., Jang, K., Wu, C., Wu, F., Liaw, R., Liang


[9] C. Wu, A. M. Bayen and A. Mehta, ”Stabilizing
Traffic with Autonomous Vehicles,” 2018 IEEE In-
t


[10] M. Papageorgiou, C. Diakaki, V. Dinopoulou, A.
Kotsialos and Yibing Wang, ”Review of road traf-


vol. 91, no. 12, pp. 2043-2067, Dec. 2003, doi:
10.1109/JPROC.2003.819610.


[11] B. Besselink and K. H. Johansson, ”String Stability
and a Delay-Based Spacing Policy for Vehicl


[12] SHLADOVER, STEVEN E. 1995. “Review of the State
of Development of Advanced Vehicle Control Syst


[13] Taniguchi, Y., Nishi, R., Tomoeda, A., Shimura, K.,
Ezaki, T., Nishinari, K. (2015). A Demonstr


[14] S. Mosharafian and J. M. Velni, ”Cooperative Adaptive
Cruise Control in a Mixed-Autonomy Traffi


[15] C. Wu, A. R. Kreidieh, K. Parvate, E. Vinitsky and
A. M. Bayen, ”Flow: A Modular Learning Frame


[16] Wu, C., Kreidieh, A., Vinitsky, E. Bayen, A.M.
in Mixed-Autonomy
(2017). Emergent Behaviors
the


[18] Liu, Lu, Maonan Wang, Man-On Pun and Xi Xiong.
“A Multi-Agent Rollout Approach for Highway Bot-


[19] Glanois, C., Weng, P., Zimmer, M. et al. A survey on
interpretable reinforcement learning. Mach


[20] LeCun, Yann and Courant. “A Path Towards Au-
tonomous Machine Intelligence Version 0.9.2, 2022-


[21] M.A. Veldman, S. Doolaard, R.J. Bosker, T.A.B.
Snijders, Young children working together. Coope


B. Shi, L. He, and Y. Qiao. “DILU: A KNOWLEDGE-
DRIVEN APPROACH TO AUTONOMOUS DRIV-
ING WITH LARGE L


[23] Guo, Taicheng, Xiuying Chen, Yaqi Wang, Ruidi
Chang, Shichao Pei, N. Chawla, Olaf Wiest and Xi-


[24] Talebirad, Yashar and Amirhossein Nadiri. “Multi-
Agent Collaboration: Harnessing the Power of 


arXiv:2404.11584, 2024.


[35] Lai S, Xu Z, Zhang W, et al. Large language models
as traffic signal control agents: Capacity a


[36] Da L, Liou K, Chen T, et al. Open-ti: Open traffic
intelligence with augmented language model[J


[37] Lopez P A, Behrisch M, Bieker-Walz L, et al. Mi-
croscopic traffic simulation using sumo[C]//20


Learning


[25] Duan, Y., Chen, X., Houthooft, R., Schulman,
(2016). Benchmarking Deep Re-
J. Abbeel, P.
inforc


48:1329-1338 Available


[27] John Schulman, Sergey Levine, Philipp Moritz,
Michael Jordan, and Pieter Abbeel. 2015. Trust re


[28] Schulman J, Wolski F, Dhariwal P, et al. Proxi-
mal policy optimization algorithms[J]. arXiv pr


[29] Salimans T, Ho J, Chen X, et al. Evolution strategies
as a scalable alternative to reinforcemen


[30] Mania H, Guy A, Recht B. Simple random search pro-
vides a competitive approach to reinforcemen


[31] Hao Mei, Junxian Li, Bin Shi, and Hua Wei, Reinforce-
ment learning approaches for traffic sign


[32] Mei, H., Lei, X., Da, L. et al. Libsignal: an open library
for traffic signal control. Mach Lea


[33] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang,
Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang


[34] Masterman T, Besen S, Sawtell M, et al. The land-
scape of emerging ai agent architectures for 


\end{document}